#+TODO: TODO IN-PROGRESS DONE
#+ORG-IMAGE-ACTUAL-WIDTH: 500px

* Journal de bord
** Backlog
*** TODO LivingFog: Automatiser le setup de k3s sur un cluster de Raspberry
- Permettrait de graduate de minikube
  - Et d'apprendre ainsi tout ce qu'il gère pour moi automatiquement
- Et d'aller plus loin dans notre mise en place automatique d'environnements de dev et de test
- Ressources
  - Blogpost: https://medium.com/@stevenhoang/step-by-step-guide-installing-k3s-on-a-raspberry-pi-4-cluster-8c12243800b9
*** TODO SmartSense: Étudier la mise en place d'un schéma de fichier de config
- L'initialisation des noeuds reposent sur un fichier config.ini
- Pour le moment, je le retro-ingénérie pour identifier les différentes options possibles
  - Et leurs valeurs respectives
- Serait intéressant d'avoir un schéma précisant son format
  - Qui pourrait s'accompagner d'un outil de validation de fichier de config
  - Et p-e d'Enums utilisable dans le code Python
    - Plutôt que des chaînes de caractères error-prones
*** TODO SmartSense: Étudier la mise en place d'un protocole de messages
- Les différents composants de l'application s'échangent des messages pour communiquer des données et commandes
  - Noeud <-> Serveur
  - Mais aussi entre les différents modules d'un même composant
    - e.g. *InaManager* et *MainApp* du noeud
- A l'air d'utiliser pour le moment des chaînes de caractères codées en dur
- Error prone et peu lisible
- Voir si des outils permettent de décrire le schéma de communication
- Et de générer du code permettant guidant la création ou la lecture de messages
*** TODO SmartSense: Étudier la désactivation de la compaction des shards pour la base InfluxDB embarqué
- Le workaround implémenté pour éviter les crashs de la DB embarqué pointe du doigt le mécanisme de compression des shards d'InfluxDB
  - Trop coûteux à exécuter
- Une alternative possible au workaround possible est de tout simplement désactiver ce mécanisme
  - Cette PR semble confirmer que c'est possible : https://github.com/influxdata/influxdb/pull/19691
- Voir ce qu'est exactement ce mécanisme
  - Pourquoi il se déclenche ?
  - Quel est son rôle ?
  - Quel est l'impact sur InfluxDB si on le désactive ?
- Et voir si le désactiver met fin aux crashs
*** TODO SmartSense: Étudier l'intérêt de l'utilisation d'InfluxDB pour un noeud en mode autonome
- En mode autonome, un noeud SmartSense enregistre les données collectées dans sa BD InfluxDB embarquée
- Pourquoi ce SGBD ?
- On a besoin de stocker les données
- Mais a-t-on besoin d'une time series database ?
- Identifier l'utilisation que l'on va faire de la BD locale
- Et vérifier qu'une time series database est la plus appropriée pour répondre aux besoins correspondants
*** TODO SmartSense: Automatiser/instrumenter le setup d'un noeud SmartSense
- Pour le moment, un noeud extérieur SmartSense doit être installé et configuré à la main
  - Si j'ai bien compris
- Cela me paraît error-prone et peu documenté
- Utiliser un outil d'automatisation de la configuration, genre *Ansible*, permettrait de résoudre ces problèmes
- Voir sinon l'approche utilisée pour les noeuds connectés
  - Utilise une image réseau, si j'ai bien compris ce que m'expliquait Mickaël
*** TODO SmartSense: Ajouter un test *provisioningViaMqtt* pour *MainApp*
- Mettre en place un test qui vérifie que si on démarre *MainApp* avec le flag /mqttEnabled/, on a bien
  - Envoi d'une requête de provisioning via *MqttInterface*
  - Que lorsqu'on répond via *MqttInterface*, ça conduise bien à un appel de *setProvisioning()* avec les bonnes valeurs
- Mocker *MqttInterface* ?
- Ou en faire un test d'intégration ?
*** TODO Lire *OZCAR: The French Network of Critical Zone Observatories*
- Ammar m'a recommandé cet article pour en apprendre plus sur les observatoires et leurs buts
- Le consulter
*** TODO SmartSense: Fix `test_ocd`
- Dans ce module, appelle plusieurs fois la méthode `open_ocd_manager.update()`
- Malheureusement, appels erronés puisque manque un paramètre
  - IMO le paramètre `board`
- Renseigner les valeurs correctes
*** TODO SmartSense: Déterminer si possible de diminuer le niveau de privilège requis par l'application
- Pour lancer l'application, j'ai dû passer en root
- Entre autres, nécessaire pour
  - Interagir avec le watchdog
  - Démarrer le service collectd
  - Appeler les méthodes de ftdijtag_manager
- Me paraît une mauvaise idée de ne pas gérer les droits plus finement
- Déterminer si on peut configurer le système pour permettre l'exécution de l'application en tant qu'utilisateur standard
*** TODO SmartSense: Remplacer `backup_index` par l'identifiant du noeud
- `backup_index` est un compteur utilisé pour nommer les exports de la BD
- Son utilité me semble néanmoins limité
  - Le nom des backups utilise aussi un timestamp, qui permet déjà d'ordonner les backups entre eux
  - `backup_index` est réinitialisé à chaque démarrage de l'application, i.e. ne survit pas aux crashs
  - Donc pour un noeud donné, c'est le timestamp qui fait foi finalement
- Serait plus utile IMO d'utiliser le couple <identifiant de noeud, timestamp> pour nommer les exports
  - Permettrait toujours d'ordonner les exports entre eux
  - Et permettrait d'identifier/de filtrer les exports par rapport au(x) noeud(s) à l'origine
- Proposer ce changement (MR)
*** TODO SmartSense: Supprimer le code relatif à `hwclock`
- Dans `main_app`, plusieurs instructions reposent sur la commande système `hwclock`
  - Toutes les heures, MàJ l'horloge système avec l'horloge hardware
  - À chaque synchronisation avec le NTP, MàJ l'horloge hardware avec l'horloge système
- Le problème étant que le noeud SmartSense n'est pas équipé d'une horloge hardware
- Sans serveur NTP ou connexion internet, l'horloge du noeud SmartSense drift à chaque fois qu'il est éteint
- Ces lignes sont confusantes IMO
  - En laissant penser que le noeud est équipé d'une horloge hardware
- Et elles se révèlent inutiles en l'état
- Proposer leur suppression (MR)
*** TODO SmartSense: Mise en place d'un processus de CI
- Mettre en place des tests, c'était un bon début
- Mettre en place un processus de CI, ça serait mieux
  - Garantirait la reproductibilité de l'environnement de tests
  - Et que j'oublie pas de les faire tourner avant l'intégration
- Voir comment GitLab gère ça
** Semaine du <2024-06-17 Mon> au <2024-06-21 Fri>
*** Planned
**** DONE AEI'24: Faire v2 de la présentation flash
CLOSED: [2024-06-17 Mon 14:10]
- A reçu des retours de la part de Mickaël sur la v1
  #+BEGIN_PLAIN
Les items de ta partie "Absence ou faible connectivité" je les aurais intégrés dans la description des Zones Critique sous ta ligne "milieu contraints" parce que ça découle de ça.

Lister en dessous d'"une pluralité de données" les grandes familles :

     -> Capteurs environnementaux
     -> Son avec spatialisation
     -> Vidéo RGB et Infrarouge
     -> Spectre radio fréquences 2,4GHz et Sub-GHz

Rajouter dans la partie "Noeud SmartSense" la possibilité de rajouter des modules USB pour étendre les fonctionnalités.
Rajouter les capteurs manquant sur l'illustration de la slide 2 (Capteur de luminosité, radios, caméra RGB)

Il faudrait rajouter un encart sur ce qui reste à faire/développer je pense pour être transparent. (c'est le champ "perspective" du template)

     -> Connectivité longue portée (LoRa, cellulaire, ...)
     -> Raccordement à un système de récupération d'énergie (Photovoltaïque, ...)
     -> Logiciel de traitement audio local
     -> Essais sur le campus de Beaulieu
  #+END_PLAIN
- Intégrer ces différentes remarques
**** DONE Distant: Mettre en place script fournissant données au broker
CLOSED: [2024-06-18 Tue 15:53]
- Par ex, générateur de températures
  - 1 toutes les secondes
- Les envoie au broker
- Voir pour mettre un mode de fonctionnement correcte et l'autre "défaillant"
- Reste à déterminer où se place ce script
  - Dans le cluster Kubernetes, i.e. un pod
  - Ou à l'extérieur de ce dernier, et interagit avec le broker via le service dédié
**** DONE Distant: Mettre en place pod qui traite un flux de données du broker
CLOSED: [2024-06-18 Tue 16:22]
- En guise d'exemple, faire une application qui s'abonne à un flux du broker
- Pour chaque donnée, vérifie si valeur cohérente
  - e.g. pour température, vérifie qu'est entre 0 et 100
- Lève alerte en cas de multiples valeurs incohérentes
  - Fenêtre de temps ?
  - Fenêtre de valeurs ?
- Comment représenter alerte ?
- Requête à un pod dédié me paraît une bonne idée
  - Permet de centraliser le process
- Ou repasser par le broker carrément ?
  - À ce moment là, est-ce qu'on prend un broker avec persistence des messages ?
**** TODO Kubernetes: Identifier et mettre en place un process de développement
- Développer pour Kubernetes implique un certain nombre d'étapes
  - Dockerisation de l'application
  - Récupération de l'image Docker par k8s
  - MàJ des pods concernés
- Clarifier la méthode à adopter pour fluidifier ces étapes
**** IN-PROGRESS SmartSense: Rédiger un fichier de config Ansible pour le noeud SmartSense
- Maintenant que j'ai pu déployer un noeud SmartSense et identifier les différentes étapes nécessaires
- Serait utile de
  - Documenter le processus
  - L'automatiser
- Ansible est semblerait l'outil adapté pour cela
- Automatiser le déploiement d'un noeud SmartSense en utilisant Ansible
**** IN-PROGRESS LivingFog: Décomposer le playbook en rôles
- Guillermo m'a suggéré de regrouper l'ensemble des tâches de mon playbook en différents rôles
  - Requirements
  - Docker
  - Kubernetes
  - CNI
- De plus, Khaled m'a expliqué avoir défini une version *add* et une version *remove* de chacun de ses rôles
- Me paraît une feature intéressante à mettre en place
- Préparer le terrain
**** TODO SmartSense: Lire *Practical evaluation of Wi-Fi HaLow performance*
- Article de Sébastien Maudet de 2023
- Dispo ici : https://www.sciencedirect.com/science/article/pii/S2542660523002809
- Permettra p-e d'avoir une meilleure vision de l'état actuelle de la technologie
*** Done
- AEI'24: Faire la v2 de la présentation flash
  - A effectué les changements suivants
    - Ajouté la partie travaux en cours
    - Précisé les grandes familles de capteurs
    - Mentionné la possibilité de brancher des cartes d'extensions
    - Annoté l'image du noeud pour indiquer la caméra à basse résolution
  - À voir avec Mickaël ce qu'il en pense
  - Reste à
    - [X] Vérifier ce qu'on indique comme affiliations/labos
    - [X] Ajouter les logos des différentes organisations
- Distant: Mettre en place pod qui traite un flux de données du broker
  - [X] Définir et utiliser utilisateur dédié dans image Docker
    - Dans alpine, se fait avec les commandes `addgroup` et `adduser`
    - Création du groupe et de l'utilisateur au début du Dockerfile
    - Switch sur ce nouvel utilisateur juste avant CMD
  - [X] Passer par paramètres host et port
    - Approche choisie par le biais d'un env file
      - https://dev.to/jakewitcher/using-env-files-for-environment-variables-in-python-applications-55a1
    - Permet juste de récupérer des strings
      - Parse en entier le port avec int()
    - Peut passer le fichier à l'application conteneurisée via l'option suivante
      - docker run --env-file=.env --rm distant-simple-anomaly-detector
  - [X] Mise en place et utilisation du fichier __main__
    - Si présent, correspond au script exécuté avec la commande `python -m module`
      - https://docs.python.org/fr/3/library/__main__.html
    - Permet de simplifier l'exécution de l'application
  - [-] Ajouter tests
    - [X] Définir format des messages
      - Serait bien de voir ce qui est utilisé au niveau de l'observatoire
      - De la forme suivante, cf. screenshot 24.5 page 41 de la doc
        "device,key1=value1,key2=value2,...,keyn=valuen"
      - Avec
        - key sur 2 caractères
        - value qui comprend l'unité
          - Toujours sur 1 caractère ?
      - Pas sûr pour le 1er composant du message
        - Suppose qu'il s'agit d'un identifiant du capteur
      - Peut partir sur des messages du type
      - "T=40C"
    - [X] Lecture d'un message avec donnée valide
    - [-] Lecture d'un message avec donnée invalide
  - [X] Ajouter tâche CI d'exécution des tests
    - Ajout d'un stage "test"
      - Part de l'image python:3.12.4-alpine
      - Installe poetry via pip
      - Installe les deps du projet
      - Exécute les tests
    - Le stage "release" ne se lance plus par contre
    - Pourquoi donc ?
    - J'avais mis des "" en trop dans ma condition
  - [X] Rédiger config Kubernetes
    - Configuration du pod
    - Configuration de la config map pour fournir les bons paramètres
    - Config finale
      #+BEGIN_PLAIN
apiVersion: v1
kind: ConfigMap
metadata:
  name: anomaly-detector-config
data:
  .env: |-
    MQTT_HOST=mosquitto.default.svc.cluster.local
    MQTT_PORT=1883
    LOGGING_LEVEL=DEBUG
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: anomaly-detector-deployment
  labels:
    app: anomaly-detector
spec:
  selector:
    matchLabels:
      app: anomaly-detector
  template:
    metadata:
      labels:
        app: anomaly-detector
    spec:
      containers:
      - name: anomaly-detector
        image: distant-simple-anomaly-detector:latest
        imagePullPolicy: IfNotPresent
        volumeMounts:
          - name: anomaly-detector-configmap-volume
            mountPath: /.env
            subPath: .env
            readOnly: true
      volumes:
      - name: anomaly-detector-configmap-volume
        configMap:
          name: anomaly-detector-config
      #+END_PLAIN
    - Pas super convaincu de ma façon de fournir le .env
    - Voir s'il y a mieux
- Distant: Mettre en place script fournissant données au broker
  - Ajout d'un script *basic_producer.py* dans le repo de *distant-simple-anomaly-detector*
  - Fonctionnement
    - Récupère hostname et port via env
      - Peut ainsi le faire communiquer avec cluster k8s local, VMs, picocluster
    - Instancie client MQTT
    - Boucle à l'infini
      - Génère valeur entre -50 et 150
      - Génère payload correspondant
      - Envoi message au broker
** Semaine du <2024-06-10 Mon> au <2024-06-14 Fri>
*** Planned
**** DONE SmartSense: Contacter Sarramia et Mezhoud pour leurs slides sur le flux de donnée
CLOSED: [2024-06-11 Tue 16:14]
**** DONE LPWAN'24: Contacter Stéphanie pour organiser mission
CLOSED: [2024-06-11 Tue 16:30]
- Après discussion avec Guillaume et Mickaël, encouragé de participer à LPWAN'24
  - https://cpham.perso.univ-pau.fr/LPWAN24/
- Évènement a lieu à Pau le 08 et 09 juillet
  - Prévoir transport la veille
    - Train au départ de Rennes le 07/07 à 12:00, arrivée à 18:35 à Pau
    - Train au départ de Pau le 10/07 à 07:25, arrivée à 13:48 à Rennes
- Juste, quand démarre et fini exactement l'évènement ?
  - L'an dernier ça a duré les 2 jours entiers
  - https://journees-lpwan-2023.liglab.fr/programme.html
**** DONE SmartSense: Faire v1 de la présentation flash pour AEI 2024
CLOSED: [2024-06-13 Thu 14:58]
- Doit fournir une présentation flash (2-3min) à AEI 2024 d'ici le <2024-06-17 Mon>
- Faire une ébauche et itérer avec Mickaël
**** DONE LPWAN'24: Voir Stéphanie pour faire l'ordre de mission
CLOSED: [2024-06-13 Thu 14:58]
**** DONE Distant: Mettre en place service pour permettre aux pods d'interagir avec le broker
CLOSED: [2024-06-14 Fri 16:58]
- Est-ce qu'on doit passer par la même approche pour permettre aux pods d'interagir avec l'instance Mosquitto ?
- Ou comme il s'agit de pods au sein d'un même namespace, peut s'en affranchir ?
- Ou une autre est plus indiquée ?
- Quoique, est-ce qu'on mettrait vraiment le broker dans le même namespace que les pods qui le lisent ?
**** IN-PROGRESS Distant: Mettre en place pod qui traite un flux de données du broker
- En guise d'exemple, faire une application qui s'abonne à un flux du broker
- Pour chaque donnée, vérifie si valeur cohérente
  - e.g. pour température, vérifie qu'est entre 0 et 100
- Lève alerte en cas de multiples valeurs incohérentes
  - Fenêtre de temps ?
  - Fenêtre de valeurs ?
- Comment représenter alerte ?
- Requête à un pod dédié me paraît une bonne idée
  - Permet de centraliser le process
- Ou repasser par le broker carrément ?
  - À ce moment là, est-ce qu'on prend un broker avec persistence des messages ?
**** TODO Distant: Mettre en place script fournissant données au broker
- Par ex, générateur de températures
  - 1 toutes les secondes
- Les envoie au broker
- Voir pour mettre un mode de fonctionnement correcte et l'autre "défaillant"
- Reste à déterminer où se place ce script
  - Dans le cluster Kubernetes, i.e. un pod
  - Ou à l'extérieur de ce dernier, et interagit avec le broker via le service dédié
**** IN-PROGRESS SmartSense: Rédiger un fichier de config Ansible pour le noeud SmartSense
- Maintenant que j'ai pu déployer un noeud SmartSense et identifier les différentes étapes nécessaires
- Serait utile de
  - Documenter le processus
  - L'automatiser
- Ansible est semblerait l'outil adapté pour cela
- Automatiser le déploiement d'un noeud SmartSense en utilisant Ansible
**** IN-PROGRESS LivingFog: Décomposer le playbook en rôles
- Guillermo m'a suggéré de regrouper l'ensemble des tâches de mon playbook en différents rôles
  - Requirements
  - Docker
  - Kubernetes
  - CNI
- De plus, Khaled m'a expliqué avoir défini une version *add* et une version *remove* de chacun de ses rôles
- Me paraît une feature intéressante à mettre en place
- Préparer le terrain
**** TODO SmartSense: Lire *Practical evaluation of Wi-Fi HaLow performance*
- Article de Sébastien Maudet de 2023
- Dispo ici : https://www.sciencedirect.com/science/article/pii/S2542660523002809
- Permettra p-e d'avoir une meilleure vision de l'état actuelle de la technologie
*** Done
- Distant: Mettre en place service pour permettre aux pods d'interagir avec le broker
  - Est-ce que je redéploie un nouveau service `ClusterIP` ?
  - Ou est-ce que je réutilise le service `NodePort` existant ?
  - Va plutôt partir sur cette seconde approche pour le moment
- Distant: Mettre en place pod qui traite un flux de données du broker
  - Comment permettre à un pod de communiquer avec un service ?
    - Kubernetes fait tourner un service DNS
    - Et enregistre automatiquement les pods et services créés
      - https://kubernetes.io/docs/concepts/services-networking/dns-pod-service/
    - De la forme
      - <my-svc>.<my-namespace>.svc.<cluster-domain>[.example]
    - Dans mon cas
      - mosquitto.default.svc.cluster.local
  - Reste le problème du port
    - NodePort alloue un port au service dans la range 30000-32767
      - https://kubernetes.io/docs/concepts/services-networking/service/#type-nodeport
    - Peut fixer le port utiliser par le service mosquitto dans ce cas, 31883
  - [-] Reste plus qu'à mettre en place une petite application qui
    - [ ] Récupère ces paramètres
      - Pour le moment, codé en dur
    - [X] Instancie un client MQTT
    - [X] S'inscrit à un topic
    - [X] Traite les messages reçus
      - Se contente d'afficher le message pour le moment
    - [ ] Poste un message dans le broker en cas d'anomalie
  - [ ] Comment dockeriser proprement une application Python ?
    - Est-ce que j'introduis par exemple Poetry dans l'image ?
    - Ou y a un moyen plus simple pour mettre en place les dépendances ?
  - [ ] Comment mettre en place un pipeline propre & simple pour déployer sur k8s l'application ?
    - Jusqu'à maintenant, n'a utilisé que des applications/images déjà disponibles dans des registres publiques
    - Là faudrait que je génère ma propre image
    - Et que je la rende disponible à mes clusters, VMs & k8s
    - Utiliser le registre public de Docker, tout simplement ?
  - Suite à ma discussion avec Matthieu, partirais sur
    - La réalisation d'un package de l'application
    - L'enregistrement de ce package sur le registre gitlab
    - Création d'une image Docker installant ce package via pip et lançant l'application
    - Matthieu m'a expliqué qu'on peut facilement installer une app Python
  - Étapes suivies
    - Création du projet avec Poetry
      - poetry new distant_simple_anomaly_detector
    - Build du projet
      - poetry build
    - Création d'un repo git sur gitlab
      - https://gitlab.inria.fr/mnicolas/distant-simple-anomaly-detector
    - Déploiement du package python sur le repo
      - poetry config repositories.gitlab https://gitlab.inria.fr/api/v4/projects/53204/packages/pypi
      - poetry publish --repository gitlab -u <username> -p <access_token>
    - Démarrage d'un conteneur Docker pour expérimenter avant de passer au Dockerfile
      - docker run --rm -it python /bin/bash
    - Installation du package
      - Via le registry
        - pip install --index-url https://token:<access_token>@gitlab.inria.fr/api/v4/projects/53204/packages/pypi/simple distant-simple-anomaly-detector
      - Peut aussi l'installer via le repo
        - pip install git+https://token:<access_token>@gitlab.inria.fr/mnicolas/distant-simple-anomaly-detector.git
    - Exécuter le script Python
      - Package installé à /usr/local/lib/python3.12/site-packages/distant_simple_anomaly_detector/
      - Peut donc le lancer avec python3 /usr/local/lib/python3.12/site-packages/distant_simple_anomaly_detector/main.py
      - Mais y a pas un moyen plus propre ?
      - Si si
        - python -m distant_simple_anomaly_detector.main
    - Création du Dockerfile correspondant
      - Créer un venv /opt/venv/
      - Ajouter le venv/ au PATH
        - Permet de masquer le Python par défaut avec celui-ci
      - Cloner le repo
      - Lancer `main` de l'app
    - Mise en place d'une tâche de CI pour build l'image Docker automatiquement et l'enregistrer dans le registry
      - Lien utile : https://docs.gitlab.com/ee/ci/variables/predefined_variables.html
      - Pose la question de comment gérer l'access token au repo
      - Existe une variable d'env CI_REPOSITORY_URL
      - Comment l'utiliser dans le Dockerfile ?
        - Existe notion de /secrets/
          - https://docs.docker.com/build/building/secrets/
      - Et est-ce que je veux déléguer la génération d'image Docker uniquement à Gitlab ?
        - Et donc je peux me permettre de rely sur ses vars d'env
      - [X] Ou est-ce que je veux pouvoir build des images Docker, e.g. pour tester avant de push
      - Peut, par soucis d'uniformisation, peut me plier aux vars d'env de Gitlab
        - Et faire en sorte de générer l'équivalent de mon côté
    - Déploiement sur Minikube
      - Avant de tester sur les VMs/picocluster
      - Vais essayer de faire tomber ça en marche en local
      - Besoin de partager l'image Docker avec minikube
      - i.e. besoin de recréer l'image avec le daemon Docker utilisé par minikube
        - eval $(minikube docker-env)
        - docker build
      - Et d'indiquer à la création de ne pas pull l'image
        #+BEGIN_PLAIN
apiVersion: apps/v1
kind: Deployment
metadata:
  name: anomaly-detector-deployment
  labels:
    app: anomaly-detector
spec:
  selector:
    matchLabels:
      app: anomaly-detector
  template:
    metadata:
      labels:
        app: anomaly-detector
    spec:
      containers:
      - name: anomaly-detector
        image: distant-simple-anomaly-detector:latest
        imagePullPolicy: IfNotPresent
        #+END_PLAIN
      - Peut ensuite démarrer le pod
      - Ok, c'est tombé en marche
      - MAIS le port à utiliser n'est pas le nodePort (31883) du service
      - Mais le port (1883)
      - A pas de log par contre
    - À terme, faudrait plusieurs environnements/settings de tests
      - [X] App lancée "nature", interagissant avec un broker tiers
      - [?] App dockerisée
      - [X] App déployée au sein du cluster k8s
        - Comment on teste ça ?
        - Est-ce qu'on setup à chaque fois un cluster k8s de test, déploie les apps qui nous intéressent ?
        - Ou est-ce qu'on maintient un jumeau de notre cluster k8s en permanence, un staging environment ?
          - Peut utiliser un outil comme minikube/microk8s/kind pour cette étape
        - Et on y effectue une sorte de rolling release où on monitore le comportement des pods nouvelle version avant de valider/rollback le changement ?
          - Comment on ferait ça ?
  - Questions restantes
    - Comment tester l'appli ?
      - Suffit de faire une fonction prenant les données intéressantes du message en paramètre
        - Topic, payload
      - Qu'on puisse faire des tests unitaires dessus
    - Comment gérer proprement le passage de paramètres au script Python ?
      - Notamment le host et le port du message broker
    - Comment gérer le déclenchement de la génération de l'image Docker ?
      - Uniquement quand on intègre dans main ou qu'on ajoute un tag ?
    - Comment gérer la génération de l'image Docker à la fois pour desktop et pour RPi ?
    - Comment utiliser un autre user que root dans l'image Docker ?
    - Comment configurer Mosquitto proprement ?
    - Serait intéressant de voir au niveau du SED si quelqu'un est familier du process/pipeline de Local Kubernetes Development ?
- Réunion suivi du <2024-06-11 Tue>
  - Préparation
    - LivingFog
      - La dernière fois, venais de mettre en place un cluster k8s sur le picocluster
      - A reproduit/adapté les étapes pour pouvoir travailler sur VMs
      - Et a automatisé le processus avec Ansible
        - Avec Guillermo, a fait une revue du code
          - M'a indiqué comment structurer le playbook à l'aide des rôles
          - M'a donné accès au Ansible pour l'overleaf Inria
        - A amélioré mon playbook en me basant dessus
        - Mais encore quelques difficultés
          - Sur bonne utilisation des rôles et la gestion des dépendances de modules Ansible
            - Est-ce que chaque rôle gère ses dépendances ?
            - Comment gérer les rôles où des groupes d'hosts différents doivent effectuer des tâches
        - Vois Matthieu Simonin dans l'aprem pour en discuter
      - On a eu une réunion avec Ammar et Guillaume avec nos partenaires de l'OSUR et de Potsdam dans le cadre du projet Distant
        - Projet de mise en place d'un système de monitoring/assurance qualité des données dans le cadre de l'observatoire de la rivière Kaligandaki au Népal
        - Idée est ne pas remplacer l'existant, seulement de s'y greffer
        - i.e. qu'on duplique le flux de données, qu'on le donne en entrée à notre noeud fog
        - Et que ce dernier fasse tourner une ou des applications qui observent le flux, analysent les données, et émettent une alerte en cas de détection d'une anomalie
        - Actuellement sur la réalisation d'un PoC avec Kubernetes
          - Une appli de production de données, externe
          - Qui les transmet à un message broker, géré par k8s
          - Et une autre appli de monitoring, gérée par le cluster, qui les consomme et génère des alertes en cas d'anomalies
        - Progression
          - A mis en place le message broker et la configuration pour le rendre accessible depuis l'extérieur du cluster k8s
          - A pu tester écrire et lire depuis une app s'exécutant depuis l'extérieur du cluster k8s
          - À présent, vois pour interagir avec le broker depuis l'app de monitoring
          - Plutôt en train de voir comment conteneuriser proprement une application python
          - Et comment mettre en place un pipeline propre pour développer et tester l'application sur VMs/picocluster facilement
    - SmartSense
      - A profité des connaissances gagnées sur Ansible avec LivingFog pour améliorer le playbook
        - Mais quelques difficultés
          - Quand installe le gestionnaire de dépendances poetry, celui-ci n'est pas dispo
          - Vois pour gérer/configurer le virtual env utilisé manuellement
        - Me pose la question de conteneuriser ou non les composants de SmartSense
          - SmartSense Node App
          - InfluxDB
          - Grafana
      - A participé aux journées TF du <2024-05-28 Tue> au <2024-05-30 Thu>
        - Présentation de SmartSense dans le cadre des sessions café-bazar
          - Bien déroulé, pas de bug à reporter
          - A noté intérêt sur la partie multi-capteurs, consultation en temps réel des données et carte d'extension
          - Sur la partie calculs en local, problématique partagée dans le cadre d'une caméra spectromètre
            - Silvert Gousset et Samuel Barnola de l'Institut de Planétologie et d'Astrophysique de Grenoble (IPAG)
        - Discussion avec Olivier Berder
          - M'a parlé des travaux de Sébastien Maudet, doctorant à Nantes, qui a fait des évaluations de WiFi HaLow dans le cadre de sa thèse
            - Des papiers sont disponibles
          - M'a parlé des travaux de Samir El Rhaz, doctorant à Lannion, qui étudie/travaille avec Wi-SUN, un dérivé de Zigbee
          - Serait intéressant de prendre le temps de se pencher sur leurs travaux/ces technos
        - Présentation intéressante IMO sur le flux de données
          - Par David Sarramia et Jérémy Mezhoud qui travaillent sur le CEBA
          - Donnaient des bonnes pratiques sur comment structurer/quels champs inclure dans ses données pour les rendre pérennes/exploitables
          - Pareil, serait intéressant de relire ça à tête reposée pour voir ce qu'on peut en tirer
        - Question personnelle pour mieux cerner le projet
          - Vu l'autonomie du noeud, semble plus adapté à des expérimentations sur une journée
          - À quoi correspond ce type d'expérimentation ? Avez-vous un exemple ?
      - Atelier expérimentation et instrumentation (AEI) 2024 aura lieu à Rennes du 25 au 28 juin
        - Workshop/conf sur la conception de capteurs
          - Public + d'électroniciens si j'ai bien compris ?
        - Avec l'aide de Mickaël, a proposé d'y refaire la démo de SmartSense
        - Doit envoyer une présentation flash d'ici le 17 juin
  - Questions
    - [X] Conteneurisation des composants de SmartSense ?
      - Pour plusieurs composants de SmartSense, plutôt que de les installer manuellement
      - Pourrait les déployer par le biais de conteneurs
        - SmartSense Node App
          - Mode privilégié ou autorisation manuelle du bus I2C
        - InfluxDB
        - dnsmasq
      - Systemd vs Docker ?
    - [X] Prochaines étapes pour l'antenne ?
    - [X] Participation aux journées LPWAN'2024 ?
      - À Pau les 8 et 9 juillet
      - https://cpham.perso.univ-pau.fr/LPWAN24/
      - Portent sur les technologies LPWAN
        - P-e plus sur la partie conception/challenges qu'utilisation cependant
      - Pas de date précise pour les inscriptions sur le site
  - Notes
    - Quel est le protocole de données utilisé dans l'observatoire népalais pour transmettre les données des capteurs au data logger ?
      - SDI-12
    - Est-ce qu'un Compute Module consomme autant qu'un RPi équivalent ?
    - Voir si le shell peut être une source d'erreur Ansible
    - Dockeriser peut être une piste d'amélioration
    - Wi-SUN
      - Actuellement en cours d'évaluation au sein de l'équipe
      - Portée de plusieurs centaines de mètres/voir le km
      - Débit > LoRa (10-100ko/h)
    - Utilisation du noeud SmartSense
      - Dans le cadre de sciences participatives sur durée courte
        - Comptage d'insectes sur un w-e
      - Autonomie limite cas d'utilisations à titre de démonstration
        - Expérience avec les bus
      - Peut être utilisé de manière exploratoire
        - Sait ce qu'on veut mesurer, mais pas comment
        - Fait des mesures avec noeud SmartSense pour voir si une approche sort du lot
        - Reviens ensuite avec capteur spécialisé
- Réunion Ansible avec Matthieu du <2024-06-11 Tue>
  - Questions
    - [X] Est-ce que chaque rôle gère ses propres dépendances ?
      - Un "requirements" par rôle ?
      - Oui
    - [X] Peut donc pas au sein d'un rôle refaire un filtre sur le host ?
      - Cas d'actions de configuration qui ne concernent que les workers
      - Cas des actions pour permettre à un worker de rejoindre le cluster k8s
      - Possible, pas très propre, mais possible
    - [ ] Peut préciser l'user courant pour des tâches d'un rôle ?
      - Dans requirements, si je veux installer des outils autre chose qu'en tant que root ?
    - [X] Comment gérer les dépendances des modules Ansible, notamment les librairies python ?
      - Regardais pour utiliser le module kubernetes
        - https://docs.ansible.com/ansible/latest/collections/kubernetes/core/k8s_module.html
      - Mais a pour dépendances deux modules Python
        - https://pypi.org/project/PyYAML/
        - https://pypi.org/project/jsonpatch/
      - Obtiens une erreur quand utilise pip
        - Pas le droit d'installer en global
      - Peut utiliser un virtualenv
      - Mais comment réutiliser ce virtualenv lors de mes tâches avec ce module ?
        Peut supprimer fichier
      - Voir https://gitlab.inria.fr/discovery/enoslib/-/issues/195
    - [ ] Au sujet de Python, des bonnes pratiques pour dockeriser application ?
      - Juste copier le virtual env préparé dans l'image finale ?
  - Notes
    - Voir pour packager l'application et la push sur le registry gitlab
      - Peut aussi push les images Docker sur gitlab au fait
- SmartSense: Faire v1 de la présentation flash pour AEI 2024
  - Mise en place d'un git
    - https://gitlab.inria.fr/smartsense/smartobs/2024-aei
  - Réalisation d'un premier jet et envoi à Mickaël
- Réunion non-permanents du <2024-06-14 Fri>
  - Travaille toujours sur le prototype d'appli pour le projet Distant
    - Données arrivent sur message broker, app qui process ces données pour détecter des anomalies
    - Orchestrée par k8s
  - Done
    - [X] Comment exposer le message broker aux applications extérieures du cluster ?
    - [X] Comment exposer le message broker aux applications internes au cluster ?
  - M'attaque principalement au(x?) process pour développer, tester et déployer le système
    - [X] Comment distribuer une app Python ?
    - [X] Comment dockeriser proprement une app Python ?
    - Comment automatiser la génération de l'image Docker avec Gitlab ?
    - Tout en gardant un pipeline simple pour tester les changements en local sur minikube
      - Local Kubernetes Development
** Semaine du <2024-06-03 Mon> au <2024-06-07 Fri>
*** Planned
**** DONE SmartSense: Rédiger abstract pour la démo
CLOSED: [2024-06-04 Tue 08:42]
- Viens de me rendre compte que pour les AEI 2024, doit soumettre le résumé de la démo aujourd'hui
- Malheureusement pas de nouvelles de la part de Mickaël à ce sujet
- Rédiger l'abstract et lui envoyer
- Soumettre en absence de réponse de sa part
**** DONE Distant: Déployer Mosquitto sur le cluster Kubernetes
CLOSED: [2024-06-06 Thu 17:53]
- Première étape du projet est de voir comment on déploie proprement un pod
- Fichier de configuration ?
**** DONE Distant: Mettre en place service pour offrir un accès externe au broker
CLOSED: [2024-06-06 Thu 17:53]
- Voir du côté d'Ingress?
- Pareil, voir comment faire cela proprement
**** DONE LivingFog: Utiliser un inventory en YAML
CLOSED: [2024-06-10 Mon 11:31]
- Semble permettre de coupler plus élégamment les modes de déploiement avec les hosts et les variables correspondants
- Permettrait d'éviter de devoir préciser le mode et le bon inventaire lorsqu'on lance le playbook
- Confirmer ce point et mettre en place le cas échéant cette nouvelle approche
**** IN-PROGRESS SmartSense: Rédiger un fichier de config Ansible pour le noeud SmartSense
- Maintenant que j'ai pu déployer un noeud SmartSense et identifier les différentes étapes nécessaires
- Serait utile de
  - Documenter le processus
  - L'automatiser
- Ansible est semblerait l'outil adapté pour cela
- Automatiser le déploiement d'un noeud SmartSense en utilisant Ansible
**** IN-PROGRESS LivingFog: Décomposer le playbook en rôles
- Guillermo m'a suggéré de regrouper l'ensemble des tâches de mon playbook en différents rôles
  - Requirements
  - Docker
  - Kubernetes
  - CNI
- De plus, Khaled m'a expliqué avoir défini une version *add* et une version *remove* de chacun de ses rôles
- Me paraît une feature intéressante à mettre en place
- Préparer le terrain
**** TODO Terra Forma: Demander accès au Resana
- Suis intéressé par récupérer les slides présentées lors des rencontres Terra Forma
- Particulièrement celles portant sur la gestion du flux de données
- Et les bonnes pratiques sur le formatage des données
- D'après Ammar, il existe un Resana où ces documents seront postés
- Y demander l'accès
**** TODO Distant: Mettre en place script fournissant données au broker
- Par ex, générateur de températures
  - 1 toutes les secondes
- Les envoie au broker
- Voir pour mettre un mode de fonctionnement correcte et l'autre "défaillant"
- Reste à déterminer où se place ce script
  - Dans le cluster Kubernetes, i.e. un pod
  - Ou à l'extérieur de ce dernier, et interagit avec le broker via le service dédié
**** TODO SmartSense: Lire *Practical evaluation of Wi-Fi HaLow performance*
- Article de Sébastien Maudet de 2023
- Dispo ici : https://www.sciencedirect.com/science/article/pii/S2542660523002809
- Permettra p-e d'avoir une meilleure vision de l'état actuelle de la technologie
*** Done
- SmartSense: Rédiger un fichier de config Ansible pour le noeud SmartSense
  - Mise en place de plusieurs inventaires
    - Un pour dev, un de prod
  - Décomposition du playbook en rôles
  - Pour le moment a les roles suivants
    - requirements, i.e. les tâches pour installer les dépendances du système
    - influxdb, i.e les tâches pour installer et configurer InfluxDB
    - app, i.e. les tâches pour récupérer, setup et lancer l'application
  - Autres rôles à mettre en place
    - configuration, i.e. les tâches pour configurer le système
    - collectd, i.e. les tâches pour installer et setup le service collectd
    - dhcp, i.e. les tâches pour installer, setup et lancer le serveur DHCP local
    - mqtt, i.e. les tâches pour installer, setup et lancer le message broker local
    - grafana, i.e. les tâches pour installer, setup et lancer le dashboard local
  - Configuration
    - [X] Activer l'interface I2C
    - [X] Désactiver le swap
    - [ ] Setup connexion Wi-Fi
    - [ ] Enable connexion Wi-Fi
  - InfluxDB
    - [ ] Créer l'utilisateur admin
    - [X] Mettre en place la configuration appropriée
  - App
    - [ ] Installer les dépendances Python de l'application
    - [ ] Mettre en place le service systemd
  - Utilisation de Docker ?
    - Question intéressante de Khaled
    - Pourquoi installer les applications sur le système ?
    - Et non pas utiliser Docker/des containers ?
    - Me parait pas déconnant par exemple de conteneuriser InfluxDB ou Grafana par exemple
    - Avec le mode privileged, devrais pouvoir faire fonctionner aussi l'application ou collectd dans des containers
    - Permettrait d'éviter les conflits de dépendances
    - Et de contrôler facilement ce qu'on déploie/fait tourner
    - Aborder la question lors du prochain point
  - Mise en place connexion Wi-Fi
    - Commande exécutée
      #+BEGIN_PLAIN
nmcli c add \
    con-name INRIA-interne \
    type wifi \
    ifname wlan0 \
    ssid INRIA-interne \
    802-11-wireless-security.key-mgmt wpa-eap \
    802-1x.identity mnicolas \
    802-1x.anonymous-identity anonymous@inria.fr \
    802-1x.ca-cert file:///home/livingfog/Chain-TCS-4-OV.pem \
    802-1x.domain-suffix-match inria.fr \
    802-1x.eap peap \
    802-1x.phase2-auth mschapv2
      #+END_PLAIN
    - Le module nmcli ne me paraît pas adapté à ce que je veux
      - https://docs.ansible.com/ansible/latest/collections/community/general/nmcli_module.html
      - Ne vois pas comment spécifier certains paramètres
      - Genre le certificat ou l'identity
    - Suis tombé sur un rôle qui semble plus approprié
      - https://github.com/linux-system-roles/network
    - Mais a l'air d'être conçu pour les distribs Red Hat
      - Utilise yum ou dnf comme package manager j'ai l'impression
    - À creuser
- SmartSense: Rédiger abstract pour la démo
  - Abstract
    #+BEGIN_PLAIN
Afin de suivre l'évolution et les impacts du changement climatique sur l'environnement, différents pays et organisations mettent en place des Observatoires de la Zone Critique (OCZs).
Les OZCs sont des systèmes où collaborent les différentes communautés des sciences de l'environnement pour étudier et monitorer des milieux naturels donnés.
Ces observatoires collectent ainsi un large éventail de données, de disciplines différentes, afin de suivre leur évolution respective et d'élucider les liens existants entre elles.

Le déploiement d'équipements au sein des OCZs soulève cependant plusieurs enjeux.
Notamment, les capteurs et autres appareils déployés dans les OZCs ne disposent que d'une connectivité faible, voire inexistante, les OCZs pouvant se situer dans des zones reculées.
Cette connectivité faible limite la fréquence et la quantité de données que les capteurs et autres appareils peuvent remonter.
Cette contrainte entrave donc la transmission des types de données volumineux, e.g. image et son.
Elle empêche aussi l'utilisation du Cloud pour la mise en place d'applications quasi-temps réel, e.g. la détection et la réponse à des évènements à partir des données collectées.

Pour répondre à ces problématiques, nous développons le noeud SmartSense.
Le noeud SmartSense est une plateforme multi-capteurs.
Elle comprend :
- Un ensemble de capteurs environnementaux (température, lumière, humidité, pression atmosphérique, CO2, COV)
- Un télémètre
- Une centrale inertielle 9 axes
- Deux caméras infrarouges
- Quatre microphones
- Une caméra VGA
Le noeud SmartSense est aussi équipé d'un micro-ordinateur.
Ce dernier lui permet ainsi d'effectuer des traitements en local sur les données collectées pour détecter et réagir à des évènements, ou produire des données concises à partir de flux de données volumineux.

Dans le cadre de cette démonstration, nous présentons notre prototype de noeud SmartSense autonome.
Cette version courante du noeud SmartSense configure ses différents capteurs et récupère les données collectées par ces derniers.
Les données collectées sont formatées et stockées dans une base de données locale.
Un tableau de bord permet de les consulter en quasi-temps réel.
Cette version propose donc une base solide pour l'ajout des fonctionnalités proposées.
    #+END_PLAIN
  - Validé par Mickaël
  - Déposé
- LivingFog: Décomposer le playbook en rôles
  - Pour le moment, pars sur les rôles suivants
    - [X] Requirements
      - Installer les dépendances globales
    - [-] Configuration
      - [X] Vérifier que le swap est désactivé
      - [X] Vérifier que le cgroup memory est activé
      - [X] Vérifier que l'ip forwarding est activé
      - [-] Pour les workers, vérifier que l'@ip du noeud principal est renseignée dans /etc/hosts
      - En fonction de l'architecture
    - [X] containerd
    - [-] Kubernetes
  - Rencontre des difficultés à gérer hosts et roles
    - J'ai des rôles, e.g. configuration et kubernetes, qui ont des tâches en commun
    - Mais qui ont quelques tâches spécifiques aux différents groupes de hosts
      - Lors de la configuration, les workers doivent ajouter la machine master dans leur /etc/hosts
      - Lors de la création du cluster kubernetes
        - master doit créer le cluster
        - workers doivent rejoindre le cluster
    - Sauf que je n'arrive pas au sein d'un rôle à spécifier que certaines tâches concernent un groupe particulier de hosts
    - À la réflexion, c'est pas déconnant
    - Les rôles sont conçus pour concevoir des ensembles de tâches réutilisables de projet en projet
    - Pour cela, ils doivent être découplés de la notion de hosts
    - Doit donc revoir mon approche sur comment organiser mon code Ansible
  - Nouvelle organisation de rôles
    - But est de rendre l'ensemble des tâches spécifiques d'un rôle spécifique à un type de hosts
    - setup-kubernetes: all
    - init-k8s-cluster: master
    - join-k8s-cluster: worker
  - Me paraît plutôt claire comme organisation et fonctionnement
  - Mais join-k8s-cluster va poser problème
    - Destiné aux workers
    - Mais nécessite que master récupère la commande pour join au préalable
    - Comment faire cela ?
    - Rôle à paramètre ?
- LPWAN'24
  - Ammar m'a informé qu'allaient se tenir les journées LPWAN'24
  - Portent sur les différentes technologies de communications Low Power
    - "Le terme LPWAN (Low-Power Wide Area Network) est envisagé ici dans son sens large, et regroupe les technologies comme LoRa, LTE-M, NB-IoT, Mioty, 802.11ah..."
  - Les 08 et 09 juillet à Pau
  - Aborder le sujet lors de la prochaine réunion de suivi
- Kubernetes Ingress Tutorial for Beginners
  - https://www.youtube.com/watch?v=80Ew_fsV4rM
  - Idée est qu'on ne veut pas exposer une adresse IP et un port pour chaque service que l'on veut rendre accessible
  - Ingress est un reverse-proxy (?) que l'on va déployer pour gérer les requêtes
  - Et qui va s'occuper de les communiquer aux services correspondants
  - Configuration
    - S'agit d'un kind spécifique, Ingress
    - Permet ensuite de définir des règles de routage
    - Globalement, de spécifier à quel path on associe quel service
  - Par contre, nécessite la déclaration d'un host
    - Quid d'un test en local ?
    - Ah ben peut probablement utiliser localhost
  - Host correspond au entrypoint du k8s cluster
    - Peut s'agir d'une machine du cluster
    - Ou a l'air de pouvoir aussi être une machine externe qui fait le lien
  - Existe plusieurs implémentations de Ingress
  - Doit en déployer une dans le cluster k8s
    - Celle proposée par k8s au moment de la vidéo est K8s Nginx Ingress Controller
      - Bel et bien un reverse proxy tel que je connais
  - Approche proposée en cas d'install sur du bare metal est de mettre en place un proxy
    - Peut être hardware ou software
    - Qui fasse office d'entrypoint
    - Et qui transmet les requêtes au pod Ingress
  - Pourquoi pas directement exposer le pod Ingress ?
    - Peut p-e y avoir plusieurs pods Ingress
    - Et dans ce cas le proxy doit faire office de load balancer ?
  - Après avoir démarré Ingress, une IP va être attribué
    - Peut accéder au service grâce à elle
  - Si tente d'accéder à une URL inconnue, est redirigé sur un endpoint par défaut
    - Peut définir un service sur cet endpoint pour gérer de manière custom les erreurs 404
  - Config Ingress permet de gérer plusieurs services simplement
    - Soit en multipliant les paths
      - i.e. ajouter un préfixe à l'URL pour indiquer le service concerné
    - Soit en multipliant les sous-domaines
      - i.e. utiliser le sous-domaine pour indiquer le service concerné
  - Permet aussi de mettre en place connexion sécurisée
    - Juste besoin d'enregistrer le certificat SSL en tant que Secret du cluster
  - Vidéo n'aborde pas par contre la gestion des ports
    - Utilise seulement HTTP/HTTPS comme protocole
    - Mais dans mon cas, je veux rendre accessible un serveur MQTT
    - Préfère faire en sorte que le client utilise le port par défaut
  - Documentation indique aussi qu'Ingress n'est plus le composant à utiliser, mais Gateway
    - https://kubernetes.io/docs/concepts/services-networking/gateway/
    - Qui a l'air d'être orienté HTTP aussi
  - À creuser donc
- Distant: Mettre en place service pour offrir un accès externe au broker
  - Ressource suivante semble être complète
    - https://www.enabler.no/blog/mosquitto-mqtt-broker-in-kubernetes
  - Ce qui est à noter, c'est que MQTT repose sur TCP
  - Doit donc voir comment router du traffic TCP vers le broker
  - Indique que Ingress-nginx offre cette feature
  - Est-ce que Gateway aussi ?
  - Oui mais c'est encore expérimental
    - https://gateway-api.sigs.k8s.io/guides/tcp/
    - Est-ce que je peux me baser là-dessus ?
  - Dans LivingFog, utilisaient un simple service NodePort
    - https://github.com/FogGuru/livingfog/blob/main/chirpstack-kubernetes/mosquitto/service.yml
  - A parcouru *Understanding Kubernetes in a visual way*
    - Confirme que qu'un service NodePort est une bonne approche pour commencer
  - Et qu'à terme ça pourrait être mieux de passer à du Ingress
  - A déployé un système basique de message broker
    - Utilisation de mosquitto
    - Définition au préalable d'une ConfigMap pour spécifier la config de mosquitto
      - Notamment autoriser l'utilisation anonyme du broker
      - Et qu'il écoute bien sur le port 1883
    - Définition d'un service *NodePort* permettant d'exposer le message broker à l'extérieur du cluster
  - Mais n'arrive pas à l'utiliser
    - Utilise le test d'intég mis en place pour SmartSense
    - Mais obtient une erreur, impossible de se connecter
  - N'arrive pas à lire les logs du container sur VMs
    - Obtient erreur `Error from server (NotFound): the server could not find the requested resource ( pods/log mosquitto-deployment-6f7458b9d-sc4pw)`
    - Doit avoir un soucis au niveau de ma config Vagrant
  - Sur picocluster par contre, cela fonctionne
  - On a bien une trace d'une connexion au broker dans ses logs
  - Pourquoi ?
  - En fait, y a deux endroits dans le code du test où l'on définit l'adresse du message broker
    - Dans la fonction helper permettant de créer un client pour lire les messages postés
    - Dans le fichier config fournit à l'application, permettant ensuite d'instancier des *MqttInterface*
  - N'avait modifié que le 1er emplacement dans le code
  - Essayais donc de lire, mais sans écrire
  - Fonctionne désormais, après avoir modifié les deux emplacements dans le code
** Semaine du <2024-05-27 Mon> au <2024-05-31 Fri>
*** Planned
**** DONE SmartSense: Remettre en fonctionnement le noeud
CLOSED: [2024-05-27 Mon 14:23]
- Ça commence à faire un moment que je n'ai pas travaillé sur le noeud SmartSense
- Encore plus que je l'ai fait tourner dans son état d'origine
- Le redémarrer avec sa carte SD d'origine
- Vérifier la version de l'application SmartSense qui tourne
  - Avec le workaround ou sans ?
  - Serait probablement mieux avec le workaround
  - Peut p-e tune sa durée si nécessaire
- Reparcourir mes notes pour voir s'il y a d'autres détails à régler
  - Genre l'heure du noeud
**** DONE LivingFog: Faire revue de code du playbook Ansible avec Guillermo
CLOSED: [2024-05-27 Mon 14:23]
- A demandé à Guillermo s'il était disponible pour qu'on regarde ensemble ce que j'ai fait pour LivingFog
- A réunion avec lui <2024-05-27 Mon> à 10:15
**** DONE SmartSense: Préparer le voyage
CLOSED: [2024-05-27 Mon 14:33]
- Revoir un peu l'adresse du point de RDV
- Et comment s'y rendtre depuis la gare
- Voir aussi à quelle heure je dois partir de chez moi si je veux choper le train
**** DONE SmartSense: Préparer le discours de la démo
CLOSED: [2024-05-27 Mon 16:26]
- Revoir les documents que m'avait transmis Mickaël au sujet du noeud SmartSense
- Et mettre en place le discours a répéter lors de la démo
  - Qui sommes-nous ?
  - Qu'est-ce qu'on fait ?
  - Pourquoi on le fait ?
  - Comment on le fait ?
  - Avancement du projet
  - Perspectives
- Et le cas échéant, recontacter Olivier et/ou Mickaël pour obtenir infos/détails supplémentaires
**** DONE SmartSense: Tester la mise en place du trépied
CLOSED: [2024-05-31 Fri 09:30]
- Mickaël m'a passé un trépied comme support pour le noeud SmartSense pour la démo
- Voir pour monter le noeud dessus
- De façon à voir comment mettre en place le noeud
- Et les outils nécessaires pour cela
**** IN-PROGRESS SmartSense: Rédiger un fichier de config Ansible pour le noeud SmartSense
- Maintenant que j'ai pu déployer un noeud SmartSense et identifier les différentes étapes nécessaires
- Serait utile de
  - Documenter le processus
  - L'automatiser
- Ansible est semblerait l'outil adapté pour cela
- Automatiser le déploiement d'un noeud SmartSense en utilisant Ansible
**** IN-PROGRESS SmartSense: Mise en place d'un processus de CI
- Mettre en place des tests, c'était un bon début
- Mettre en place un processus de CI, ça serait mieux
  - Garantirait la reproductibilité de l'environnement de tests
  - Et que j'oublie pas de les faire tourner avant l'intégration
- Voir comment GitLab gère ça
*** Done
- LivingFog: Faire revue de code du playbook Ansible avec Guillermo
  - Repo : https://gitlab.inria.fr/smartsense/smartobs/setup-livingfog
  - Questions de mon côté
    - [ ] Comment organiser un playbook avec des tâches conditionnelles ?
      - J'ai des tâches qui concernent uniquement les machines ARM, d'autres uniquement les machines x86
      - Le mélange des deux au sein d'un playbook me paraît le complexifier inutilement
      - Comment organiser cela correctement ?
    - [ ] Comment mettre en place les différents use cases ?
      - Veut pouvoir
        - Mettre en place le cluster k8s (master + workers)
        - Faire rejoindre de nouveaux workers
        - Faire quitter des workers
        - Déployer des applications
        - Supprimer des applications
        - Détruire le cluster k8s complet (master + workers)
      - Comment permettre cela de manière claire et élégante ?
        - Et en minimisant la surface d'erreur de manips possibles
      - Est-ce bien la façon d'utiliser Ansible ?
    - [ ] Comment gérer les différents inventaires ?
      - J'ai un même playbook pour setup un cluster k8s sur VMs et sur RPis
      - Avec des variables qui diffèrent en fonction du mode d'exécution
        - Une variable `mode` à fournir à l'exécution, soit `dev` soit `test`
      - Mais je dois aussi utiliser un fichier d'inventaire différent en fonction de ce mode
      - Est-ce possible de lier l'inventaire utiliser à une variable ?
      - Ou une variable à un inventaire ?
    - [ ] Comment gérer les dépendances des modules Ansible ?
      - Certains modules Ansibles nécessitent d'installer des packages Python sur les machines hôtes
      - Peut les installer dans un virtual env
      - Mais ne vois pas comment spécifier à Ansible d'utiliser ce virtualenv
      - Des idées ?
  - Notes
    - Utilisation de rôles pour décomposer en plusieurs playbooks
    - Ajouter des tags pour gérer plus finement quoi exécuter
    - Préférer l'utilisation de modules ansible que shell
      - Pas pu m'aider par contre sur l'utilisation d'env virtuels python pour gérer les dépendances des modules
    - Se pencher sur les inventories en YAML
      - Permet de gérer les variables avec les hosts
      - Permettra p-e de coupler de manière plus propre les variables de chaque env de déploiement avec les machines correspondantes
    - M'a donné accès à un repo de Matthieu
      - Voir pour reproduire structure et pratiques
    - Ainsi qu'aux slides d'une formation du SED sur Ansible
      - Se pencher dessus
- SmartSense: Remettre en fonctionnement le noeud
  - [X] Déployer version antérieure du code
  - [X] Réinitialiser la DB
  - [X] Répertorier les commandes utiles pour la démo
    - [X] Se connecter au noeud
      - ssh pi@192.168.1.2
    - [X] Set la date du noeud
      - sudo timedatectl set-time 'yyyy-mm-dd hh:mm:ss'
      - sudo systemctl restart mainApp
    - [X] Réinitialiser la DB
      - Se connecter à influx
        - Recherche via l'historique en utilisant le mot clé password aide bien
      - drop database SensorData
    - [X] Suivre les logs de mainApp
      - journalctl -f -u mainApp
    - [X] Observer les données collectées
      - Accéder à http://192.168.1.2:3000
  - [X] Sélectionner la fréquence d'export de la DB
    - Par défaut, export toutes 50k valeurs
    - Sachant qu'on produit 100 valeurs toutes les ~5s
      - i.e. 20 val/s
    - Donc atteint le seuil après 2500s, ~40min
    - Raccord avec ce que j'ai pu observer ce matin
    - A passé le seuil pour export toutes les 100k valeurs
    - A pu stopper mainApp, modifier la valeur, redémarrer influxDB et relancer mainApp sans soucis particulier
- SmartSense: Préparer le voyage
  - A lieu à Maison Jean Kuntzmann, 110 Rue de la Chimie, 38400 Saint-Martin-d'Hères
  - Accessible depuis la gare en prenant le tram B
    - Peut payer directement via CB dans le tram
    - Ou acheter ticket à borne
    - 2€ pour le voyage
    - Devrais en avoir besoin que d'un, le bus nous redépose à la gare le jeudi
  - Tram B direction Gières, Plaine des Sports
    - Arrêt Bibliothèques Universitaires
  - Bus à 06:50
    - Comme ça j'ai encore des options si je le rate
- SmartSense: Préparer le discours de la démo
  - Équipe Taran (anciennement Cairn) basée aux centres de recherches Inria de Rennes et Lannion
  - Depuis circa 2017, développe avec partenaires industriels le noeud SmartSense
    - 3D Ouest pour la partie logicielle
    - Feichter Electronics pour la partie hardware
  - But initial était de concevoir et proposer un capteur pour application de Smart Building
    - Suivi de la consommation électrique
      - Associé à des prises connectées
      - Monitore la consommation électrique des différents équipements
      - Permet par exemple de détecter consommation anormale d'équipements (défaillance, utilisation incorrecte...)
      - En utilisant des techniques de Non-Intrusive Appliance Load Monitoring
  - Mais carte enrichie de nombreux capteurs
    - Qualité de l'air
      - CO2, Composés Organiques Volatils (COV ou Volatile Organic Compounds, VOCs), température, humidité, pression
    - Caméras
      - Simple et thermiques
    - Microphones
      - 4, permettant ainsi spatialisation
    - Accéléromètre
    - Télémètre
      - Détection d'obstacle à 4m, précision au cm
  - Qui ouvrent la voie à de nouvelles applications
    - En matière de santé et/ou sécurité
  - Et d'un micro-ordinateur, une RPi
    - Qui permet de faire directement des traitements sur les données
    - E.g. uniformiser leur format avant de les remonter au serveur centralisé
  - Sur cette base, travaille à présent sur un noeud autonome pour utilisation dans milieux naturels
    - Mais implique un peu de boulot
  - Efforts pour rendre le matériel compatible à ce nouveau type d'environnement et ses conditions
    - Tropicalisation
    - Mise en place d'une carte externe pour les capteurs nécessitant d'être au contact de l'air libre
  - Mais d'autres effort restent à fournir
    - Alimentation
      - Mise en place d'une batterie
        - Mais comment la recharger ?
      - Et rationner l'énergie en cas de quantité limitée ?
    - Gestion des données
      - Ne peut plus remonter les données à un serveur via cable Ethernet
      - Besoin de mettre en place télécommunications
        - Wi-Fi HaLow, x00m outdoor, débit variant du Ko/s au Mo/s en fonction des conditions
      - Souhaite stocker les données en local et commencer à les traiter avant de les remonter
        - e.g. plutôt que d'envoyer le flux audio ou les images, juste transmettre les évènements détectés tel que la présence d'un animal
  - Projet est de coupler ce(s) noeud(s) avec une plateforme de calcul déployée elle aussi au sein du milieu naturel
    - La plateforme LivingFog, développée au sein de l'équipe Magellan à Inria Rennes
    - Permet de centraliser les données récupérées par ensemble de capteurs
    - D'offrir puissance de calculs plus conséquente
    - Et de faire le pont avec le monde extérieur
    - Mais qui elle aussi rencontre les mêmes contraintes, notamment d'un POV énergie
    - Cadre de la thèse d'Ammar Kazem
  - Serait intéressant d'avoir votre avis sur ce type de capteurs all-in-one
    - Et vos besoins par rapport aux capteurs/plateformes de calculs sur place
  - Perspectives
    - Déploiement du noeud SmartSense sur le campus de Beaulieu de l'Université de Rennes
      - Afin de vérifier son bon fonctionnement/bonnes mesures
      - Et de tester la partie ajout d'une interface Wi-Fi
    - Déploiement de la plateforme de calculs LivingFog au sein de l'Observatoire de la rivière Kaligandaki au Népal
      - But serait de l'utiliser pour le monitoring de l'observatoire existant
        - i.e. états des capteurs
      - Assurer la qualité des données remontées
        - i.e. vérifier que les données sont cohérentes
      - Et proposer un système d'alerte le cas échéant
- SmartSense: Préparer le matériel
  - [X] Noeud SmartSense
  - [X] Chargeur batterie
  - [X] Petit cable batterie <-> noeud
  - [X] Alim du noeud
  - [X] Trépied (support + vis)
  - [X] Clés Allen 1 et 2
  - [X] PC
  - [X] Chargeur PC
  - [X] Cable Ethernet
  - [X] Adaptateur USB-C vers Ethernet
- TerraForma: Flux de données
  - But est d'intégrer les flux de données du projet à l'existant national
    - Actuellement, données remontent aux institutions locales pour traitement
    - Puis aux pôles de données/aux registres européens
    - Données peuvent néanmoins remonter directement aux pôles si pas possible de les traiter au niveau local
  - Dans TF, une nouvelle voie se rajoute
  - Idée est ensuite de créer des passerelles entre différents niveaux
  - Donnée chaude vs froide
    - Donnée chaude est une donnée très utilisée
      - Peut être utilisée pour monitorer état du système
    - Donnée froide donnée archivée
      - Permettent généralement de produire des "produits élaborés"
  - Dans e-Infrastuctures, progresse de données chaudes à données froides
  - Dans TF, gère principalement des données chaudes
  - Architecture proposée
    - Serveur local déployé sur site qui va récupérer la donnée brute puis remontée la donnée structurée au cloud TF
    - En parallèle, d'autres sites peuvent remonter les données directement aux serveurs nationaux/cloud TF
  - Big Data
    - Critères des 5V
      - Volume, Variété, Vitesse, Valeur, Véracité
    - Dans notre cas, pas si vrai sur l'aspect Volume
    - Mais retrouve in fine les mêmes problématiques que le Big Data
  - Données chaudes/froides
    - Ne va pas chercher à offrir la même QoS (vitesse de réponse) pour l'accès à une donnée chaude qu'à une donnée froide
  - Ingestion des données
    - Plutôt prendre l'approche des données non-structurées/semi-structurées
    - Vois ensuite comment les rendre exploitables
  - Parcours vers les données froides
    - Landing Zone
      - Va nettoyer, filtrer et aggréger les données reçues
      - Avant de les transmettre à la staging zone
    - Staging Zone
      - Continuer le process de nettoyage des données
      - Mais avec des données de base plus propres/contrôlées
    - Analytics Sandbox
    - Curated Data Zone
      - Données validées et exploitables
  - Besoin d'enrichir les données
    - Ajout de métadonnées pour la rendre exploitable
  - Recommande l'utilisation de thesaurus pour préciser le data type d'une donnée (dbpedia)
  - Bonnes pratiques pour TF
    - Faciliter interprétation et réutilisation
      - En métadonnées
        - Date au format ISO avec décalage horaire
        - Unité
        - Type de la donnée
        - Lieu de déploiement (appName)
      - Mettre à disposition documentation sur formatage des données
    - Minimiser métadonnées nonobstant
      - Coût en énergie/calculs
      - Introduit des coûts de maintenance/d'évolution
    - Utiliser des noms de champ fixes
      - Par ex, un champ "valeur", un champ "unité"
      - Et un champ "type" qui précise la donnée manipulée
      - Au lieu d'avoir un champ "airHumidity-val", "airTemperature-val", "airTemperature-unit"
  - Dans leur exemple, enrichisse les données à plusieurs endroits du flux
- TerraForma: Récap
  - Démo SmartSense
    - Aucun dysfonctionnement du noeud
    - Intérêt pour la partie "carte d'extension du capteur"
    - Problématique des traitements en local partagée avec d'autres capteurs
      - Silvert Gousset et Samuel Barnola de l'Institut de Planétologie et d'Astrophysique de Grenoble (IPAG)
      - Travaillent sur une caméra spectrometre
      - Génèrent un ensemble d'images, chacune capturant un spectre précis
      - Se retrouvent avec des dizaines de Mo de données à chaque prise d'échantillon
      - Souhaiteraient appliquer des traitements sur les clichés pour en extraire seulement les données utiles
    - Remarque sur l'accès aux données en local
      - Appréciaient qu'on puisse observer les données directement
      - Mais préfereraient connexion sans-fil
        - e.g. par jour de mauvais temps
    - Remarque sur la partie transmission de données
      - Pas besoin d'avoir une connexion permanente/un équipement toujours en marche
      - Mais pourrait seulement transmettre les données par intermittence
      - Remonter les données 1 fois par jour permettrait déjà de suivre que
        - L'ensemble des capteurs du noeud collectent des données
        - Les données ne sont pas aberrantes
      - Peut même mettre en place plusieurs technologies de transmission adaptées à différents use cases
        - LoRa pour données numériques
        - Wi-Fi HaLow ou autre techno pour flux plus important
  - Wi-Fi HaLow
    - Olivier Berder, chef d'équipe de Granit, m'a expliqué être en train de rapporter sur la thèse de Sebastien Maudet
    - Thèse porte sur l'évaluation de la techno Wi-Fi HaLow
      - e.g. https://www.sciencedirect.com/science/article/pii/S2542660523002809
    - Appartient au Département Réseaux et Télécommunications de l'Université de Nantes
    - Voir ses travaux et p-e entrer en contact avec eux
  - Wi-SUN
    - Olivier Berder m'a aussi présenté Samir El Rhaz, un doctorant de l'équipe Granit
    - Étudie/travaille avec Wi-SUN, 802.15.4g
    - M'ont décrit ça comme une évolution de Zigbee adaptée à des situations outdoor/long distance
    - Voir ce que j'arrive à trouver sur le sujet
  - Données collectées
    - Présentations portées par David Sarramia et Jérémy Mezhoud
      - Travaillent sur le CEBA
        - https://www.mdpi.com/1424-8220/22/7/2733
    - Portent une grande attention aux données collectées
    - De façon à les rendre exploitables à terme par des personnes externes au projet
      - e.g. spécifier l'unité utilisée pour chaque valeur mesurée au sein de la donnée
      - Mentionnaient de suivre les schémas proposés par INSPIRE ou l'utilisation de thesaurus
    - Considèrent aussi important de conserver une copie de la donnée originale
      - Avant décodage et/ou enrichissement
      - De façon à pouvoir reproduire la donnée finale, en cas de détection d'un bug dans le pipeline
      - Ou même de pouvoir auditer régulièrement le pipeline
    - Serait intéressant de récupérer leurs slides sur les bonnes pratiques présentées et recommandées
    - Et de voir lesquelles on pourrait mettre en place à notre échelle
    - Ammar m'a parlé de l'existence d'un Resana TF
      - Devrais demander les accès pour récupérer les slides intéressantes
  - Contraintes sur le terrain
    - Autant on peut mettre en place des workarounds pour certaines
      - Précipitations/neiges
        - Quelques mètres de neige par endroit au Jardin du Lautaret
        - Peut élever le noeud
      - Faune
        - Quelques vaches par endroit au Jardin du Lautaret
        - Peut mettre grillage/cloture autour du noeud
    - Autant on a pas de solutions low-tech pour d'autres
      - Température
        - Semblerait que les batteries n'aiment pas le froid
        - Emmanuel m'explique que ça provoque en effet des fuites d'énergie
  - Résilience/pérennité du système
    - Souhaite collecter des données, de préférence ininterrompue, sur plusieurs années
      - Besoin d'observer les transitions, les changements de saisons, les points de bascule
      - Besoin d'observer les répétitions/l'évolution d'une année à l'autre
    - Est-ce que notre noeud répond ou a vocation de répondre à ces problématiques ?
    - Ou a-t-il un autre use case ?
      - Mentionnait l'utilisation de capteurs pour des sessions d'une journée
      - À quel type de projet de recherche cet use case correspond ?
  - Questions persos
    - Projet Open-Source ?
      - Est-ce que le noeud SmartSense a vocation à être passé en open-source à un moment ?
      - Que ça soit d'un POV logiciel, matériel et données ?
    - Limitation de LoRa
      - Est-ce qu'on peut saturer un réseau LoRa ?
      - i.e. y a-t-il une limite physique théorique au flux de données que l'on peut remonter
      - Et est-elle une contrainte possible des observatoires ?
        - Ou faudrait un nombre de capteurs irréaliste pour l'atteindre ?
** Semaine du <2024-05-21 Tue> au <2024-05-24 Fri>
*** Planned
**** DONE LivingFog: Mettre à jour le playbook Ansible
CLOSED: [2024-05-23 Thu 11:36]
- Maintenant que j'ai identifié les premières étapes nécessaires pour setup un cluster k8s sur le PicoCluster
- Devrais documenter/automatiser cela via un playbook
- Devrais aussi retester cela sur des VMs
  - Attribué à la config réseau de Vagrant les erreurs que je rencontrais
  - Mais probable qu'elles étaient plutôt liées à ma config incomplète de containerd
- Me permettrait de valider cette première étape
- Et d'offrir une base stable pour la suite
  - Configuration de Flannel, Helm, Dashboard, GlusterFS
**** DONE SmartSense: Remonter le noeud
CLOSED: [2024-05-23 Thu 17:47]
- Pour tester la nouvelle image système, avais démonté le noeud SmartSense
- Afin de préparer au voyage aux journées TF de la semaine pro, remonter le noeud
**** DONE SmartSense: Tester le fonctionnement du noeud sur batterie
CLOSED: [2024-05-24 Fri 13:20]
- Toujours dans le but de préparer la démo
- Serait préférable en amont de faire une session de test du noeud sur batterie
- De façon à vérifier son bon fonctionnement dans ces conditions pour la fenêtre de temps de la démo
**** DONE SmartSense: Lister le matériel à emmener
CLOSED: [2024-05-24 Fri 13:28]
- En jouant avec le trépied, a remarqué qu'il me faudrait des clés Allen sur place pour mettre en place le noeud
- Histoire de rien oublier d'autre, serait bien de faire une liste de tout ce dont j'ai besoin pour la démo
**** IN-PROGRESS SmartSense: Remettre en fonctionnement le noeud
- Ça commence à faire un moment que je n'ai pas travaillé sur le noeud SmartSense
- Encore plus que je l'ai fait tourner dans son état d'origine
- Le redémarrer avec sa carte SD d'origine
- Vérifier la version de l'application SmartSense qui tourne
  - Avec le workaround ou sans ?
  - Serait probablement mieux avec le workaround
  - Peut p-e tune sa durée si nécessaire
- Reparcourir mes notes pour voir s'il y a d'autres détails à régler
  - Genre l'heure du noeud
**** IN-PROGRESS SmartSense: Tester la mise en place du trépied
- Mickaël m'a passé un trépied comme support pour le noeud SmartSense pour la démo
- Voir pour monter le noeud dessus
- De façon à voir comment mettre en place le noeud
- Et les outils nécessaires pour cela
**** IN-PROGRESS SmartSense: Préparer le discours de la démo
- Revoir les documents que m'avait transmis Mickaël au sujet du noeud SmartSense
- Et mettre en place le discours a répéter lors de la démo
  - Qui sommes-nous ?
  - Qu'est-ce qu'on fait ?
  - Pourquoi on le fait ?
  - Comment on le fait ?
  - Avancement du projet
  - Perspectives
- Et le cas échéant, recontacter Olivier et/ou Mickaël pour obtenir infos/détails supplémentaires
**** IN-PROGRESS SmartSense: Rédiger un fichier de config Ansible pour le noeud SmartSense
- Maintenant que j'ai pu déployer un noeud SmartSense et identifier les différentes étapes nécessaires
- Serait utile de
  - Documenter le processus
  - L'automatiser
- Ansible est semblerait l'outil adapté pour cela
- Automatiser le déploiement d'un noeud SmartSense en utilisant Ansible
**** IN-PROGRESS SmartSense: Mise en place d'un processus de CI
- Mettre en place des tests, c'était un bon début
- Mettre en place un processus de CI, ça serait mieux
  - Garantirait la reproductibilité de l'environnement de tests
  - Et que j'oublie pas de les faire tourner avant l'intégration
- Voir comment GitLab gère ça
*** Done
- LivingFog: Mettre à jour le playbook Ansible
  - Étapes
    - [ ] Assert qu'on est sur un debian bookworm, soit arm64 ou x86_64
    - [X] Enable le cgroup memory
      - Pas si straightforward
      - Le module lineinfile n'a pas l'air d'avoir été très adapté pour
      - Peut facilement détecter la présence ou non du pattern
      - Mais peut difficilement modifier la ligne pour l'ajouter
      - Solution un peu complexe
        - https://stackoverflow.com/questions/61972815/ansible-modify-cmdline-txt-on-raspberry-pi
      - Me paraît plus simple d'utiliser une commande shell
      - Alex m'a aidé à réaliser la cmd appropriée
        - `sed -i 'cgroup_enable=memory/!s/$/ cgroup_enable=memory' /boot/firmware/cmdline.txt`
      - Seul problème étant que cela marque comme changé l'état de la machine à chaque exec
      - Faudra p-e décomposer en 2 étapes dans ce cas
    - [X] Ajouter la machine master dans /etc/hosts des workers
    - [X] Mettre en place la config par défaut de containerd et setup containerd pour utiliser le cgroup systemd
      - Peut check si je vois la ligne intéressante dans le fichier
        - SystemdCgroup = true
      - Si non, copie fichier pré-fait
    - [X] Désactiver le swap
    - [X] Prendre en compte l'archi système lors de l'ajout du repo pour Docker
      - Ajout d'une ternary
    - [X] Créer le cluster k8s
      - Peut enregistrer le résultat de la commande `kubeadm init`
      - Notamment pour récupérer le token/la commande permettant aux workers de join
      - Exemple
        #+BEGIN_PLAIN
ok: [vm0] => {
    "init_cluster": {
        "changed": true,
        "cmd": "kubeadm init --pod-network-cidr 10.244.0.0/16             --apiserver-advertise-address 192.168.60.10             --control-plane-endpoint master\n",
        "delta": "0:02:29.268177",
        "end": "2024-05-21 15:17:55.395970",
        "failed": false,
        "msg": "",
        "rc": 0,
        "start": "2024-05-21 15:15:26.127793",
        "stderr": "W0521 15:15:26.767939   10944 checks.go:844] detected that the sandbox image \"registry.k8s.io/pause:3.6\" of the container runtime is inconsistent with that used by kubeadm.It is recommended to use \"registry.k8s.io/pause:3.9\" as the CRI sandbox image.",
        "stderr_lines": [
            "W0521 15:15:26.767939   10944 checks.go:844] detected that the sandbox image \"registry.k8s.io/pause:3.6\" of the container runtime is inconsistent with that used by kubeadm.It is recommended to use \"registry.k8s.io/pause:3.9\" as the CRI sandbox image."
        ],
        "stdout": "[init] Using Kubernetes version: v1.30.1\n[preflight] Running pre-flight checks\n[preflight] Pulling images required for setting up a Kubernetes cluster\n[preflight] This might take a minute or two, depending on the speed of your internet connection\n[preflight] You can also perform this action in beforehand using 'kubeadm config images pull'\n[certs] Using certificateDir folder \"/etc/kubernetes/pki\"\n[certs] Generating \"ca\" certificate and key\n[certs] Generating \"apiserver\" certificate and key\n[certs] apiserver serving cert is signed for DNS names [kubernetes kubernetes.default kubernetes.default.svc kubernetes.default.svc.cluster.local master] and IPs [10.96.0.1 192.168.60.10]\n[certs] Generating \"apiserver-kubelet-client\" certificate and key\n[certs] Generating \"front-proxy-ca\" certificate and key\n[certs] Generating \"front-proxy-client\" certificate and key\n[certs] Generating \"etcd/ca\" certificate and key\n[certs] Generating \"etcd/server\" certificate and key\n[certs] etcd/server serving cert is signed for DNS names [localhost master] and IPs [192.168.60.10 127.0.0.1 ::1]\n[certs] Generating \"etcd/peer\" certificate and key\n[certs] etcd/peer serving cert is signed for DNS names [localhost master] and IPs [192.168.60.10 127.0.0.1 ::1]\n[certs] Generating \"etcd/healthcheck-client\" certificate and key\n[certs] Generating \"apiserver-etcd-client\" certificate and key\n[certs] Generating \"sa\" key and public key\n[kubeconfig] Using kubeconfig folder \"/etc/kubernetes\"\n[kubeconfig] Writing \"admin.conf\" kubeconfig file\n[kubeconfig] Writing \"super-admin.conf\" kubeconfig file\n[kubeconfig] Writing \"kubelet.conf\" kubeconfig file\n[kubeconfig] Writing \"controller-manager.conf\" kubeconfig file\n[kubeconfig] Writing \"scheduler.conf\" kubeconfig file\n[etcd] Creating static Pod manifest for local etcd in \"/etc/kubernetes/manifests\"\n[control-plane] Using manifest folder \"/etc/kubernetes/manifests\"\n[control-plane] Creating static Pod manifest for \"kube-apiserver\"\n[control-plane] Creating static Pod manifest for \"kube-controller-manager\"\n[control-plane] Creating static Pod manifest for \"kube-scheduler\"\n[kubelet-start] Writing kubelet environment file with flags to file \"/var/lib/kubelet/kubeadm-flags.env\"\n[kubelet-start] Writing kubelet configuration to file \"/var/lib/kubelet/config.yaml\"\n[kubelet-start] Starting the kubelet\n[wait-control-plane] Waiting for the kubelet to boot up the control plane as static Pods from directory \"/etc/kubernetes/manifests\"\n[kubelet-check] Waiting for a healthy kubelet. This can take up to 4m0s\n[kubelet-check] The kubelet is healthy after 501.995167ms\n[api-check] Waiting for a healthy API server. This can take up to 4m0s\n[api-check] The API server is healthy after 5.002756911s\n[upload-config] Storing the configuration used in ConfigMap \"kubeadm-config\" in the \"kube-system\" Namespace\n[kubelet] Creating a ConfigMap \"kubelet-config\" in namespace kube-system with the configuration for the kubelets in the cluster\n[upload-certs] Skipping phase. Please see --upload-certs\n[mark-control-plane] Marking the node master as control-plane by adding the labels: [node-role.kubernetes.io/control-plane node.kubernetes.io/exclude-from-external-load-balancers]\n[mark-control-plane] Marking the node master as control-plane by adding the taints [node-role.kubernetes.io/control-plane:NoSchedule]\n[bootstrap-token] Using token: xopq34.xkhh08e3vk3tglb9\n[bootstrap-token] Configuring bootstrap tokens, cluster-info ConfigMap, RBAC Roles\n[bootstrap-token] Configured RBAC rules to allow Node Bootstrap tokens to get nodes\n[bootstrap-token] Configured RBAC rules to allow Node Bootstrap tokens to post CSRs in order for nodes to get long term certificate credentials\n[bootstrap-token] Configured RBAC rules to allow the csrapprover controller automatically approve CSRs from a Node Bootstrap Token\n[bootstrap-token] Configured RBAC rules to allow certificate rotation for all node client certificates in the cluster\n[bootstrap-token] Creating the \"cluster-info\" ConfigMap in the \"kube-public\" namespace\n[kubelet-finalize] Updating \"/etc/kubernetes/kubelet.conf\" to point to a rotatable kubelet client certificate and key\n[addons] Applied essential addon: CoreDNS\n[addons] Applied essential addon: kube-proxy\n\nYour Kubernetes control-plane has initialized successfully!\n\nTo start using your cluster, you need to run the following as a regular user:\n\n  mkdir -p $HOME/.kube\n  sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config\n  sudo chown $(id -u):$(id -g) $HOME/.kube/config\n\nAlternatively, if you are the root user, you can run:\n\n  export KUBECONFIG=/etc/kubernetes/admin.conf\n\nYou should now deploy a pod network to the cluster.\nRun \"kubectl apply -f [podnetwork].yaml\" with one of the options listed at:\n  https://kubernetes.io/docs/concepts/cluster-administration/addons/\n\nYou can now join any number of control-plane nodes by copying certificate authorities\nand service account keys on each node and then running the following as root:\n\n  kubeadm join master:6443 --token xopq34.xkhh08e3vk3tglb9 \\\n\t--discovery-token-ca-cert-hash sha256:90a6af24fcb37710b056db9d3c743f4bd73f7bef0a204db7d123c9a9a3abaf08 \\\n\t--control-plane \n\nThen you can join any number of worker nodes by running the following on each as root:\n\nkubeadm join master:6443 --token xopq34.xkhh08e3vk3tglb9 \\\n\t--discovery-token-ca-cert-hash sha256:90a6af24fcb37710b056db9d3c743f4bd73f7bef0a204db7d123c9a9a3abaf08 ",
        "stdout_lines": [
            "[init] Using Kubernetes version: v1.30.1",
            "[preflight] Running pre-flight checks",
            "[preflight] Pulling images required for setting up a Kubernetes cluster",
            "[preflight] This might take a minute or two, depending on the speed of your internet connection",
            "[preflight] You can also perform this action in beforehand using 'kubeadm config images pull'",
            "[certs] Using certificateDir folder \"/etc/kubernetes/pki\"",
            "[certs] Generating \"ca\" certificate and key",
            "[certs] Generating \"apiserver\" certificate and key",
            "[certs] apiserver serving cert is signed for DNS names [kubernetes kubernetes.default kubernetes.default.svc kubernetes.default.svc.cluster.local master] and IPs [10.96.0.1 192.168.60.10]",
            "[certs] Generating \"apiserver-kubelet-client\" certificate and key",
            "[certs] Generating \"front-proxy-ca\" certificate and key",
            "[certs] Generating \"front-proxy-client\" certificate and key",
            "[certs] Generating \"etcd/ca\" certificate and key",
            "[certs] Generating \"etcd/server\" certificate and key",
            "[certs] etcd/server serving cert is signed for DNS names [localhost master] and IPs [192.168.60.10 127.0.0.1 ::1]",
            "[certs] Generating \"etcd/peer\" certificate and key",
            "[certs] etcd/peer serving cert is signed for DNS names [localhost master] and IPs [192.168.60.10 127.0.0.1 ::1]",
            "[certs] Generating \"etcd/healthcheck-client\" certificate and key",
            "[certs] Generating \"apiserver-etcd-client\" certificate and key",
            "[certs] Generating \"sa\" key and public key",
            "[kubeconfig] Using kubeconfig folder \"/etc/kubernetes\"",
            "[kubeconfig] Writing \"admin.conf\" kubeconfig file",
            "[kubeconfig] Writing \"super-admin.conf\" kubeconfig file",
            "[kubeconfig] Writing \"kubelet.conf\" kubeconfig file",
            "[kubeconfig] Writing \"controller-manager.conf\" kubeconfig file",
            "[kubeconfig] Writing \"scheduler.conf\" kubeconfig file",
            "[etcd] Creating static Pod manifest for local etcd in \"/etc/kubernetes/manifests\"",
            "[control-plane] Using manifest folder \"/etc/kubernetes/manifests\"",
            "[control-plane] Creating static Pod manifest for \"kube-apiserver\"",
            "[control-plane] Creating static Pod manifest for \"kube-controller-manager\"",
            "[control-plane] Creating static Pod manifest for \"kube-scheduler\"",
            "[kubelet-start] Writing kubelet environment file with flags to file \"/var/lib/kubelet/kubeadm-flags.env\"",
            "[kubelet-start] Writing kubelet configuration to file \"/var/lib/kubelet/config.yaml\"",
            "[kubelet-start] Starting the kubelet",
            "[wait-control-plane] Waiting for the kubelet to boot up the control plane as static Pods from directory \"/etc/kubernetes/manifests\"",
            "[kubelet-check] Waiting for a healthy kubelet. This can take up to 4m0s",
            "[kubelet-check] The kubelet is healthy after 501.995167ms",
            "[api-check] Waiting for a healthy API server. This can take up to 4m0s",
            "[api-check] The API server is healthy after 5.002756911s",
            "[upload-config] Storing the configuration used in ConfigMap \"kubeadm-config\" in the \"kube-system\" Namespace",
            "[kubelet] Creating a ConfigMap \"kubelet-config\" in namespace kube-system with the configuration for the kubelets in the cluster",
            "[upload-certs] Skipping phase. Please see --upload-certs",
            "[mark-control-plane] Marking the node master as control-plane by adding the labels: [node-role.kubernetes.io/control-plane node.kubernetes.io/exclude-from-external-load-balancers]",
            "[mark-control-plane] Marking the node master as control-plane by adding the taints [node-role.kubernetes.io/control-plane:NoSchedule]",
            "[bootstrap-token] Using token: xopq34.xkhh08e3vk3tglb9",
            "[bootstrap-token] Configuring bootstrap tokens, cluster-info ConfigMap, RBAC Roles",
            "[bootstrap-token] Configured RBAC rules to allow Node Bootstrap tokens to get nodes",
            "[bootstrap-token] Configured RBAC rules to allow Node Bootstrap tokens to post CSRs in order for nodes to get long term certificate credentials",
            "[bootstrap-token] Configured RBAC rules to allow the csrapprover controller automatically approve CSRs from a Node Bootstrap Token",
            "[bootstrap-token] Configured RBAC rules to allow certificate rotation for all node client certificates in the cluster",
            "[bootstrap-token] Creating the \"cluster-info\" ConfigMap in the \"kube-public\" namespace",
            "[kubelet-finalize] Updating \"/etc/kubernetes/kubelet.conf\" to point to a rotatable kubelet client certificate and key",
            "[addons] Applied essential addon: CoreDNS",
            "[addons] Applied essential addon: kube-proxy",
            "",
            "Your Kubernetes control-plane has initialized successfully!",
            "",
            "To start using your cluster, you need to run the following as a regular user:",
            "",
            "  mkdir -p $HOME/.kube",
            "  sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config",
            "  sudo chown $(id -u):$(id -g) $HOME/.kube/config",
            "",
            "Alternatively, if you are the root user, you can run:",
            "",
            "  export KUBECONFIG=/etc/kubernetes/admin.conf",
            "",
            "You should now deploy a pod network to the cluster.",
            "Run \"kubectl apply -f [podnetwork].yaml\" with one of the options listed at:",
            "  https://kubernetes.io/docs/concepts/cluster-administration/addons/",
            "",
            "You can now join any number of control-plane nodes by copying certificate authorities",
            "and service account keys on each node and then running the following as root:",
            "",
            "  kubeadm join master:6443 --token xopq34.xkhh08e3vk3tglb9 \\",
            "\t--discovery-token-ca-cert-hash sha256:90a6af24fcb37710b056db9d3c743f4bd73f7bef0a204db7d123c9a9a3abaf08 \\",
            "\t--control-plane ",
            "",
            "Then you can join any number of worker nodes by running the following on each as root:",
            "",
            "kubeadm join master:6443 --token xopq34.xkhh08e3vk3tglb9 \\",
            "\t--discovery-token-ca-cert-hash sha256:90a6af24fcb37710b056db9d3c743f4bd73f7bef0a204db7d123c9a9a3abaf08 "
        ]
    }
}
        #+END_PLAIN
      - Me paraît délicat d'extraire uniquement les deux dernières lignes
    - [X] Déployer le plugin CNI
      - Doit faire de l'interface utilisée un paramètre
      - A trouvé comment faire avec le système de template d'Ansible
      - Rencontre désormais un problème au moment de déployer la configuration
      - Le module Kubernetes pour Ansible a des dépendances
        - https://docs.ansible.com/ansible/latest/collections/kubernetes/core/k8s_module.html#ansible-collections-kubernetes-core-k8s-module
      - Qui ne sont pas installées par défaut sur le noeud master
        #+BEGIN_PLAIN
TASK [Deploy flannel configuration] ********************************************************************************************************************************************************************************************************
fatal: [vm0]: FAILED! => {"changed": false,
       "msg": "Failed to import the required Python library (kubernetes) on master's Python /usr/bin/python3.
               Please read the module documentation and install it in the appropriate location.
               If the required library is installed, but Ansible is using the wrong Python interpreter, please consult the documentation on ansible_python_interpreter"}
        #+END_PLAIN
      - Et pas moyen de les installer simplement
        - PyYAML pas dispo dans apt
      - Retombe sur mon problème d'env virtuel sur la machine qui est setup
      - Voir cette approche ?
        - https://stackoverflow.com/questions/20575084/best-way-to-always-run-ansible-inside-a-virtualenv-on-remote-machines
    - [X] Faire rejoindre aux workers le cluster k8s
      - Suis tombé sur un exemple simple
        - https://github.com/npflan/k8s-ansible/blob/main/kubeadm-join.yml
      - Consiste à utiliser la commande `kubeadm token create --print-join-command`
      - À stocker la commande obtenue comme variable du master
      - Et de la récupérer et de l'exécuter sur les workers
      - Permet ainsi à de nouveau workers de rejoindre le cluster facilement
  - Notes
    - Possible de debug la valeur d'une variable à l'aide d'une tâche
      #+BEGIN_PLAIN
- name: Print all available facts
  ansible.builtin.debug:
    var: cgroup_containerd
      #+END_PLAIN
  - Questions
    - Comment organiser un playbook avec des tâches conditionnelles ?
      - J'ai des tâches qui concernent uniquement les machines ARM, d'autres uniquement les machines x86
      - Le mélange des deux au sein d'un playbook me paraît le complexifier inutilement
      - Comment organiser cela correctement ?
    - Comment mettre en place les différents use cases ?
      - Veut pouvoir
        - Mettre en place le cluster k8s (master + workers)
        - Faire rejoindre de nouveaux workers
        - Faire quitter des workers
        - Déployer des applications
        - Supprimer des applications
        - Détruire le cluster k8s complet (master + workers)
      - Comment permettre cela de manière claire et élégante ?
        - Et en minimisant la surface d'erreur de manips possibles
      - Est-ce bien la façon d'utiliser Ansible ?
    - Comment gérer les différents inventaires ?
      - J'ai un même playbook pour setup un cluster k8s sur VMs et sur RPis
      - Avec des variables qui diffèrent en fonction du mode d'exécution
        - Une variable `mode` à fournir à l'exécution, soit `dev` soit `test`
      - Mais je dois aussi utiliser un fichier d'inventaire différent en fonction de ce mode
      - Est-ce possible de lier l'inventaire utiliser à une variable ?
      - Ou une variable à un inventaire ?
    - Comment gérer les dépendances des modules Ansible ?
      - Certains modules Ansibles nécessitent d'installer des packages Python sur les machines hôtes
      - Peut les installer dans un virtual env
      - Mais ne vois pas comment spécifier à Ansible d'utiliser ce virtualenv
      - Des idées ?
- Réunion projet Distant du <2024-05-22 Wed>
  - Préparation
    - Participant-es
      - Christoff Andermann
      - Laurent Longuevergne
      - Kristen Cook
    - Rivière monitorée : Kaligandaki River
    - Objectifs
      - Remettre en place l'équipement d'observation à Lete
        - Je crois que Christoff s'en est occupé
      - Déployer noeuds Fogs dans l'/les Observatoires du Kaligandaki Catchment (bassin versant) pour
        - Assurer qualité des données
        - Mettre en place des traitements sur place
    - Assurer qualité des données
      - Monitorer état des différents composants de l'observatoires (capteurs, data loggers, équipement réseau)
      - Filtrer les données incohérentes
        - e.g. températures trop basses ou élevées
        - Peut mettre en place algo d'apprentissage pour aller plus loin
      - Générer des alertes précises sur les dysfonctionnements observés
    - Mettre en place des traitements sur place
      - Mécanisme de détection et d'alerte de crues
    - Questions
      - Quelles données doit stocker un noeud Fog ?
        - Mentionne que le noeud Fog doit offrir "a secured data storage and transmission for further archiving and public distribution"
        - À quelles données cela fait référence ?
          - Monitoring ?
          - Données environnementales ?
      - Où se place le noeud dans l'architecture du système ?
        - Dans le schéma, se place en parallèle des data loggers
        - Mais si veut détecter les données erronnées, doit les récuperer aussi
        - Est-ce qu'il se place entre les capteurs et les data loggers ?
      - Comment tester localement ?
        - Dispose-t-on d'équipement pour mettre en place un mini observatoire ici ?
          - Capteurs, data loggers ?
          - A-t-on un noeud Fog outdoor-proof ?
  - Réunion
    - Question principale du CNRS
      - What does it means to have long-term measurements/observatories ?
    - Faudra développer cet aspect lors de notre projet/réponse/rapport
    - Réunion de suivi vers décembre
    - Rapport en novembre
    - Kristen a déjà prévu un séjour au Népal d'octobre à décembre
      - Faudrait organiser notre voyage à cette période
    - Besoins
      - Modifier l'observatoire pour le rendre plus pérenne/substainable
        - Besoins de remote management
        - Existe un remote access, mais via Potsdam
        - Mais support de Potsdam diminue/disparait, besoin d'un nouveau moyen
      - Besoin d'observations continues
        - Long-term
        - Seasonal data
        - Détection des évènements extraordinaires
    - Rivière intéressante
      - Traverse plusieurs zones sédimentaires/altitudes/climats le long de son parcours
      - Région dynamique : activité tectonique, mousson
      - Beaucoup d'interactions entre les différents systèmes
      - Majorité du débit s'infiltre dans le sol
        - Comment cela fonctionne ?
      - Mesures
        - Activités sismiques (débit important par rapport à d'autres flux)
          - But est de traiter ces données en local pour produire une synthèse du résultat obtenu
        - Paramètres du climat
        - Concentration en sédiments
        - Niveau de l'eau
        - Topologie du terrain (voir si le lit de la rivière évolue)
        - Font aussi des coupes d'arbre pour suivre l'impact/la composition des moussons années après années
    - Globalement, observatoire fonctionne correctement majoritairement
      - Mais nécessite un entretien continu
    - Alerte crues
      - En fonction de la distance avec l'origine, fenêtre de temps varie de minutes à heures
    - Besoins
      - Monitorer le système
        - Pour le moment, si système fail, attendre que Christoff check les données pour détecter la panne
        - Dépend donc de la fréquence à laquelle les données de ce sensor
        - Si les données sont juste bogus, peut prendre bien plus de temps à s'en apercevoir
        - Besoin de mettre en place de la redondance
        - Ou mettre en parallèle les données d'un sensor avec celles d'autres sensors (détecter la pluie avec l'activité sismique)
      - Sécurité/pérennité des données
      - Local Apps
      - Routine to monitor the quality of data
        - How to distinguish good/bad data ?
      - 1ère piste serait de mettre en place un mécanisme de détection des données bogus
      - Croix-verte
        - Y a déjà des capteurs mis en place
        - Peut mettre en place des capteurs utilisés au Népal
        - De façon à qu'on puisse interagir/les manipuler
      - Commence à mettre en place une base de données
      - Faudrait se greffer au système sans le remplacer
      - Comment faire pour cross-compare des données de débit/throughput différents ?
      - Dispose déjà d'un dataset des 10 dernières années
        - Avec des données valides et bogus
        - Peut utiliser cela pour l'apprentissage
  - Discussion avec Ammar
    - En débriefant informellement avec Ammar après la réunion
    - M'a tenu des propos particulièrement intéressants
    - IHO la plateforme fog devrait être découplée du système
    - Et seulement permettre des applications répondant au besoin
    - Pose la question de notre rôle
      - Est-ce concevoir et mettre à disposition la plateforme Fog ?
        - Auquel cas dois seulement déterminer comment la plateforme s'intègre et interagit avec le système
        - Mais doit restée générique
        - Et fournir des PoCs
      - Ou de concevoir et mettre en place les applications répondant au besoin ?
        - Dans ce cas, doit pousser plus loin la partie dev
        - Mais peut se permettre d'être plus spécifique au système
    - Mais en bref, devrais seulement reposer sur les flux de données existants dans le système
      - Par exemple monitorer l'état des sensors en vérifiant que le sensor fournit des données
    - Plutôt que d'avoir un rôle actif dans le système
      - E.g. ping régulier des capteurs pour s'assurer de leur fonctionnement
- SmartSense: Remettre en fonctionnement le noeud
  - A retrouvé la carte SD originelle
  - Le noeud fonctionne correctement
  - Pour rappel, peut s'y connecter avec la commande suivante
    - ssh pi@192.168.1.2
  - Commande pour spécifier l'heure
    - sudo timedatectl set-time 'yyyy-mm-dd hh:mm:ss'
  - Juste besoin de redémarrer mainApp ensuite
    - sudo systemctl restart mainApp
  - Fonctionne instant pour le hardware et collectd
  - Mais ne fonctionne pas tout de suite pour les données environnementales
  - Notamment à cause du délai introduit par le buffer
  - À setup au préalable
  - Ah mais c'est parce que c'est une version modifiée par mes soins de `DataToLocalInfluxDB`
  - Avec notamment une taille de buffer augmentée à 5k points
    - Ce qui correspond à ~4min de données
  - Et n'utilisant qu'une DB unique
  - Pourrait ramener cette fréquence de rafraichissement/collecte de données à 15/30s
    - Si 5k => 240s
    - 30s => 625
    - 15 => 310
    - Raccord avec le comportement initial
      - Buffer de 100 points, écriture toutes les ~5s
  - Après si la config initiale était un buffer de 100, pourquoi ne pas rester là-dessus ?
    - Cela fonctionnait
  - [ ] Remettre la version "livrée" de l'application
  - [ ] Réinitialiser la DB
  - [ ] Répertorier les commandes utiles pour la démo
- Réunion Guillaume du <2024-05-23 Thu>
  - A progressé sur la mise en place du cluster k8s sur le PicoCluster
    - Lors de la dernière réunion, avais commencé à le setup sur le PicoCluster après avoir atteint une impasse sur VMs
    - Mais a rencontré la même erreur, et plus encore, sur le PicoCluster
    - Voyant que ce n'était pas lié à l'environnement VMs mais à ma config, a pu creuser et me débloquer sur ce point
    - A réussi à mettre en place le cluster k8s et à vérifier son bon fonctionnement avec une application Hello World
  - Fort de ces connaissances, a pu retester le setup de k8s sur VMs et arriver au bout cette fois-ci
  - A automatisé cette partie là avec un playbook Ansible
    - A pu retester son fonctionnement sur VMs
    - À refaire tourner sur RPis
  - Vois Guillermo lundi matin pour revue de code
    - Histoire d'avoir des retours et conseils sur cette partie
    - Et répondre à quelques
  - Pour la suite
    - Utilisateur connecté à un des workers
    - Objectif suivi
      - Mettre à disposition la plateforme/noeud Fog
      - Mettre en place l'alimentation de données
        - Capteurs écrivent dans un bus de message tournant sur le noeud Fog
      - Mettre en place template pour pods pour piocher dans ce bus
        - Et déterminer la qualité des données
    - Questions
      - Quelle temporalité de données stockées dans le message bus ?
        - Réponse: on considère qu'ils traitent uniquement les données à partir de leur mise en ligne
      - Comment fournir les données au message bus ?
        - Ça va dépendre de leur station/système actuel
        - Pour le moment, générer et fournir ces données à la main au message bus
        - Et se pencher plutôt sur la partie
          - Déploiement d'un message bus
            - Quel message bus ?
          - Déploiement d'un service pour l'exposer aux pods
          - Déploiement d'un pod qui s'y connecte, traite les données et déclenche alerte
    - Estime en avoir pour 2-3 mois
- SmartSense: Tester le fonctionnement du noeud sur batterie
  - A mis la batterie en charge
  - Le chargeur affiche en être au niveau 3/8 de charge
    - 1 étant batterie vide
    - 7 ayant l'air d'être batterie pleine
    - À quoi correspond le 8 du coup ?
      - Alim en continue ?
  - Au bout de ~10/20min, a remarqué qu'a atteint le niveau 6
    - Ça vaut le coup de monter à 7 ?
  - Au bout de ~1h, était toujours au niveau 6
  - A arrêté la charge à ce stade
  - Et mis en route le noeud sur batterie
  - À 10h22 pour être précis
  - Fonctionne parfaitement
  - À voir combien de temps il tourne avec cette configuration
  - Pas moyen de suivre le niveau de charge de la batterie en fonctionnement ?
  - Fonctionne toujours à 13h13
  - L'a arrêté à 13h20
  - Chargeur indique niveau de charge 2
    - Puis au bout de 4min le niveau 3
  - Niveau de charge remonté à 6 au bout ~1h
- SmartSense: Lister le matériel à emmener
  - Noeud SmartSense
  - Chargeur de batterie
    - Me paraît plus prudent de l'avoir avec moi
  - Petit cable batterie <-> noeud
    - Pour pouvoir alimenter le noeud sur batterie
  - Alim du noeud
    - Au cas où la batterie rencontre un défaut?
  - Trépied (son support + vis)
  - Clé Allen 1 pour fixation au trépied
    - Pour les vis qui fixe le noeud SmartSense au support
    - Reste à trouver la bonne clé pour ça
  - Clé Allen 2 pour fixation au trépied
    - Pour la vis qui serre le support du noeud à la base du trépied
    - Plus grand diamètre que la clé précédente
  - Voir avec celles que j'ai à la maison si certaines correspondent
  - Tournevis cruciforme ?
    - Pour les vis de la vitre du noeud SmartSense
    - Je sais pas si j'aurais besoin de retirer/remettre la vitre
    - À méditer au cours du w-e
  - PC
    - Toujours mieux pour interagir avec le noeud
    - Et pour le présenter
  - Chargeur PC
    - Mon chargeur de Mac était compatible de mémoire
    - Sinon je dois avoir le chargeur du HP chez moi
  - Cable Ethernet
    - Pour relier ma machine au noeud SmartSense
  - Adapteur USB-C vers Ethernet
    - Viens de réaliser que je n'ai pas de port Ethernet sur ma machine
    - Besoin d'un adapteur donc
    - Martin m'a prêté son mini-hub
    - Faudra que je teste ça
- SmartSense: Préparer le discours de la démo
  - Présentation partagée par Mickaël
    - Le secteur du batiment (fonctionnement des batiments et constructions) représente 40% des émissions de C02
      - Curieux de la répartition entre fonctionnement et construction
    - Efforts pour réduire ce coût énergétique
    - Smart Building Automation
      - Réduire consommation énergétique des habitats en maintenant le confort des usager-ères
      - Notamment en réagissant aux conditions environnementales/évènements (maison vide)
    - Noeud SmartSense conçu dans cette optique
    - Carte équipée de multiples capteurs
      - Micros
        - 4 micros disposés sur la carte
        - Permettent de capturer le son et de faire de la spatialisation sonore
      - Capteurs qualité de l'air
        - Température, taux d'humidité, CO2, pression
      - Caméra
        - Utilisée nowadays ?
        - Me semble pas avoir observé la moindre photo prise par le noeud SmartSense
        - Après, maintenant que le noeud est enfermé dans le boitier, c'est moins pratique
      - Caméras infrarouge
        - High-Res et Low-Res
      - Accéléromètre
      - Magnétomètre
      - Télémètre
      - Antennes radio
        - < 1Ghz
        - 2.4Ghz (Bluetooth & Wi-Fi)
        - Ultra-Wide Band (UWB)
          - Prévue semblerait pour de la localisation
          - De ce que je vois, c'est de la short-range
          - Un peu de mal à piger dans notre contexte l'usage
          - Détection d'obstacle ?
        - Que je sache, aucune n'est exploitée actuellement
** Semaine du <2024-05-14 Tue> au <2024-05-17 Fri>
*** Planned
**** DONE LivingFog: Déployer LivingFog sur le PicoCluster
CLOSED: [2024-05-16 Thu 16:56]
- Pour prendre en main les différents outils que je vais utiliser dans le projet
- Le mieux est de les utiliser dans le cadre d'un PoC
- Ici, l'idée est de déployer LivingFog sur 2 noeuds du PicoCluster
  - Noeud principal
  - Noeud worker
- Permet aussi de me familiariser avec l'écosystème LivingFog
  - https://github.com/FogGuru/livingfog
  - http://www.fogguru.eu/livingfog/
- De remonter les difficultés/incompréhensions rencontrées
- Et de commencer à identifier ce qui a trait à LoRaWAN
  - Pour une potentielle excision future
**** DONE LivingFog: Déployer un cluster Kubernetes sur VMs
CLOSED: [2024-05-21 Tue 08:53]
- J'avais tenté de mettre en place un cluster k8s sur des VMs
- Mais avais rencontré une erreur lors de `kubeadm join`
- Erreur que j'imputais à l'utilisation de la mauvaise interface réseau par k8s/flannel
- Lors du déploiement sur RPi, j'ai pu découvrir que l'absence de config de containerd pouvait aussi être une cause possible
- J'ai aussi appris comment spécifier l'interface réseau à utiliser à flannel
- Procéder à la mise en place d'un cluster k8s sur VMs, histoire de vérifier et valider le process
**** IN-PROGRESS SmartSense: Rédiger un fichier de config Ansible pour le noeud SmartSense
- Maintenant que j'ai pu déployer un noeud SmartSense et identifier les différentes étapes nécessaires
- Serait utile de
  - Documenter le processus
  - L'automatiser
- Ansible est semblerait l'outil adapté pour cela
- Automatiser le déploiement d'un noeud SmartSense en utilisant Ansible
**** IN-PROGRESS SmartSense: Mise en place d'un processus de CI
- Mettre en place des tests, c'était un bon début
- Mettre en place un processus de CI, ça serait mieux
  - Garantirait la reproductibilité de l'environnement de tests
  - Et que j'oublie pas de les faire tourner avant l'intégration
- Voir comment GitLab gère ça
**** IN-PROGRESS LivingFog: Mettre à jour le playbook Ansible
- Maintenant que j'ai identifié les premières étapes nécessaires pour setup un cluster k8s sur le PicoCluster
- Devrais documenter/automatiser cela via un playbook
- Devrais aussi retester cela sur des VMs
  - Attribué à la config réseau de Vagrant les erreurs que je rencontrais
  - Mais probable qu'elles étaient plutôt liées à ma config incomplète de containerd
- Me permettrait de valider cette première étape
- Et d'offrir une base stable pour la suite
  - Configuration de Flannel, Helm, Dashboard, GlusterFS
*** Done
- Réunion avec Guillaume du <2024-05-14 Tue>, 14h00
  - Préparation
    - LivingFog
      - Fini de parcourir le playbook Ansible et de faire le tour des différents composants de LivingFog
      - Pas trop compris quelques points
        - Incorpore un petit outil *k8s-healthcheck*
          - https://github.com/emrekenci/k8s-healthcheck
          - Monitore l'état des différents composants Kubernetes
            - apiServer, etcd, control plane, deployments, etc.
          - Projet date de 6 ans et semble peu utilisé
          - Toujours d'actualité ?
          - Ou remplacé désormais par un plugin ou une feature native ?
        - Configure Gluster pour avoir 2 replicas par volume
          - Mais ne précise qu'une seule brick par volume
          - ne devrait pas y avoir au moins 2 bricks du coup ?
      - A tenté de lancer le playbook sur le picocluster, sans succès
      - Suis passé sur des VMs Vagrant pour essayer de débugger et gérer plus facilement l'état du système
      - Version de Docker pas compatible avec nouveau OS
        - Semble utiliser cgroup v1
        - Alors que les OS récents ont l'air d'être passé à cgroup v2
      - Préférable d'upgrade la version de Docker
        - Et de passer à un système de conteneurs respectant la Container Runtime Interface (CRI)
        - Plutôt d'installer containerd non ?
      - Erreur lors de l'installation de Kubernetes
        - Rencontre une erreur liée à la dépréciation de apt-key
        - Comprends pas pourquoi je rencontre pas la même erreur pour Docker
      - A aussi MàJ la version de Kubernetes installé
      - Mais du coup a rencontré diverses erreurs
        - Flannel
          - Des propriétés qui sont passés du statut beta au statut officiel
        - Dashboard
          - Ne s'installe plus de la même manière
          - Nécessite Helm
      - A pu setup le cluster sur mes VMs
        - kubeadm init
        - kubeadm join
      - Mais rencontre une erreur à l'utilisation liée au réseau
        - k8s utilise l'interface réseau par défaut
        - Dans le cas de mes VMs, ne correspond pas à l'interface réseau pour laquelle j'ai spécifié les @ip
        - Du coup les noeuds du cluster n'arrivent pas à communiquer
      - Devrait pas rencontrer ce problème sur le picocluster
  - Questions
    - LivingFog
      - CNI
        - Propose plusieurs choix possibles comme CNI
        - Calico, Canal, Flannel (défaut)
        - Quelles sont les différences entre ces outils ?
    - SmartSense
      - A emprunté un dongle Wi-Fi à la DSI pour SmartSense
      - Mais serait bien que je leur rende un jour
      - Est-ce que je peux passer commande d'un ou plusieurs simples dongles Wi-Fi simples ?
    - Népal
      - Des nouvelles d'une réunion de démarrage ?
        - Non, il faudrait
        - Serait bien de faire ça assez vite
      - Si voyage prévu cette année, quelle fenêtre ?
        - Que je ne me fasse pas surprendre
        - Avant l'hiver probablement
      - Quel est le plan ?
        - Doit fournir un rapport d'avancement en fin d'année
        - Doit donc d'ici là avoir de la matière et un début de solution
        - Idée serait de déployer le noeud fog sur le campus
        - Et d'avoir avancer sur ces problématiques
          - Monitoring du système
          - Fonctionnement du noeud fog en milieu naturel
            - Sources d'énergie à disposition
              - Peut mettre en place des panneaux solaires
              - Présence aussi d'une turbine hydraulique au niveau du village
                - Mais si elle produit une quantité d'énergie limitée, faudrait pas non plus piller cette ressource
            - Doit être capable de redémarrer après coupure d'énergie
          - Qualité des données
            - Un simple filtre sur les valeurs pour détecter celles qui sont incohérentes
          - Un objectif suivant du projet est d'alerter quant aux crues
- Réunion de suivi du <2024-05-16 Thu>
  - Préparation
    - SmartSense
      - A mis en place une nouvelle image système basé sur RPi OS bookworm 64bits
      - A dû corriger des bugs introduits lors du refactoring des modules du code
      - Mais a pu lancer l'application sur cette nouvelle image
      - A fait tourner l'appli 24h, sans rencontrer le problème lié à InfluxDB
      - Mais problème avec la carte SD récupérée
        - Extrêmement lente, e.g. plusieurs secondes pour exécuter ls
      - A souhaité setup une seconde carte avant de procéder à des tests plus poussés
      - A permis de faire le point sur les différentes étapes nécessaires pour créer l'image
        - Dépendances à installer
        - Configuration, e.g. de l'interface I2C
      - A commencé à rédiger un playbook Ansible pour automatiser le setup de l'image système
      - Sinon en profiter pour brancher un dongle Wi-Fi et le faire fonctionner
        - TP-Link TL-WN823N, récupéré à la DSI
        - A pu me connecter à INRIA-Interne avec
        - C'était pas clair qu'un driver était disponible pour le chipset de ce dongle
      - Sinon, Raspberry a annoncé la RPi Compute Module 4S
        - Une nouvelle carte au format du CM3
        - Permettrait un boost de perfs sans devoir redesign le noeud SmartSense
        - Notamment niveau RAM, des modèles ont 8Go
    - LivingFog
      - A repris avec Guillaume les travaux sur LivingFog
      - Me penche sur le playbook Ansible utilisé pour setup le cluster k8s
        - Cluster sur lequel tournent ensuite les applis
      - Playbook date de plusieurs années
      - Et n'est plus particulièrement à jour
        - Paquets, versions, configuration
      - Reprend from scratch le setup
      - Là a mis en place le cluster Kubernetes sur le picocluster
      - Vois à présent pour automatiser ces premières étapes
      - Et pour réintroduire les composants suivants dans le système
    - TerraForma
      - Pour rappel, les journées TF ont lieu du <2024-05-28 Tue> au <2024-05-30 Thu>
      - Y présenterait le noeud SmartSense dans le cadre d'une démo
      - Échangé avec Mickaël au sujet des démos/présentations précédentes
        - Et récupéré du contenu (présentations, commandes utiles)
      - Creuse cela ces prochains jours
  - Questions
    - Antenne Wi-Fi
      - Des nouvelles de la DSI à propos de la mise en place de cet équipement ?
  - Notes
    - Connexion Wi-Fi
      - Serait intéressant de pouvoir se connecter à un réseau sans login/mdp
      - Pourrait utiliser le réseau robotique
        - Réseau ad-hoc géré pour les drones
      - Voir avec eux le réseau qu'ils ont mis en place, comment, et pour quelles raisons/usages
      - Et voir si on peut en tirer des enseignements
- Kubernetes
  - Pods and Containers - Kubernetes Networking | Container Communication inside the Pod
    - https://www.youtube.com/watch?v=5cNrTU6o3Fw
  - Container Network Interface (CNI) Simplified | Kubernetes Networking | Pod Security Group
    - https://www.youtube.com/watch?v=kA0C44nTwjU
    - CNI est l'interface entre la Container Runtime Interface (CRI) et le network
    - Configure les routes réseau
      - Globalement, quand CRI ajoute/retire un container (et donc pod), demande à la CNI d'ajouter le pod au réseau
    - Tous les pods peuvent communiquer entre eux w/o Network Address Translation (NAT)
    - Tous les nodes peuvent communiquer avec les pods w/o NAT
    - Pour gérer le multi-tenant, doit mettre en place des Kubernetes Network Policies
      - Si je veux pas que mes pods puissent aller fouiller ceux du voisin
      - S'applique sur des pods selectionnés grâce aux labels
      - Policies décomposées en 2 parties
      - ingress gère les connexions entrantes
      - egress les connexions sortantes
      - Peut spécifier les adresses d'entrée/sortie et les tags des pods que doivent avoir les pods correspondants
      - e.g. seuls les pods du namespace myproject ayant pour rôle frontend ont le droit de contacter mes pods db
    - Certaines CNIs ont l'air de proposer la Pod Security Group
      - Permet de définir une police de sécurité au niveau d'un pod donné
      - Mais comprend pas trop par qui ces polices sont établies
      - Parle que c'est une feature pour le multi-tenant
      - Est-ce que c'est à l'owner de la plateforme de les mettre en place ?
        - Mais dans ce cas, doit connaître les détails des applications qui tournent sur la plateforme
      - Est-ce que c'est aux users de la plateforme de les mettre en place ?
        - Mais dans ce cas, s'applique aux pods des autres tenants ?
        - Si oui, possibilité de deny une ressource aux autres
    - Différences entre Service Mesh et Network Policy Enforcer
      - Service Mesh se place au niveau de la couche application
      - Tandis que Network Policy Enforcer au niveau de la couche réseau
      - Les deux font office de load balancer, mais pour des entités différentes
        - Après, ne vois pas trop ce qu'est le Service Mesh pour le moment
  - Kubernetes Services explained | ClusterIP vs NodePort vs LoadBalancer vs Headless Service
    - https://www.youtube.com/watch?v=T4Z7visMM4E
    - Pods ont une @IP
    - Mais sont éphémères
    - Ne peut pas rely sur ces @IP
    - Services permettent de fournir une @IP stable
    - Permettent aussi de faire du load balancing
    - Plusieurs types de Services proposés par Kubernetes
    - ClusterIP
      - Default type
      - Remplit le rôle d'un service interne
      - Cluster k8s dispose d'une front-end, Ingress
      - Lorsque requête arrive, Ingress redirige cette dernière vers service approprié
        - Utilise des règles définies via configuration pour déterminer à quel service déléguer la requête
      - Service se base alors sur le selector défini pour déterminer à quel pod envoyer la requête
      - Pods doivent avoir déclaré les labels correspondants
      - Utilise alors le targetPort déclaré pour déterminer sur quel port contacter le pod sélectionné
      - Si Service doit gérer des requêtes destinées à des ports différents des pods
        - e.g. pod MongoDB qui expose un container MongoDB mais aussi un container de metrics
      - Peut alors lui-même ouvrir plusieurs ports et accepter des requêtes sur les différents ports
    - Headless
      - Client/Pod veut communiquer directement avec un Pod spécifique
      - e.g. stateful applications
        - Pod exposant une BD qui démarre et qui doit récupérer l'état depuis le pod précédent (chain replication)
      - Pour créer un tel service, se base sur la config de ClusterIP
      - Juste on précise que ce n'est pas un clusterIP Service
        - `clusterIP=None`
      - Va alors créer le service as Headless
    - NodePort
      - Ici, créé et expose le service directement au niveau du noeud
        - i.e. pas seulement accessible en interne, noeuds peuvent être directement contactés de l'extérieur du réseau
      - Configuration nécessite de spécifier le port du endpoint, targetPort, du service lui-même, port, et du noeud, nodePort
      - Warning: valeur de nodePort doit être l'intervalle 30000-32767
      - Approche considérée inefficace et non-secure
      - Pas utilisée en production, même pour permettre les connexions externes
        - Plutôt en dev pour tester rapidement un service
      - Repose sur Ingress ou un service LoadBalancer si nécessaire
    - LoadBalancer
      - Service devient accessible depuis l'extérieur du cluster par le biais d'un load balancer d'un cloud provider
        - i.e. type spécifique à l'environnement cloud
    - Notes
      - Endpoint
        - Quand met en place un Service, k8s créé un objet Endpoint correspondant
        - Permet de lister et référencer pour chaque service les actual endpoints
- LivingFog: Déployer LivingFog sur le PicoCluster
  - Quelques erreurs rencontrées
  - Ajout du cgroup memory
    - Ajout de `cgroup_enable=memory` dans /boot/firmware/cmdline.txt
    - https://forums.raspberrypi.com/viewtopic.php?t=203128
    - Doit reboot dans la foulée
  - Permet à la commande kubeadm init de réussir
  - Mais cluster plante ensuite
  - Commandes kubectl échouent
    - Erreur XXX:6443 Connection refused
    - Le serveur API, qui tourne en local sur le noeud, qui plante régulièrement semblerait
  - Comprend pas particulièrement d'où vient l'erreur
  - Configuration de flannel pour utiliser l'interface eth0 des noeuds
    - Ajout de l'argument `--iface=eth0` dans la configuration de flannel
    - Dans la config du DaemonSet
      - Qu'est-ce que c'est ?
    - Modification effectuée
      #+BEGIN_PLAIN
      containers:
      - args:
      - --ip-masq
      - --kube-subnet-mgr
      - --iface=eth0
      #+END_PLAIN
  - Permet de garantir que les noeuds communiquent via la bonne interface réseau
  - Mais ne corrige pas mon problème
  - Configuration de containerd
    - Les pages suivantes attribuent cette erreur à une configuration invalide de containerd
      - https://www.vnoob.com/2022/12/kubectl-6443-connection-refused/
      - https://github.com/containerd/containerd/issues/4581
    - Effectivement, j'ai pas touché à la conf de containerd
      - Juste retiré la ligne `disabled_plugins["cri"]` comme indiqué dans la doc de kubeadm
      - Mais ma conf par défaut ne ressemble pas à la conf par défaut proposée par containerd
    - Remplacement de la conf par défaut
      - `containerd config default | sudo tee /etc/containerd/config.toml`
    - Utiliser le cgroup systemd pour containerd
      - `SystemdCgroup = true`
      - Indiqué ici : https://kubernetes.io/docs/setup/production-environment/container-runtimes/#containerd
  - Tombe en marche !
  - A pu faire join les autres noeuds du PicoCluster
  - A pu démarrer une application hello-world sur le cluster k8s
    - kubectl create deployment hello-node --image=registry.k8s.io/e2e-test-images/agnhost:2.39 -- /agnhost netexec --http-port=8080
  - État du cluster
    #+BEGIN_PLAIN
livingfog@pc0:~ $ kubectl get nodes -o wide
NAME   STATUS   ROLES           AGE    VERSION   INTERNAL-IP    EXTERNAL-IP   OS-IMAGE                         KERNEL-VERSION      CONTAINER-RUNTIME
pc0    Ready    control-plane   22h    v1.30.1   192.168.1.10   <none>        Debian GNU/Linux 12 (bookworm)   6.6.28+rpt-rpi-v8   containerd://1.6.31
pc1    Ready    <none>          21h    v1.30.1   192.168.1.11   <none>        Debian GNU/Linux 12 (bookworm)   6.6.20+rpt-rpi-v8   containerd://1.6.31
pc2    Ready    <none>          4m8s   v1.30.1   192.168.1.12   <none>        Debian GNU/Linux 12 (bookworm)   6.6.20+rpt-rpi-v8   containerd://1.6.31
livingfog@pc0:~ $ kubectl get pods -A -o wide
NAMESPACE      NAME                          READY   STATUS    RESTARTS       AGE     IP             NODE   NOMINATED NODE   READINESS GATES
default        hello-node-55fdcd95bf-zzhzh   1/1     Running   1 (21h ago)    21h     10.244.1.3     pc1    <none>           <none>
kube-flannel   kube-flannel-ds-5ssnd         1/1     Running   1 (21h ago)    21h     192.168.1.10   pc0    <none>           <none>
kube-flannel   kube-flannel-ds-csbff         1/1     Running   3 (21h ago)    21h     192.168.1.11   pc1    <none>           <none>
kube-flannel   kube-flannel-ds-kn6ft         1/1     Running   0              5m10s   192.168.1.12   pc2    <none>           <none>
kube-system    coredns-7db6d8ff4d-gtt92      1/1     Running   1 (21h ago)    22h     10.244.0.76    pc0    <none>           <none>
kube-system    coredns-7db6d8ff4d-qmrfz      1/1     Running   1 (21h ago)    22h     10.244.0.77    pc0    <none>           <none>
kube-system    etcd-pc0                      1/1     Running   68 (21h ago)   22h     192.168.1.10   pc0    <none>           <none>
kube-system    kube-apiserver-pc0            1/1     Running   67 (21h ago)   22h     192.168.1.10   pc0    <none>           <none>
kube-system    kube-controller-manager-pc0   1/1     Running   81 (21h ago)   22h     192.168.1.10   pc0    <none>           <none>
kube-system    kube-proxy-26fmx              1/1     Running   0              5m10s   192.168.1.12   pc2    <none>           <none>
kube-system    kube-proxy-mlg2x              1/1     Running   4 (21h ago)    21h     192.168.1.11   pc1    <none>           <none>
kube-system    kube-proxy-t89tf              1/1     Running   10 (21h ago)   22h     192.168.1.10   pc0    <none>           <none>
kube-system    kube-scheduler-pc0            1/1     Running   74 (21h ago)   22h     192.168.1.10   pc0    <none>           <none>
    #+END_PLAIN
  - Comment exposer le pod hello-node?
  - En mettant en place un service de type NodePort
    - kubectl expose deployment hello-node --type=NodePort --port=8080
    - Résultat
      #+BEGIN_PLAIN
livingfog@pc0:~ $ kubectl get services -o wide
NAME         TYPE        CLUSTER-IP      EXTERNAL-IP   PORT(S)          AGE   SELECTOR
hello-node   NodePort    10.105.13.107   <none>        8080:30171/TCP   32m   app=hello-node
kubernetes   ClusterIP   10.96.0.1       <none>        443/TCP          22h   <none>
      #+END_PLAIN
  - De manière non-intuitive, indique que mon application est accessible sur pc0:30171
    - wget 131.254.162.2:30171 me permet bien d'interroger in fine le pod et de récupérer sa réponse
  - Pas clair si j'ai juste de la chance là-dessus
  - Comment ça fonctionne un service NodePort ?
    - Est-ce un simple routage ?
    - Ou est-ce un fonctionnement plus complexe ?
    - Si cela prend la forme d'un composant, e.g. un pod, est-ce que le service est forcément mis en place sur pc0, i.e. le control plane ?
    - Ou est-ce qu'il peut être affecté à n'importe quel noeud ?
  - De ce que je lis, affecte tous les nodes du cluster
    - https://medium.com/@berkayhelvacioglu/understanding-and-using-nodeport-services-in-kubernetes-5eba94666646
    - A NodePort service is a type of service that binds a service to a specific port number on each Kubernetes node.
    - Once a NodePort service is created, you can access it from outside the cluster using the IP address of any Kubernetes node and the assigned port number.
  - Effectivement, peut accéder à mon appli depuis n'importe lequel de mes noeuds sur le port indiqué
    - http://<node-ip>:30171
  - C'est bon à savoir
- LivingFog: Déployer un cluster Kubernetes sur VMs
  - Histoire de valider mes résultats d'hier, j'ai relancé le setup du cluster k8s sur VMs
  - Pas pu enable le cgroup memory
    - Pas de fichier /boot/firmware/cmdline.txt sur Debian
  - Mais en fait, enabled par défaut
    #+BEGIN_PLAIN
vagrant@master:~$ cat /sys/fs/cgroup/cgroup.controllers
cpuset cpu io memory hugetlb pids rdma misc
    #+END_PLAIN
  - A pu constaté que la config de base de containerd est vide, à l'exception de la désactivation du plugin CRI
    - Pas celle avec containerd config default
    - Mais celle qui est présente suite à l'install de containerd dans /etc/containerd/config.yaml
  - A tenté de démarrer le cluster sans avoir désactivé le swap
    - Échoue au cours de la commande init, au moment de démarrer kubelet
  - A tenté de déployer flannel sans spécifier l'interface réseau
    - Cluster semble fonctionnel
    - Mais a rencontré une erreur lorsque j'ai mis en place un Service NodePort pour mon appli Hello World
    - Le service est créé, j'ai pas noté d'erreur particulière
    - Mais en pratique, ne peut contacter l'appli que via l'url <ip_worker>:<port_service>
    - Et non pas en utilisant l'ip de n'importe quel noeud du cluster, ici le master
    - En modifiant la config de flannel pour utiliser l'interface permettant aux noeuds de communiquer, résout le problème
  - A perdu la commande générée par kubeadm init pour permettre à un noeud de rejoindre
    - A appris l'existence d'une commande pour regénérer la commande souhaitée
    - kubeadm token create --print-join-command
  - Reste plus qu'à automatiser tout ça
- LivingFog: Mettre à jour le playbook Ansible
  - Étapes
    - [ ] Enable le cgroup memory
      - Est-ce que ça doit vraiment être géré par Ansible ?
      - Vu que le fichier /boot/firmware/cmdline.txt correspond à une seule et unique ligne
      - Peut p-e juste remplacer la ligne
      - Sauf qu'une partie de la ligne semble spécifique au device
        - root=PARTUUID=<value>
        - Correspond au partition ID de notre système
          - cf. https://raspberrytips.com/raspberry-pi-cmdline-txt/
          - Par exemple si notre carte SD est partitionnée en /boot (p1) et /root (p2)
          - Et a pour disk ID 0xf7cf2c94
            - Obtenu via `sudo fdisk -l`
          - Obtient f7cf2c94-02 comme value
      - Nécessite un redémarrage
    - [ ] Mettre en place la config par défaut de containerd et setup containerd pour utiliser le cgroup systemd
      - Comment rendre cet ensemble de tâches exécuté only-once ?
        - i.e. générer le fichier de config par défaut, le modifier et remplacer le fichier de conf actuel
      - Actions idempotentes
      - Peut même rendre le tout atomique en maintenant une copie du fichier à côté du playbook et en la copiant directement
      - Mais vais "changer" l'état du noeud à chaque exécution du playbook
      - Et donc forcer un redémarrage du service containerd inutile
      - Est-ce qu'Ansible permet de gérer ce type d'actions proprement ?
    - [ ] Désactiver le swap
      - Sur Debian
        - sudo swapoff -a
        - Ah non, ça désactive le swap que pour cette session
        - Doit plutôt identifier quel unit gérée par systemd est responsable du swap
          #+BEGIN_PLAIN
  vagrant@worker:~$ sudo systemctl --type swap
    UNIT                                                                      LOAD   ACTIVE SUB    DESCRIPTION           >
    dev-disk-by\x2duuid-351a3e49\x2d6c73\x2d4e98\x2da3c7\x2d908a8c12839f.swap loaded active active /dev/disk/by-uuid/351a>
          #+END_PLAIN
        - Et la désactiver
        - sudo systemctl mask "dev-disk-by\x2duuid-351a3e49\x2d6c73\x2d4e98\x2da3c7\x2d908a8c12839f.swap"
      - Sur RPi
        - sudo systemctl disable dphys-swapfile
    - [ ] Créer le cluster k8s
      - sudo kubeadm config images pull
      - sudo kubeadm init --pod-network-cidr 10.244.0.0/16 --apiserver-advertise-address 192.168.1.10 --control-plane-endpoint pc0
    - [ ] Déployer le plugin CNI
      - Utilise pour le moment la config par défaut de flannel
      - Modifiée pour spécifier d'utiliser l'interface appropriée pour que les noeuds communiquent entre eux
    - [ ] Faire rejoindre aux workers le cluster k8s
** Semaine du <2024-05-06 Mon> au <2024-05-07 Tue>
*** Planned
**** IN-PROGRESS LivingFog: Déployer LivingFog sur le PicoCluster
- Pour prendre en main les différents outils que je vais utiliser dans le projet
- Le mieux est de les utiliser dans le cadre d'un PoC
- Ici, l'idée est de déployer LivingFog sur 2 noeuds du PicoCluster
  - Noeud principal
  - Noeud worker
- Permet aussi de me familiariser avec l'écosystème LivingFog
  - https://github.com/FogGuru/livingfog
  - http://www.fogguru.eu/livingfog/
- De remonter les difficultés/incompréhensions rencontrées
- Et de commencer à identifier ce qui a trait à LoRaWAN
  - Pour une potentielle excision future
**** IN-PROGRESS SmartSense: Rédiger un fichier de config Ansible pour le noeud SmartSense
- Maintenant que j'ai pu déployer un noeud SmartSense et identifier les différentes étapes nécessaires
- Serait utile de
  - Documenter le processus
  - L'automatiser
- Ansible est semblerait l'outil adapté pour cela
- Automatiser le déploiement d'un noeud SmartSense en utilisant Ansible
**** IN-PROGRESS SmartSense: Mise en place d'un processus de CI
- Mettre en place des tests, c'était un bon début
- Mettre en place un processus de CI, ça serait mieux
  - Garantirait la reproductibilité de l'environnement de tests
  - Et que j'oublie pas de les faire tourner avant l'intégration
- Voir comment GitLab gère ça
*** Done
- LivingFog: Déployer LivingFog sur le PicoCluster
  - IMO faudrait procéder à quelques modifications
  - Surtout MàJ les versions installées de Docker et de Kubernetes
  - Les versions récentes de k8s n'utilisent plus Docker-Engine mais un Container Runtime (CR) respectant la CR Interface (CRI)
    - Entre autres, containerd
    - cf. https://kubernetes.io/docs/setup/production-environment/tools/kubeadm/install-kubeadm/#installing-runtime
  - Devrais plutôt setup LivingFog avec ça nowadays
    - S'installe soit depuis les binaires officiels
      - https://github.com/containerd/containerd/blob/main/docs/getting-started.md
    - Soit via le repo apt de Docker
      - https://docs.docker.com/engine/install/debian/
  - De même, pour kubeadm
    - Installe pour le moment depuis le repo pour Ubuntu Xenial
  - Setup Vagrant
    - Plutôt que de bosser à même les RPis
    - Préfère travailler sur des VMs pour le moment
    - Avec Vagrant, spawn 2 VMs
      #+BEGIN_PLAIN
Vagrant.configure("2") do |config|
  config.vm.define "master" do |master|
    master.vm.box = "generic/debian12"
    master.vm.hostname = "master"
    master.vm.network "private_network", ip: "192.168.60.10"

    master.vm.provider "virtualbox" do |vb|
      vb.gui = false
      vb.memory = "4096"
    end
  end

  config.vm.define "worker" do |worker|
    worker.vm.box = "generic/debian12"
    worker.vm.hostname = "worker"
    worker.vm.network "private_network", ip: "192.168.60.11"

    worker.vm.provider "virtualbox" do |vb|
      vb.gui = false
      vb.memory = "4096"
    end
  end
end
      #+END_PLAIN
    - A configuré les hosts d'Ansible en accord
      #+BEGIN_PLAIN
vm0 ansible_ssh_host=127.0.0.1 ansible_port=2222   ansible_user=vagrant    ansible_ssh_private_key_file=/home/mnicolas/Documents/livingfog/kubeadm-ansible/.vagrant/machines/master/virtualbox/private_key
vm1 ansible_ssh_host=127.0.0.1 ansible_port=2200   ansible_user=vagrant    ansible_ssh_private_key_file=/home/mnicolas/Documents/livingfog/kubeadm-ansible/.vagrant/machines/worker/virtualbox/private_key

[master]
vm0

[node]
vm1

[kube-cluster:children]
master
node
      #+END_PLAIN
    - Utilise cela pour le moment
  - Setup LivingFog
    - [X] MàJ à la dernière version de Docker
    - [X] Installer containerd
    - Par défaut, le plugin CRI est désactivé
    - Nécessite de modifier /etc/containerd/config.toml
    - [X] Modifier la config de containerd
      - Ajout d'une tâche qui commente la ligne
    - [X] Redémarrer le service containerd
      - Pour prendre en compte l'activation du plugin CRI
    - Rencontre une erreur pour la tâche *Deploy kubernetes dashboard into cluster*
      - Exécute la commande suivante
        - kubectl --kubeconfig={{ kubeadmin_config }} apply -f https://raw.githubusercontent.com/kubernetes/dashboard/master/aio/deploy/recommended.yaml
      - Sauf que ce n'est plus la façon de procéder nowadays
        - cf. https://github.com/kubernetes/dashboard?tab=readme-ov-file#installation
      - Plutôt à base d'Helm
    - Rencontre une erreur non-reproductible concernant le Container Network Interface
      - À la première exécution du playbook, les tâches concernant flannel échouent
      - Mais à une seconde exécution, les deux tâches passent
      - Faux positif?
      - A-t-on toujours besoin d'un CNI ?
      - Oui
      - Étape suivante indiquée quand démarre le cluster avec kubeadm init
      - Commande utilisées
        - sudo kubeadm init --pod-network-cidr 10.244.0.0/16 --token b0f7b8.8d1767876297d85c --control-plane-endpoint vm0
        - sudo kubeadm join --token - b0f7b8.8d1767876297d85c -discovery-token-unsafe-skip-ca-verification vm0:6443
      - Ne fonctionne pas sans le paramètre --pod-network-cidr
        - Semble être à cause de flannel qui spécifie ce cidr à un moment dans config par défaut
      - Ai-je besoin de l'option --control-plane-endpoint ?
        - Pris d'un doute
        - Pense que oui, avec mon setup actuel
        - Permet en fait d'éviter d'utiliser l'@ip de l'interface par défaut
    - Rencontre une erreur avec l'@ip utilisée
      - Par défaut, Vagrant met en place et semble requérir une interface réseau eth0
      - Distincte de l'interface réseau correspondant au réseau privé que je veux mettre en place
      - Et donc notamment avec une autre @ip
      - Sauf que Kubernetes lui veut utiliser l'interface réseau par défaut pour son fonctionnement
      - La configuration par défaut se base donc sur cette interface réseau que je ne maitrise pas
      - A contourné le problème initialement en ajoutant vm0 comme host connu de mes noeuds
      - Et en utilisant vm0 dans la commande kubeadm init/join
      - Mais a l'air de rencontrer un conflit avec flannel
        - Est-ce que lui aussi se base sur l'interface réseau par défaut ?
      - En parcourant brièvement la config proposée par défaut pour flannel, ne vois pas d'option par rapport à cela ?
        - À creuser
** Semaine du <2024-04-29 Mon> au <2024-05-03 Fri>
*** Planned
**** DONE Magellan: Préparer présentation sur CRDTs
CLOSED: [2024-05-02 Thu 10:54]
- Me suis proposé de présenter mes travaux précédants dans le cadre du séminaire Magellan <2024-05-02 Thu>
- À la réflexion, me paraît pas super intéressant de parler de RLS spécifiquement
- Préfererais parler des CRDTs de manière générale
- Préparer présentation
**** DONE LivingFog: Étudier le playbook *Kubeadm Ansible Playbook*
CLOSED: [2024-05-03 Fri 14:21]
- Ce playbook semble être le point d'entrée
  - https://github.com/FogGuru/livingfog/blob/main/kubeadm-ansible/site.yaml
- Il est décrit comme un moyen simple pour déployer un cluster Kubernetes sur le picocluster
- Personnellement, il ne me semble pas si simple
- Étudier le playbook
- Et identifier les différentes tâches et étapes qu'il effectue
**** IN-PROGRESS SmartSense: Rédiger un fichier de config Ansible pour le noeud SmartSense
- Maintenant que j'ai pu déployer un noeud SmartSense et identifier les différentes étapes nécessaires
- Serait utile de
  - Documenter le processus
  - L'automatiser
- Ansible est semblerait l'outil adapté pour cela
- Automatiser le déploiement d'un noeud SmartSense en utilisant Ansible
**** IN-PROGRESS SmartSense: Mise en place d'un processus de CI
- Mettre en place des tests, c'était un bon début
- Mettre en place un processus de CI, ça serait mieux
  - Garantirait la reproductibilité de l'environnement de tests
  - Et que j'oublie pas de les faire tourner avant l'intégration
- Voir comment GitLab gère ça
**** IN-PROGRESS LivingFog: Déployer LivingFog sur le PicoCluster
- Pour prendre en main les différents outils que je vais utiliser dans le projet
- Le mieux est de les utiliser dans le cadre d'un PoC
- Ici, l'idée est de déployer LivingFog sur 2 noeuds du PicoCluster
  - Noeud principal
  - Noeud worker
- Permet aussi de me familiariser avec l'écosystème LivingFog
  - https://github.com/FogGuru/livingfog
  - http://www.fogguru.eu/livingfog/
- De remonter les difficultés/incompréhensions rencontrées
- Et de commencer à identifier ce qui a trait à LoRaWAN
  - Pour une potentielle excision future
*** Done
- Magellan: Préparer présentation sur CRDTs
  - Plan
    - [X] Local-First Softwares
      - Coast team interested in collaborative peer-to-peer applications to ensure
        - Availability
        - Ownership
        - Privacy
      - Coined a few years ago as Local-First Softwares
    - [X] Challenges of data replication in peer-to-peer systems
      - Peers collaborate on a shared document
      - Each peer owns a copy of the document
      - May edit it without synchronous coordination (aka consensus)
      - Systems have to ensure eventual consistency
      - Despite the different orders of integration of updates
      - Rely on conflict resolution mechanisms to do so
    - [X] CRDTs: an overview
      - What are CRDTs ?
      - Specification of CRDTs
      - Current use-cases
      - Ongoing research topics
    - [X] What are CRDTs ?
      - New specification of data types
      - Embed in their design a conflict resolution mechanism
      - Properties of CRDTs
        - Enable updates without coordination
        - Ensure SEC
    - [X] Specification of CRDTs
      - Several CRDTs may be proposed for a given data type
      - What mainly characterize one CRDT is its
        - Conflict resolution semantic
        - Synchronisation model
    - [X] Conflict resolution semantic
      - Encounters new scenarios only possible in distributed settings
      - Result of such scenarios is undefined
      - To design the CRDT, have to define what is the expected result
      - Several semantics may be proposed for a given data type
        - e.g. Add-Wins, Remove-Wins, Causal Length for Set
    - [X] Synchronisation model
      - How changes are propagated to other nodes also impact the design of the CRDT
      - Several approaches were proposed
    - [X] State-based synchronisation
      - Nodes periodically send their current states to others
      - A merge function enables nodes to fuse the current state and received one to obtain an up-to-date one
        - merge has to be associative, commutative and idempotent
      - Limits
        - Not suited for real-time updates propagation
    - [X] Operation-based synchronisation
      - Sending the whole state may be expensive according to data type
      - Instead, encode updates as messages called operations
      - And broadcast them to other nodes
        - Smaller footprint, suited to real-time
      - Concurrent operations have to be commutative
        - But no constraints on the idempotency and associativity of operations
      - However, operations may have to be delivered in given order
        - e.g. insertion of an element must be delivered before its deletion
      - Operation-based CRDTs have to be pair with a delivery layer to handle network faults
        - Re-ordering of operations
        - De-duplication of operations
        - Anti-entropy mechanism to detect and retrieve lost operations
    - [-] Delta-based synchronisation
      - Proposed more recently as a "best of the two worlds approach"
      - Instead of encoding updates as arbitrary messages
      - Encode updates as the minimal corresponding states, called deltas
      - Nodes use merge function to integrate deltas in their own copy
      - Nodes broadcast their whole state as an anti-entropy mechanism
    - [X] Recap of synchronisation models
    - [X] State of the art
      - Collaborative peer-to-peer applications (MUTE, Teletype, Apple Notes)
        - Libraries have been proposed to ease development of such applications (Automerge, Yjs)
      - Multi-master replication for distributed databases (Redis, Microsoft Azure CosmoDB)
    - [X] Ongoing research topics
      - Designing CRDTs for other data types
        - Tree data type
      - Reducing overhead introduced by the conflict resolution mechanism
        - Topic of my PhD, in the context of Sequence-CRDTs
      - Designing CRDTs ensuring global invariants
        - An always positive counter
        - Integrity constraints in relational databases
      - Designing CRDTs for systems with Byzantin adversaries
      - Trade-offs between efficiency and user experience
        - By default, users expect that applications enforce the causal consistency model
          - e.g. expect the answer to follow the question, not the other way around
        - Enforcing this consistency model is expensive, especially as the number of nodes increases
        - CRDTs can relax this constraint
        - What is the impact of these "anomalies" on user experience?
  - A une version quasi-complète des slides
  - Reste à
    - [X] Remplacer figure pour Op-based CRDTs par scénario plus simple
    - [X] Animer les figures
    - [X] Améliorer slide de synthèse
    - [-] Préparer back-up slides sur Delta-based CRDTs
- Réunion Guillaume
  - Notes
    - Depuis la semaine dernière, prend en main le picocluster et livingfog
    - Picocluster
      - A re-déployé de nouvelles images systèmes sur 3 RPis
      - Et les a configurées pour qu'elles se connaissent
      - A préparé une d'entre elles pour qu'elle se charge du déploiement du système
        - Installation d'Ansible
        - Installation de Python
        - Enregistrement de la clé ssh sur les autres RPis
    - LivingFog
      - A parcouru le repo github
      - Et lu la documentation technique
      - Me paraît plutôt claire sur les différents composants du système
      - Notamment, pointe les composants liés à LoRaWAN
        - Chirpstack
        - Et ses dépendances Postgres et Redis
      - Juste pas au clair sur
        - La partie *Preprocessor Python Code*
          - Traduit les trames Chirpstack au format InfluxDB
          - Description mentionne qu'il génère aussi les trames MQTT correspondantes et les re-fournit au broker MQTT
          - Pour qui ?
        - La partie *Server PC*
          - Sert à centraliser l'ensemble des données collectées par les différents clusters LivingFog
          - Et à offrir une interface de monitoring
          - Y en a-t-il un de déployé ?
          - N'a pas l'air d'envoyer de messages aux fogs
          - C'est bien le cas ?
      - Parcours à présent les playbooks Ansible mis à disposition sur le repo
      - Le *Kubeadm Ansible Playbook* semble tout indiqué pour commencer
        - Met en place et configure seulement un cluter k8s
        - i.e. rien de lié à LoRaWAN
      - Avant de le lancer, essaie de comprendre ce qu'il fait exactement
      - Les différents composants qu'il installe
        - Quid des tâches CNI, MetalLB, GlusterFS ?
        - Et du rôle de Helm ?
      - Et ce qu'il déploie
        - Quid de Vagrant ?
      - Pour déployer ensuite les services, faudra me pencher sur les scripts dans *chirpstack-kubernetes*
        - Et filtrer ce qui nous intéresse ici
  - Questions
    - [ ] Trames MQTT du Preprocessor Python Code ?
    - [X] Server PC déployé ?
      - Non
      - Mais un serveur à disposition si besoin
    - [X] Server PC communique avec clusters ?
      - Non
      - Communication du cluster aux capteurs, c'est compliqué en LoRa
    - [X] À terme, utiliser kubeadm ou k3s ?
      - kubeadm car k3s n'était pas forcément connu à l'époque
      - Pourquoi pas plus tard
  - Réunion
    - Nepal
      - But de l'appel est l'amélioration d'observatoires existant
        - Cas de l'observatoire au Népal
        - Plusieurs années qu'ils suivent l'état d'une rivière
        - Des capteurs à plusieurs endroits de la rivière
        - Font des prélèvements de l'eau
        - Remonte les données en 3G à l'heure actuelle
        - Tout le réseau cellulaire du Népal est derrière une @ip
        - Utilise des VPNs pour recréer un réseau derrière
        - Réseau cellulaire instable globalement
        - Projet de Christoff, Laurent comme chef d'équipe
        - De temps en temps, ne reçoit plus de données en FR
        - But est de mettre en place un système de monitoring de l'observatoire
          - Mettre en place des alertes en cas de dysfonctionnement
          - Catégoriser les pannes rencontrées
          - Mettre en place prétraitements sur place
          - Mettre en place un système d'alertes en cas de crues
          - Avec des contraintes énergétiques, réseau
            - Peut ne plus avoir d'énergie (solaire, hydraulique)
            - Donc doit pouvoir résister à une coupure
      - Objectif serait de déployer prototype ici à Rennes
        - Mise en place d'un noeud Fog
    - LivingFog
      - Partie multi-tenants
        - Pas une priorité
        - Mais serait bien de comprendre comment cela marche
        - Un sujet qui risque de monter lors du déploiement de TerraForm
        - Faut regarder du côté des namespaces
- LivingFog: Étudier le playbook *Kubeadm Ansible Playbook*
  - [X] Kubernetes-master
    - Installe la version 1.18 de kubelet, kubectl, kubeadm
    - Démarre le cluster
    - Met en place le dashboard
  - [X] cni
    - Container Network Interface
    - Fournit les configurations des différentes interfaces réseau possibles pour les containers
    - La configuration utilisée est ensuite précisée dans le fichier group_vars/all.yaml
    - La documentation mentionne flannel
    - Pourquoi celui-ci par rapport à un autre ?
      - Ne vois pas les implications pour le moment
  - [X] Kubernetes-node
    - Installe k8s
    - Fait rejoindre le cluster
  - [X] Helm
    - Est un service de recettes pour k8s
    - Installe Helm
    - Et donne les droits sur la config du cluster à Tiller
      - Le composant qui tourne au sein du cluster k8s
      - Et qui se charge d'effectuer les modifications de l'état
    - Tiller semble ne plus exister dans la version courante de Helm
    - Service optionnel
    - Non mis en place avec la config actuelle
  - [X] MetalLB
    - Est un service de load-balancing pour k8s
      - https://metallb.org/
    - Installe et configure MetalLB
    - La config générale actuelle skip l'installation de ce composant
    - À quel endroit du système a-t-on besoin de load balancing ?
      - Y a-t-il plusieurs instances d'un même service dans un cluster LivingFog?
  - [X] Healthcheck
    - S'agit d'une petite app
      - https://github.com/emrekenci/k8s-healthcheck
    - Qui monitore la santé des différents noeuds du cluster k8s
    - Installe et déploie l'application au sein du cluster
    - Feature optionnelle, actuellement désactivée
  - [X] GlusterFS-node
    - GlusterFS est un distributed FS
    - Offre sous un namespace unique des fichiers répartis sur plusieurs serveurs
    - Chaque serveur appartenant au cluster va héberger des bricks
    - GlusterFS expose alors l'ensemble des bricks sous la forme d'un volume unique
    - Les fichiers vont être répartis en fonction de la configuration choisie
      - Distributed Glusterfs Volume
        - Pas de réplication
        - Chaque fichier est stocké sur une brick unique
      - Replicated Glusterfs Volume
        - Réplication activée
        - Chaque fichier va être stockée sur l'ensemble des bricks
        - Les bricks sont donc des répliques les unes des autres
        - Pas possible d'avoir un replication factor < nombre de noeuds ?
      - Distributed Replicated Glusterfs Volume
        - Une combinaison des settings précédents
        - Utilise des Replicated Glusterfs Volume au sein d'un Distributed Glusterfs Volume
        - Smart ma foi
        - Possible dans ce cas de spécifier le replication factor utilisé
      - Dispersed Glusterfs Volume
        - Repose sur des erasure codes
        - i.e. split le fichier en chunks, disséminé sur les différentes bricks
        - Les chunks sont enrichis de façon à permettre de recomposer le fichier en cas de panne d'une ou plusieurs bricks
        - Entraîne cependant un overhead en terme de stockage
          - Moins lourd qu'une copie complète nonobstant
        - Donc ici, va créer ces chunks et les répartir sur les bricks
      - Distributed Dispersed Glusterfs Volume
        - Utilise des Dispersed Glusterfs Volume au sein de Distributed Glusterfs Volume
        - Permet de répartir les fichiers sur des Disperse Volumes distincts
    - Utilise consistent hashing et une DHT pour placer les fichiers sur les bricks
      - Avec les problèmes inhérents
      - e.g. modification du set de bricks modifiant la répartition supposée des fichiers existants
      - e.g. modification du set de bricks alors qu'une brick est offline, qui va résulter en des overlaps entre bricks
    - Description du playbook
      - Installe et active GlusterFS
      - Rien de fancy semblerait
  - [X] GlusterFS-master
    - Installe et active GlusterFS
    - Setup le distributed FS
    - Créé les différents volumes GlusterFS
      - mosquitto, postgre, influxdb
      - 2 replicas par volume
      - Rebalance si changement du cluster
      - A l'air par contre d'utiliser 1 seule brick
      - Donc des Replicated Glusterfs Volume ?
      - Nope, faudrait plusieurs briques pour ça
      - Comprends pas trop la config, est-elle valide ?
      - Laisse le bénéfice du doute, ça doit être une brick par noeud
  - Par contre, ne vois pas la mention de version des différents composants
    - Y a une version pour le docker engine dans kubeadm-ansible/roles/docker/defaults/main.yml
      - 17.03
      - On est à la 26.1
    - Mais pas pour le reste
    - Est-ce une bonne ou une mauvaise idée ?
    - On a par contre des tâches/arguments qui ont l'air d'être spécifiques à des bugs, donc à des versions
      - Dans kubeadm-ansible/roles/kubernetes/master/tasks/init.yml
      - Vu l'historique de l'issue référencée, pas sûr que le problème initial soit effectivement corrigé nowadays
    - Indiquer les versions permettrait de décrire un environnement fonctionnel
      - Et d'assurer qu'on pourra le reproduire
    - Par contre, spécifier les versions risque de bloquer le projet dans un état donné
      - Et nécessiterait un effort supplémentaire de maintenance
    - Ah y aussi une version pour k8s spécifiée dans kubeadm-ansible/group_vars/all.yml
      - 1.18
      - On est à la 1.30
  - Par contre, le playbook repose aussi sur l'utilisation de yum
    - Plus trop d'actualité
    - Si je pige bien, a été remplacé par dnf depuis
  - Où et quand est mis en place/configuré le VPN ?
    - Le README ainsi que le fichier de vars mentionne l'existence d'un VPN
    - Je ne vois où ce dernier est créé
    - Et son rôle
  - Questions
    - À quoi sert healthcheck vs les applications de monitoring ?
    - À quoi sert Helm ici ?
    - À quoi sert Vagrant ici ?
    - Est-ce que les dépendances du projet sont toujours d'actualités ?
      - Repose par exemple sur GlusterFS et MetalLB
      - Est-ce que k8s propose à présent nativement ces features ?
    - Est-ce que la config des volumes GlusterFS est valide ?
      - Incohérence entre le nombre de replicas et la brick fournie
    - Comment les bricks sont gérées par GlusterFS ?
      - Est-ce que c'est par noeud ?
      - Auquel cas, on retombe sur nos pattes
      - Chaque noeud possède sa brick
      - Et les fichiers vont être répartis sur ces bricks, avec un replication factor de 2
- LivingFog: Déployer LivingFog sur le PicoCluster
  - Problèmes rencontrés au lancement du playbook
  - N'arrive pas à installer docker-ce avec la version demandée
    - En retirant le num de version, cela fonctionne
    - À voir si je rencontre une erreur plus tard
    - Problème étant aussi que le repo ajouté et utilisé pour docker-engine est vieux
    - Correspond à Ubuntu Xenial, i.e. 2016
    - Serait p-e bien de mettre à jour cela
  - N'arrive pas à installer kubeadm
    - Repo utilisé est là aussi trop ancien
    - La clé PGP n'a plus l'air valide
** Semaine du <2024-04-22 Mon> au <2024-04-26 Fri>
*** Planned
**** DONE SmartSense: Traiter les problèmes relevés lors de l'uniformisation du code
CLOSED: [2024-04-22 Mon 09:02]
- J'ai noté plusieurs points problématiques au cours de mon passage sur l'ensemble des modules
  - Le module *sensor_board* est inutilisé
    - Code obsolète à supprimer ?
  - La librairie *telnetlib* est dépréciée
    - Passer à https://pypi.org/project/telnetlib3/ ?
  - Une conditionnelle étrange est effectuée dans le module *tlc59108_manager*
    - https://gitlab.inria.fr/smartsense/3douest/node-app/smartsense-node-app/-/blob/3feb5c56dc5e74dcea9eb2f2ba98bec40029acdf/TLC59108Manager.py#L370
    - Revient à `if data[0]`
    - `and` est-il l'opérateur désiré ?
    - Simplifier/corriger
  - Les appels à *OpenOCDManager.update()* dans *test_ocd* manquent d'un paramètre
    - J'ai l'impression que c'est le paramètre *board*
    - Ajouter les valeurs correctes ?
  - *USBBoardManager.read_data()* et *parse_to_mqtt()* sont particulièrement sujets aux retours de Pylint
    - Trop d'instructions
    - Trop de branchements
    - Trop de blocs imbriqués
    - Refactorer pour améliorer la lisibilité du code ?
    - Voir si on peut aussi factoriser du code à l'aide de fonctions
  - Corriger ces problèmes
    - Ou si pas de solution claire et rapide, créer une issue correspondante
**** DONE SmartSense: Suggestions
CLOSED: [2024-04-22 Mon 09:24]
- Mécanisme d'export de la BD
  - Remplacer `backup_index` par l'identifiant du noeud
- Supprimer le code relatif à hwclock
  - Notre noeud SmartSense ne dispose pas d'une horloge BIOS
  - Ce code semble donc sans effet et s'avère déroutant
**** DONE SmartSense: Intégrer les branches `fix/misc_fixes` et `feat/make-optional-influxdb-export`
CLOSED: [2024-04-22 Mon 13:31]
- Déployer ces versions sur le noeud SmartSense
- S'assurer de leur bon fonctionnement
- Intégrer la branche de fixes
- Faire une MR pour la branche concernant les exports ?
**** CANCELLED LivingFog: Automatiser la création d'un cluster de Raspberry virtualisé
- Plutôt que de travailler avec le cluster physique
- Serait intéressant d'apprendre à provisionner un cluster virtuel de Raspberry
- Permettrait de pouvoir créer un environnement pour dev et effectuer des tests
  - Notamment comment setup k8s ou équivalent sur un tel système
**** DONE LivingFog: Mettre en place réseau de RPis
CLOSED: [2024-04-26 Fri 10:57]
- Pour pouvoir déployer application sur cluster
- Doit au préalable permettre à ma machine de communiquer avec les différentes RPis
  - Ou toute autre machine
- Notamment, doit pouvoir récupérer les adresses IP de toutes les machines du cluster
- Et pouvoir s'y connecter en SSH
- Voir comment setup un tel environnement
**** IN-PROGRESS SmartSense: Rédiger un fichier de config Ansible pour le noeud SmartSense
- Maintenant que j'ai pu déployer un noeud SmartSense et identifier les différentes étapes nécessaires
- Serait utile de
  - Documenter le processus
  - L'automatiser
- Ansible est semblerait l'outil adapté pour cela
- Automatiser le déploiement d'un noeud SmartSense en utilisant Ansible
**** IN-PROGRESS SmartSense: Mise en place d'un processus de CI
- Mettre en place des tests, c'était un bon début
- Mettre en place un processus de CI, ça serait mieux
  - Garantirait la reproductibilité de l'environnement de tests
  - Et que j'oublie pas de les faire tourner avant l'intégration
- Voir comment GitLab gère ça
**** IN-PROGRESS LivingFog: Déployer LivingFog sur le PicoCluster
- Pour prendre en main les différents outils que je vais utiliser dans le projet
- Le mieux est de les utiliser dans le cadre d'un PoC
- Ici, l'idée est de déployer LivingFog sur 2 noeuds du PicoCluster
  - Noeud principal
  - Noeud worker
- Permet aussi de me familiariser avec l'écosystème LivingFog
  - https://github.com/FogGuru/livingfog
  - http://www.fogguru.eu/livingfog/
- De remonter les difficultés/incompréhensions rencontrées
- Et de commencer à identifier ce qui a trait à LoRaWAN
  - Pour une potentielle excision future
**** IN-PROGRESS LivingFog: Étudier le playbook *Kubeadm Ansible Playbook*
- Ce playbook semble être le point d'entrée
  - https://github.com/FogGuru/livingfog/blob/main/kubeadm-ansible/site.yaml
- Il est décrit comme un moyen simple pour déployer un cluster Kubernetes sur le picocluster
- Personnellement, il ne me semble pas si simple
- Étudier le playbook
- Et identifier les différentes tâches et étapes qu'il effectue
*** Done
- SmartSense: Suggestions
  - Création de tâches respectives
- SmartSense: Intégrer les branches `fix/misc_fixes` et `feat/make-optional-influxdb-export`
  - Changements pour run le programme sur l'image actuelle
    - Chemin du fichier de config dans main_app
    - Désactiver l'utilisation du watchdog et du client MQTT dans fichier de config
    - Activer l'utilisation de l'instance InfluxDB locale dans fichier de config
    - Augmenter les tailles de buffer dans data_to_local_influxdb
  - Pas sûr que l'on veuille intégrer ces modifs là dans le repo
  - Version du programme lancée
  - Fonctionne sans problème particulier
  - Intégré
- SmartSense: Rédiger un fichier de config Ansible pour le noeud SmartSense
  - Références
    - https://dev.to/pencillr/test-ansible-playbooks-using-docker-ci0
    - https://medium.com/@abhi.in/setting-up-an-ssh-server-within-a-docker-container-a-step-by-step-guide-4af15fe8787e
    - https://askubuntu.com/questions/46424/how-do-i-add-ssh-keys-to-authorized-keys-file
  - Existe un mode pull
    - Est-ce que cela simplifierait le déploiement d'un nouveau noeud ?
  - A trouvé le moyen de faire crash ma machine en jouant avec docker/podman
  - J'ai perdu mes notes dans l'affaire, recommençons
  - Liste de tous les modules Ansible
    - https://docs.ansible.com/ansible/latest/collections/index_module.html
  - Problème avec git
    - https://docs.ansible.com/ansible/latest/collections/ansible/builtin/git_module.html
    - Permet de cloner le repo
    - Mais gitlab nécessite une authentification, même pour seulement cloner en HTTPS
    - Module ne permet pas de spécifier les credentials via paramètres
    - Utiliser ssh ?
      - Comment gérer la clé utilisée dans ce cas ?
    - Suis tombé sur la notion de Project Access Token
      - https://docs.gitlab.com/ee/user/project/settings/project_access_tokens.html
      - Permettent de conférer un ensemble de droits
      - Notamment read_repository qui donne un droit d'accès en lecture seule au repo
      - Peut alors cloner un projet en utilisant l'url
        - https://<username>:<project_access_token>@<repo_url>.git
      - Où
        - username, une string quelconque, non-vide
          - i.e. peut mettre n'importe quoi tant qu'on met quelque chose
        - project_access_token, le token lui-même
    - A créé un token
    - Mais n'arrive toujours pas à cloner le repo
      - Pas autorisé à cloner le repo
      - Le token est bel et bien valide
        - Obtiens un message d'erreur différent si je modifie la valeur du token par une valeur random
    - Donc plutôt un problème de droits donnés par le token itself
      - https://docs.gitlab.com/ee/user/permissions.html
      - J'avais créé un token <guest,read_repository>
      - Pas suffisant pour un projet privé
      - Dois plutôt associer le rôle reporter au token
    - A recréé un nouveau token
      - Cette fois-ci avec le couple <reporter,read_repository>
    - Cette fois-ci, le token est fonctionnel
    - Pose dorénavant la question de la gestion des secrets
  - Problème avec systemd
    - Concerne plutôt mon setup de test avec le conteneur docker
    - La philosophie de Docker correspondant à un service par conteneur
    - Un conteneur n'utilise pas systemd par défaut
    - Donc quand j'essaie de démarrer influxdb comme un service, échoue
  - Problème avec le influxdb
    - Concerne plutôt les différents modules pour influxdb
    - influxdb_user a pour dépendance le package python `requests`
      - https://docs.ansible.com/ansible/latest/collections/community/general/influxdb_user_module.html#ansible-collections-community-general-influxdb-user-module
    - Mais rencontre des difficultés à installer cette dépendance
      - Besoin de créer un env virtuel si je veux l'installer avec pip
      - Mais pas moyen de préciser l'env virtuel à utiliser au module influxdb_user
    - Installer les dépendances via les paquets debian plutôt ?
    - Ou ne pas utiliser ce module ansible ?
    - Viens de retomber sur la commande pipx inject --include-apps ansible <package>
    - C'est p-e ça qu'il faut que j'utilise
    - Nope, ça ne corrige pas mon problème
    - C'est bien sur la machine target que je dois installer les dépendances du module
    - Fonctionne avec les paquets debian
      - Mais pas trop fan d'installer des paquets juste pour un plugin
    - Me paraît mieux d'utiliser l'API HTTP
      #+BEGIN_PLAIN
curl -XPOST 'http://localhost:8086/query?u={{ influxdb_username }}&p={{ influxdb_password }}' --data-urlencode "q=CREATE USER {{ influxdb_username }} WITH PASSWORD '{{ influxdb_password }}' WITH ALL PRIVILEGES"
curl -XPOST 'http://localhost:8086/query?u={{ influxdb_username }}&p={{ influxdb_password }}' --data-urlencode "q=CREATE DATABASE SensorData WITH SHARD DURATION 1h NAME short_duration"
      #+END_PLAIN
    - À tester
- SmartSense: Monitorer fonctionnement de l'application sans workaround sur nouveau OS
  - A redémarré l'application <2024-04-22 Mon> dans la matinée
  - À mon arrivée <2024-04-23 Tue>, noeud et application toujours en fonctionnement
  - Mais clavier ne répondait plus
  - Après un hard reset du noeud, fonctionne de nouveau
  - Après redémarrage du noeud et lancement d'InfluxDB
    - i.e. main_app ne tourne pas pour le moment
    - Consommation mémoire : 255Mo (16,3%)
    - Consommation disque : 543Mo
      - Pour info, occupe que 4,2G des 28G de la carte
- Réunion avec Guillaume
  - LivingFog
    - Reprendre LivingFog
    - Essayer de déployer sur un couple de RPi
      - Repo: https://github.com/FogGuru/livingfog
      - Installation Kubernetes
      - Configuration Kubernetes
    - Voir pour exciser la partie concernant LoRaWAN
    - Évaluer si on repart from scratch pour le déploiement ou si on peut juste retirer les morceaux
    - Faire retours sur la doc
      - http://www.fogguru.eu/livingfog/
- Discussion avec Matthieu
  - Ansible
    - M'explique que la gestion des secrets est compliquée
    - Lui utilise un fichier de variables, partagé via un autre channel
    - Possibile d'enregistrer des secrets dans GitLab
      - Si met en place de la CD
      - Vois pas trop comment on pourrait mettre cela en place avec nos noeuds autonomes
      - Curieux aussi de comment setup ansible dans un tel contexte, i.e. pour permettre au noeud de CD de s'authentifier auprès des machines cibles
  - CI
    - M'explique qu'il répète les étapes nécessaires pour setup son application dans chaque phase du pipeline
    - Mécanisme de cache permet de limiter le coût de tout réinstaller à chaque fois
    - Devrais partir sur cette approche
    - Raffinera par la suite si besoin et le temps
- LivingFog: Automatiser la création d'un cluster de Raspberry virtualisé
  - Guillaume m'a donné un PicoCluster pour travailler
  - Un cluster virtuel serait utile, mais du coup plus une priorité
- LivingFog: Déployer LivingFog sur le PicoCluster
  - Le site pointe vers une documentation technique
    - http://www.fogguru.eu/wp-content/uploads/2021/04/The-LivingFog-Platform.pdf
  - Me parait bien pour commencer
  - Indique qu'il existe des configurations Kubernetes pour 5 ou 10 noeuds
  - Dans mon cas, vise 2
  - Devra sûrement adapter cela
  - Schéma suivant représente l'architecture système de LivingFog
    - [[file:img/2024-04-24-living-fog-architecture.png]]
  - Composé de clusters
    - Dispose d'un broker MQTT qui sert de point d'entrée et qui permet aux services de communiquer entre eux
    - Dispose de Chirpstack
      - Ensemble d'applications liées à LoRaWAN
      - Chirpstack Application Server
        - Qui ne s'occupe pas du tout de faire tourner l'application
        - Mais de gérer le système
          - Lister les différents composants du système (sensors, gateways)
          - Mais aussi users, organizations et applications
        - Gère aussi le chiffrement des communications
      - Chirpstack Network Server
        - Qui s'occupe principalement de dé-dupliquer les messages reçus
          - Les capteurs émettent leurs messages via LoRa
          - Toute gateway LoRa recevant un message le retransmet
          - Ainsi, si plusieurs gateways reçoivent un même message, elles ré-émettront toutes le même message
          - Le Network Server centralise les messages reçus et les dédupliquent
        - Filtre aussi les messages pour ne conserver que les notres
          - Comme dit précédemment, toute gateway LoRA recevant un message le retransmet
          - Même ceux provenant de devices nous appartenant pas
            - e.g. provenant d'un système déployé au même endroit par une autre organisation
    - Plusieurs autres composants déployés sont en fait des dépendances de Chirpstack
      - Postgres SQL, utilisé par Chirpstack Application Server
      - Redis, utilisé par Chirpstack Network Server
    - Le Preprocessor Python Code semble lui aussi lié à LoRaWAN
      - Fait le pont entre Chirpstack et InfluxDB
        - Traduit l'hexadecimal produit par Chirpstack pour InfluxDB
      - Description du service mentionne qu'il génère aussi les trames MQTT correspondantes
      - Et les re-fournit au broker
      - N'apparaît pas sur la figure
      - Curieux des services à qui ces nouvelles trames sont destinées
    - Dispose d'une instance InfluXDB
      - Utilisée pour stocker les données de manière pérenne
    - Dispose d'une stack pour le monitoring
      - Monitore le fonctionnement
        - De chaque machine du cluster
        - Du cluster Kubernetes itself
        - Et des Kubernetes namespaces
          - Qu'est-ce ?
          - Semble être une feature permettant d'isoler différents groupes de ressources les uns des autres
            - Pour du multi-tenant par exemple ?
      - Mentionne les composants suivant
        - Prometheus
        - Kube-state-metrics
        - Node exporter, qui a l'air d'être un service de Prometheus
        - Mentionne node-port, mais pas réussi à identifier clairement ce dont il s'agit
  - Les différents clusters remontent leurs données à un Server PC
    - Serveur hébergeant une instance d'InfluxDB
    - Une instance de Prometheus fédérant celles des clusters
    - Et un grafana exposant les données de tout cela
    - Que signifie le PC ?
  - Intéressant de remarquer que les communications ne sont qu'uni-directionnelle pour le moment
    - Des clusters au Server PC
  - Pose la question de si j'ai besoin de déployer ce Server PC pour Living Fog
    - Est-ce un composant optionnel du système ?
      - Je suppose
    - Est-ce qu'un Server PC est actuellement déployé ?
      - Je doute
  - A démarré le picocluster
    - Machine principale a pour IP 131.254.160.97
      - IPs des autres ?
    - Suis con, c'est sûr INRIA-interne que la RPi est co
    - Pas besoin que la machine soit whitelist pour ça
    - A tenté de la connecter en filaire, sans succès
      - Machine n'est donc probablement pas whitelist
    - L'ensemble des machines ne doit donc pas avoir accès au réseau
    - C'est Ammar qui a setup la machine
      - Qu'est-ce qu'il faisait avec ?
        - Juste l'install
      - Pas de SSH ?
        - Nope
- LivingFog: Mettre en place réseau de RPis
  - Pour les RPis, Mulugeta propose de leur attribuer des @IP fixes
    - 192.168.1.1[0-9]
    - Configuration correspondante à saisir dans /etc/network/interfaces
      #+BEGIN_PLAIN
auto eth0
iface eth0 inet static
address 192.168.1.10
netmask 255.255.255.0
gateway 192.168.1.1
dns-nameservers 192.168.1.1 8.8.8.8
      #+END_PLAIN
  - Ne comprends pas trop à quoi correspondent l'adresse 192.168.1.1 utilisée ici
  - La machine hôte ?
    - Est-ce qu'une machine peut servir de gateway ?
    - Oui : https://www.baeldung.com/linux/network-gateway
  - Peut sinon simplement être un routeur
  - Mais par contre dans le cadre du réseau de l'IRISA, Guillaume m'expliquait que c'était détecté et coupé par le SI
  - Dans notre cas de figure, on souhaite donc éviter ce comportement
  - Connecter chaque machine à Internet, e.g. via Inria-Interne ?
  - Permettrait même de rendre l'install du cluster autonome
  - Depuis l'une machine, on exécute Ansible pour configurer le cluster
    - Bien plus simple
  - Dans ce cas, configuration requise
    - Pour chaque RPi
      - RPi OS
      - Avec Python3
        - Dispo par défaut avec RPi OS Lite
      - Avec le serveur ssh d'activé
      - Avec une adresse ip fixe pour l'interface eth0
      - Et un hostname
      - Avec une connexion internet via INRIA-interne
      - Avec la clé ssh publique de la RPi principale comme ~/.ssh/authorized_keys
    - Pour la RPi principale
      - Avec un couple de clés ssh défini
      - Avec Ansible
  - Procédure pour setup
    - Pour chaque RPi
      - Installer RPi OS
        - Set hostname
        - Enable ssh lors de l'install
      - Comme le système de fichiers de l'image système est ensuite accessible, peut
        - Définir une adresse ip fixe pour l'interface ethernet
          - Dans /etc/network/interfaces.d/picocluster
            #+BEGIN_PLAIN
auto eth0
iface eth0 inet static
address 192.168.1.1[0-9]
netmask 255.255.255.0
            #+END_PLAIN
    - Démarrer le cluster
    - Sur la RPi principale
      - Récupérer le certificat pour INRIA-interne
      - Générer paire de clés ssh
      - Renseigner clé publique comme authorized_keys des autres RPis
    - Puis utiliser Ansible pour
      - Setup Inria-interne
      - Setup LivingFog
  - Essayons ça
    - A installé RPi OS pour pc0
    - A rédigé script pour setup connexion à INRIA-interne
    - A créé une config ethernet sur ma machine pour interagir avec le picocluster
    - De cette manière, a pu me connecter au cluster
    - Mais connexion instable
    - Vais plutôt travailler depuis pc0 dans ce cas
    - A installé RPi OS pour pc1 et pc2
    - Arrive depuis pc0 à me co en ssh sur pc1 et pc2
    - Génération de la clé ssh de pc0
      - ssh-keygen -t ed25519
    - Ajout de la clé publique comme authorized_keys des autres RPi
      - ssh-copy-id -i /home/livingfog/id_ed25519.pub 192.168.1.1[1-2]
    - Installation de pipx sur pc0
    - Exécution de pipx ensurepath
    - Exit/Login
    - Installation d'Ansible
      - pipx install --include-deps ansible
      - pipx inject --include-apps ansible argcomplete
  - Ça me paraît bon pour le setup
  - Peut revenir au déploiement de LivingFog sur ce cluster
- LivingFog: Étudier le playbook *Kubeadm Ansible Playbook*
  - Repose sur des rôles
    - Qui ont l'air d'être la feature d'Ansible pour décomposer un Playbook en différents fichiers
    - cf. https://docs.ansible.com/ansible/latest/playbook_guide/playbooks_reuse.html
      - For some use cases, simple playbooks work well. However, starting at a certain level of complexity, roles work better than playbooks.
      - A role lets you store your defaults, handlers, variables, and tasks in separate directories, instead of in a single long document.
      - Roles are easy to share on Ansible Galaxy.
      - For complex use cases, most users find roles easier to read, understand, and maintain than all-in-one playbooks.
  - [X] Docker
    - Installe le Docker engine en prenant en compte la distrib (Debian/RedHat)
    - Fixe la version de Docker utilisé
      - Pourquoi ?
    - Copie ensuite deux fichiers de config
      - Fichiers
        - docker.service.j2
        - docker.j2
      - À quoi servent ces fichiers, quelle est leur utilité ?
      - docker.j2 permet de clear plusieurs variables utilisées dans sa commande de démarrage
        - INSECURE_REGISTRY
        - DOCKER_OPTS
      - Et permet de spécifier le driver utilisé pour le stockage
      - docker.service.j2 permet de spécifier la commande utilisée pour démarrer le service docker
      - Et notamment de spécifier le host du daemon
    - Modifie ensuite à la volée le fichier docker.j2 pour spécifier de nouveaux registres
    - Finalement, démarre le service Docker
  - [X] Commons
    - Regroupe des helpers définit pour simplifier l'écriture des tâches principales
    - Chargés comme dépendances via le meta/main.yml de chaque rôle
  - [ ] Kubernetes-master
    - Utilise kubeadm pour déployer le cluster
      - Donc installe la version "vanilla" de kubernetes ?
      - Ça serait pas mieux k3s pour les RPis ?
      - Quoique les RPi 4 sont un minimum puissantes
  - [ ] cni
  - [ ] Kubernetes-node
  - [ ] Helm
  - [ ] MetalLB
  - [ ] Healthcheck
  - [ ] GlusterFS-node
  - [ ] GlusterFS-master
** Semaine du <2024-04-15 Mon> au <2024-04-19 Fri>
*** Planned
**** DONE SmartSense: Créer image système pour le noeud
CLOSED: [2024-04-17 Wed 17:09]
- Besoin de créer une nouvelle image pour
  - Passer à un OS 64-bits et ainsi avoir un OS supporté par InfluxDB
  - Obtenir les dernières MàJ et les derniers drivers pour dongle Wi-Fi
- A appris qu'on peut travailler à même le noeud
  - Suffit de démonter la vitre et de sortir du boitier le noeud
  - A alors accès à la connectique pour brancher écran/clavier/souris
- Peut donc travailler de manière très itérative
  - Créer une image de base
  - Installer InfluxDB
  - Setup l'application
  - Démarrer avec cette configuration et voir erreur(s) rencontrée(s)
**** DONE SmartSense: Incorporer les correctifs de l'application au repo
CLOSED: [2024-04-18 Thu 14:10]
- Lors du déploiement de la dernière version de l'application, a rencontré plusieurs problèmes
  - Modification de l'appel au constructeur INA dans ina219_manager
    - Utilise désormais le `busnum=1`
  - Modification de l'API de pyftdi dans ftdijtag_manager
    - Utilise désormais des valeurs issues de l'enum BitMode comme second paramètre pour `set_bitmode`
  - Modification de l'API de Thread dans ina219_manager
    - Utilise désormais la méthode `is_alive()`
  - Modification de la méthode `__produce_msgs()` de usb_board_manager
    - Itère désormais sur une liste temporaire des items du dict des serial_devices plutôt que sur le dict directement
  - Modification du constructeur de MainApp dans le fichier éponyme
    - Désactive les timers et appels à `kick_watchdog()`, `send_alive_message()` et `send_ip_address()` en cas de non-déploiement d'un broker MQTT
  - Modification de `main()` dans main_app
    - Ajout d'un appel à `start_process()` pour démarrer notre instance de `InfluxDBInterface`
  - Modification de pyproject.toml
    - Changer la version de Python supportée à la "^3.11"
      - Version par défaut de la version courante de Raspberry OS
    - Déclarer une tâche pour exécuter l'application
**** DONE SmartSense: Identifier l'origine du log "failed to store statistics"
CLOSED: [2024-04-18 Thu 15:31]
- De manière périodique, toutes les ~40s, le noeud log ce message
  - influxd-systemd-start.sh: ts=<timestamp> lvl=info msg="failed to store statistics" log_id=<id> service=monitor error=timeou
- Identifier son origine et résoudre le problème
**** DONE SmartSense: Monitorer fonctionnement de l'application sans workaround sur nouveau OS
CLOSED: [2024-04-19 Fri 09:02]
**** DONE SmartSense: Rendre optionnel le mécanisme du workaround pour les crashs d'InfluxDB
CLOSED: [2024-04-19 Fri 16:07]
- Pour faciliter les tests/comparaisons entre le comportement du noeud avec ou sans ce workaround
- Et pour unifier le software des noeuds centralisés et autonomes
- Ça serait plus simple de mettre en place un flag permettant d'activer/désactiver le workaround
**** DONE SmartSense: Corriger le formatage des entrées pour InfluxDB
CLOSED: [2024-04-19 Fri 16:07]
- InfluxDB définit et utilise le line protocol comme format d'entrée pour les données
  - https://docs.influxdata.com/influxdb/v1/write_protocols/line_protocol_reference/
- Le noeud formate donc les données récupérées de la sorte
- En expérimentant avec le stockage des données sous forme de fichier, me suis rendu compte qu'on gérait incorrectement les chaînes de caractères
  - Pas de ""
  - cf. [[file:files/influxdb-example-import.txt]]
- Dans la majorité des cas, InfluxDB accepte tout de même les données
- MAIS rencontré une erreur bloquante dans le cas de la valeur `unknow` pour le champ `leptonVersion`
  - [[file:files/influxdb-example-import.txt]]:69
- Lors de l'import de cette ligne, InfluxDB rencontrait une erreur
  - L'entrée était perdue
  - Et la commande d'import indiquait que l'ensemble des entrées n'avaient pas pu être insérées
  - Dans les faits, seule la ligne incriminée avait l'air manquante
- Identifier où le formatage s'effectue
- Corriger la gestion des chaînes de caractère
**** IN-PROGRESS SmartSense: Traiter les problèmes relevés lors de l'uniformisation du code
- J'ai noté plusieurs points problématiques au cours de mon passage sur l'ensemble des modules
  - Le module *sensor_board* est inutilisé
    - Code obsolète à supprimer ?
  - La librairie *telnetlib* est dépréciée
    - Passer à https://pypi.org/project/telnetlib3/ ?
  - Une conditionnelle étrange est effectuée dans le module *tlc59108_manager*
    - https://gitlab.inria.fr/smartsense/3douest/node-app/smartsense-node-app/-/blob/3feb5c56dc5e74dcea9eb2f2ba98bec40029acdf/TLC59108Manager.py#L370
    - Revient à `if data[0]`
    - `and` est-il l'opérateur désiré ?
    - Simplifier/corriger
  - Les appels à *OpenOCDManager.update()* dans *test_ocd* manquent d'un paramètre
    - J'ai l'impression que c'est le paramètre *board*
    - Ajouter les valeurs correctes ?
  - *USBBoardManager.read_data()* et *parse_to_mqtt()* sont particulièrement sujets aux retours de Pylint
    - Trop d'instructions
    - Trop de branchements
    - Trop de blocs imbriqués
    - Refactorer pour améliorer la lisibilité du code ?
    - Voir si on peut aussi factoriser du code à l'aide de fonctions
  - Corriger ces problèmes
    - Ou si pas de solution claire et rapide, créer une issue correspondante
**** IN-PROGRESS SmartSense: Mise en place d'un processus de CI
- Mettre en place des tests, c'était un bon début
- Mettre en place un processus de CI, ça serait mieux
  - Garantirait la reproductibilité de l'environnement de tests
  - Et que j'oublie pas de les faire tourner avant l'intégration
- Voir comment GitLab gère ça
**** IN-PROGRESS LivingFog: Automatiser la création d'un cluster de Raspberry virtualisé
- Plutôt que de travailler avec le cluster physique
- Serait intéressant d'apprendre à provisionner un cluster virtuel de Raspberry
- Permettrait de pouvoir créer un environnement pour dev et effectuer des tests
  - Notamment comment setup k8s ou équivalent sur un tel système
**** TODO SmartSense: Suggestions
- Mécanisme d'export de la BD
  - Remplacer `backup_index` par l'identifiant du noeud
- Supprimer le code relatif à hwclock
  - Notre noeud SmartSense ne dispose pas d'une horloge BIOS
  - Ce code semble donc sans effet et s'avère déroutant
*** Done
- SmartSense: Créer image système pour le noeud
  - Installation manuelle
    - A installé la dernière version de Raspberry OS
      - Basée sur Debian 12
      - https://downloads.raspberrypi.com/raspios_lite_arm64/images/raspios_lite_arm64-2024-03-15/2024-03-15-raspios-bookworm-arm64-lite.img.xz
    - A MàJ le système
      - sudo apt update
      - sudo apt upgrade -y
    - Installation des dépendances
      - sudo apt -y install git python3-dev pipx
    - Configuration de pipx et installation outils Python
      - pipx ensurepath
      - source ~/.bashrc
      - pipx install poetry
      - pipx install poethepoet
    - A installé InfluxDB
      - wget https://dl.influxdata.com/influxdb/releases/influxdb_1.8.10_arm64.deb
      - sudo apt install ./influxdb_1.8.10_arm64.deb
    - A modifié la config InfluxDB
      - Dans /etc/influxdb/influxdb.conf
      - index-version = "tsi1"
      - max-concurrent-compactions = 1
    - A DL l'application
      - git clone https://gitlab.inria.fr/smartsense/3douest/node-app/smartsense-node-app.git
    - A installé les dépendances de l'application
      - poetry install --no-root
  - En a profité pour créer la configuration adéquate pour se co à Eduroam lorsqu'utilise la RPi 3
    - nmtui a permis de créer le gros de la config
      - Profile name: Config Wifi
      - Device: wlan0
      - SSID: eduroam
      - Mode: client
      - Security: WPA & WPA2 Enterprise
      - Authentication: PEAP
      - Anonymous identity: anonymous@inria.fr
      - Domain: inria.fr
      - CA cert: file:///home/mnicolas/Chain-TCS-4-OV.pem
      - CA cert password: <vide>
      - PEAP version: Automatic
      - Inner authentication: MSCHAPv2
      - Username: mnicolas
      - Password: <password>
      - Reste vide
    - nmcli con up "Config Wifi"
  - Rencontre une erreur à ce stade
    - Liée au fait que la RPi est à Python 3.11
    - Alors que mon PC est à Python 3.12
    - Et que donc la config du pyproject.toml que j'ai fait utilise cette version
    - Erreur
      #+BEGIN_PLAIN
$ poetry install
The currently activated Python version 3.11.2 is not supported by the project (^3.12).
Trying to find and use a compatible version.

Poetry was unable to find a compatible version. If you have one, you can explicitly use it via the "env use" command.
      #+END_PLAIN
  - Peut downgrade la version de python indiquée comme supportée par l'application dans le pyproject.toml
  - Mais reporte le problème
    #+BEGIN_PLAIN
$ poetry install
Installing dependencies from lock file

Package operations: 21 installs, 0 updates, 0 removals

  - Installing spidev (3.6): Failed

  ChefBuildError

  Backend subprocess exited when trying to invoke build_wheel

  /tmp/tmpfud7f8a0/.venv/lib/python3.11/site-packages/setuptools/dist.py:476: SetuptoolsDeprecationWarning: Invalid dash-separated options
  !!

          ********************************************************************************
          Usage of dash-separated 'description-file' will not be supported in future
          versions. Please use the underscore name 'description_file' instead.

          By 2024-Sep-26, you need to update your project and remove deprecated calls
          or your builds will no longer be supported.

          See https://setuptools.pypa.io/en/latest/userguide/declarative_config.html for details.
          ********************************************************************************

  !!
    opt = self.warn_dash_deprecation(opt, section)
  running bdist_wheel
  running build
  running build_ext
  building 'spidev' extension
  creating build
  creating build/temp.linux-x86_64-cpython-311
  x86_64-linux-gnu-gcc -Wsign-compare -DNDEBUG -g -fwrapv -O2 -Wall -g -fstack-protector-strong -Wformat -Werror=format-security -g -fwrapv -O2 -fPIC -I/tmp/tmpfud7f8a0/.venv/include -I/usr/include/python3.11 -c spidev_module.c -o build/temp.linux-x86_64-cpython-311/spidev_module.o
  spidev_module.c:28:10: fatal error: Python.h: No such file or directory
     28 | #include <Python.h>
        |          ^~~~~~~~~~
  compilation terminated.
  error: command '/usr/bin/x86_64-linux-gnu-gcc' failed with exit code 1


  at ~/.local/pipx/venvs/poetry/lib/python3.11/site-packages/poetry/installation/chef.py:164 in _prepare
      160│
      161│                 error = ChefBuildError("\n\n".join(message_parts))
      162│
      163│             if error is not None:
    → 164│                 raise error from None
      165│
      166│             return path
      167│
      168│     def _prepare_sdist(self, archive: Path, destination: Path | None = None) -> Path:

Note: This error originates from the build backend, and is likely not a problem with poetry but with spidev (3.6) not supporting PEP 517 builds. You can verify this by running 'pip wheel --no-cache-dir --use-pep517 "spidev (==3.6)"'.
    #+END_PLAIN
  - Arrive à reproduire le problème avec docker, en se basant sur python:3.11
  - Et ne le rencontre pas en me basant sur python:3.12
  - Semblerait que lié à l'absence d'headers
    - cf. https://stackoverflow.com/questions/21530577/fatal-error-python-h-no-such-file-or-directory
  - Problème résolu en installant le paquet debian python3-dev
    - Ajout de la commande au process décrit ci-dessus
  - Nouvelle erreur au lancement
    - Commande : poetry run python3 main_app.py
    - Stacktrace
      #+BEGIN_PLAIN
Traceback (most recent call last):
  File "/home/app/smartsense-node-app/main_app.py", line 612, in <module>
    main()
  File "/home/app/smartsense-node-app/main_app.py", line 538, in main
    Utils.kick_watchdog()
  File "/home/app/smartsense-node-app/utils.py", line 20, in kick_watchdog
    wdt = open("/dev/watchdog")
          ^^^^^^^^^^^^^^^^^^^^^
PermissionError: [Errno 13] Permission denied: '/dev/watchdog'
      #+END_PLAIN
    - Pas déconnant comme erreur
  - Le problème étant que je rencontre ensuite un problème de virtualenv si j'exécute la même commande en sudo
    - Poetry/PoeThePoet ont été installé uniquement pour l'utilisateur courant (mnicolas)
      - Accessibles dans /home/mnicolas/.local/bin/
    - À l'exécution de la commande en tant que root, ne trouve pas Poetry, ni l'environnement virtuel associé au projet
  - Ré-installe les dépendances mais en tant que root ce coup-ci
    - sudo su
    - pipx ensurepath
    - exit
    - sudo su
    - pipx install poetry
    - pipx install poethepoet
    - poetry install --no-root
  - Démarre désormais, mais plante car n'accède pas au fichier de config
  - Comportement étrange, le noeud redémarre quelques secondes après
    - Pourquoi ?
    - N'a pas l'air d'être lié au timeout de wait_provisioning()
  - Test avec un fichier placeholder
    - Progresse jusqu'à l'erreur suivante
    - Erreur au niveau du bus I2C
    - Mais toujours le même problème de crash/reboot de la RPi quelques secondes après l'interruption du programme Python
  - A activé le port I2C
    - En interactif
      - sudo raspi-config
    - En commande
      - sudo raspi-config nonint do_i2c 0
  - Active bien le port
    - ls /dev/i2*
      - /dev/i2c-1 /dev/i2c-2
    - lsmod | grep i2c
      - i2c_dev      16384  0
      - i2c_bcm2835  16384  0
  - Mais rencontre toujours la même erreur
  - La page de la librairie INA indique une cause possible de l'erreur
    - https://pypi.org/project/pi-ina219/
    - Paramètre `busnum` manquant dans le constructeur de l'objet INA
    - Ajout du paramètre `busnum=1` dans le constructeur de l'objet INA, dans ina219_manager.py
    - Fix le problème
  - Rencontre désormais une erreur avec collectd
    - Installation de collectd
      - sudo apt install collectd
    - La config par défaut est quasi identique à celle du noeud SmartSense
    - Seule la configuration du plugin network a été modifié
    - Pour le moment, la laisse par défaut
  - Continue de planter
  - Me suis penché un peu plus en détails sur les logs
    - Par défaut, n'utilise pas syslog
    - Mais systemd-journald
      - Qui a l'air d'être l'outil utilisé désormais
    - Peut facilement lire les derniers logs graĉe à la commande suivante
      - journalctl -e
    - Mais les logs de l'application brillent par leur absence
      - Retrouve des logs d'InfluxDB
        - Requêtes exécutées à la main
        - Message périodique "failed to store statistics"
      - Retrouve un log fait à la main
        - A exécuté un script custom important le module syslog et l'utilisant pour log un message
      - Mais pas de log habituel
    - Alex me fait remarquer que les logs de journald sont des binaires
      - Est-ce que ça implique des traitements interrompus/de la corruption de logs lors de crash ?
  - A fait tourner quelques tests sur la machine
    - Tests concernés
      - test_data_to_local_influxdb_parse_mqtt
      - test_data_to_local_influxdb_send_points
      - test_main_app_handle_provisioning_msg
    - Fonctionnent correctement
    - Font bien apparaître dans le journal des logs issus de l'application
      - Envoi des points à InfluxDB
      - Provisioning de MainApp
    - A déclenché de manière surprenante des traitements
      - Découvertes de périphériques USB
    - Logs de mtp-probe indiquent que les périphériques sur le bus, device 7 à 12, ne sont pas des MTP devices
      - Qu'est-ce que cela indique ?
      - Semble correspondre au Media Transfer Protocol, i.e. le protocol permettant d'échanger des fichiers entre un ordi et un device indépendant connecté en USB, genre Android
  - Trouvé la cause du redémarrage intempestif du noeud
    - S'agit du watchdog
    - Consiste en un système similaire à un heartbeat
    - Tant qu'on le kick régulièrement, ne fait rien
    - Mais si n'est plus kick, reboot la machine
    - Pourquoi je n'ai pas rencontré ce phénomène sur l'ancienne version du noeud ?
      - Problème de droit ?
    - A commenté les lignes relatifs à son fonctionnement
      - Ajouter une option de configurer pour le rendre optionnel ?
  - Peut désormais lancer l'application, l'interrompre lors d'une erreur et consulter ses logs
    - Retrouve bien désormais les logs de l'application dans le journal
  - Erreur rencontrée lors de l'exécution de `mainApp.reset_sensor_board()`
    - Logs et stacktrace
      #+BEGIN_PLAIN
[FirmwareUpdateManager] resetSensorBoard failed
[FirmwareUpdateManager] exception : 'int' object has no attribute 'name'
[FirmwareUpdateManager] Traceback (most recent call last):
  File "/home/mnicolas/smartsense-node-app/firmware_update_manager.py", line 77, in reset_sensor_board
    ftdi_jtag_manager.open_and_configure_ftdi_jtag(1)
  File "/home/mnicolas/smartsense-node-app/ftdijtag_manager.py", line 89, in open_and_config_re_ftdi_jtag
    self.ftdi_jtag.set_bitmode(0xFF, 0x00)
  File "/root/.cache/pypoetry/virtualenvs/smartsense-node-app-8çLRNG6r-py3.11/lib/python3.11/site-packages/pyftdi/ftdi.py", line 1237, in set_bitmode
    self.log.debug('bitmode: %s', mode.name)

  AttributeError: 'int' object has no attribute 'name'
      #+END_PLAIN
    - Provient d'une modification dans la librairie pyftdi
      - https://github.com/eblot/pyftdi/commit/5d4be11ed8c22a5c0474ef797d08978a869ae86b
    - La fonction `set_bitmode` utilise désormais un enum
      - Valeurs utilisées correspondent à BitMode.RESET et BitMode.BITBANG
        - https://github.com/eblot/pyftdi/blob/master/pyftdi/ftdi.py#L133-L134
    - En remplaçant les entiers utilisés par l'enum, corrige l'erreur
  - Erreur rencontrée lors de l'exécution de `ina219_manager.read_all()`
    - Logs et stacktrace
      #+BEGIN_PLAIN
[INA219Manager] exit process with exception : 'Timer' object has no attribute 'isAlive'
[INA219Manager] Traceback (most recent call last):
  File "/home/mnicolas/smartsense-node-app/ina219_manager.py", line 91, in read_all
    if not t.isAlive():

  AttributeError: 'Timer' object has no attribute 'isAlive'
      #+END_PLAIN
    - Effectivement, la classe Timer, classe fille de Thread, n'a pas de propriété `isAlive`
    - Mais elle a une méthode `is_alive()`
      - https://docs.python.org/3/library/threading.html#threading.Thread.is_alive
    - Correction de l'appel
      - ina219_manager, ligne 90, était le seul endroit où on y faisait référence
  - Erreur rencontrée lors de l'exécution de `usb_board_manager.read_data()`
    - Logs et stacktrace
      #+BEGIN_PLAIN
[USBBoardManager] exit process with exception : dictionary keys changed during iteration
[USBBoardManager] Traceback (most recent call last):
  File "/home/mnicolas/smartsense-node-app/usb_board_manager.py", line 352, in read_data
    self.__produce_msgs()
  File "/home/mnicolas/smartsense-node-app/usb_board_manager.py", line 312, in __produce_msgs
    for ser, serial_device in self.serial_devices.items():
RuntimeError: dictionary keys changed during iteration
      #+END_PLAIN
    - Me souviens avoir modifié cette boucle en passant lorsque je renommais tous les modules/toutes les propriétés pour être en accord avec les conventions python
    - Possible que je me sois emmêlé les pinceaux à cette occasion
    - Effectivement, avant le code correspondait à : https://gitlab.inria.fr/smartsense/3douest/node-app/smartsense-node-app/-/blob/546da1e5e3000c3d05850789ac9c1a4222750287/USBBoardManager.py#L288-337
    - On a bien une modification du dictionnaire au cours de son parcours
      - De ce que je comprends, initialement, on ne connait pas le nom des différents MCUs
      - Les premiers échanges entre l'app et les MCUs servent alors à les identifier
      - Dans `parse_and_update_unknown_mcu()`, récupère le nom d'un MCU dans sa réponse et met à jour le dictionnaire des serial_devices en conséquence
        - https://gitlab.inria.fr/smartsense/3douest/node-app/smartsense-node-app/-/blob/main/usb_board_manager.py?ref_type=heads#L440-441
      - D'où l'erreur déclenchée
    - A recréé l'erreur sur un plus petit exemple
      #+BEGIN_PLAIN
toto = {"a": 1, "b": 2}
for key, _ in toto.items():
    if key == "a":
        toto["c"] = toto.pop(key)
      #+END_PLAIN
    - Mais rencontre aussi l'erreur indiquée sur un exemple similaire à l'ancienne version du code
      #+BEGIN_PLAIN
toto = {"a": 1, "b": 2}
for key in toto:
    if key == "a":
        toto["c"] = toto.pop(key)
print(toto)
      #+END_PLAIN
    - Plusieurs options
      - Créer une copie du dictionnaire
        - foo = dict(self.serial_devices)
      - Extraire les items du dictionnaire
        - items = [*self.serial_devices.items()]
      - Modifier le programme pour dissocier le parcours du dictionnaire et le rename de ses entrées
        - P-e garder ça pour une 2nde passe
  - Erreur rencontrée lors de l'exécution de `usb_board_manager.__enable_mcu()`
    - Logs et stacktrace
      #+BEGIN_PLAIN
[USBBoardManager] __enableMCU : 'ascii' code can't decode byte 0xed in position 50: ordinal not in range(128)
[USBBoardManager] Traceback (most recent call last):
  File "/home/mnicolas/smartsense-node-app/usb_board_manager.py", line 160, in __enable_mcu
    buffer_str = buffer.decode("ascii")

UnicodeDecodeError: 'ascii' code can't decode byte 0xed in position 50: ordinal not in range(128)
      #+END_PLAIN
  - Erreur rencontrée lors de l'exécution de `main_app.send_alive_message()`
    - Logs et stacktrace
      #+BEGIN_PLAIN
Traceback (most recent call last):
  File "/home/mnicolas/smartsense-node-app/main_app.py", line 612 in <module>
    main()
  File "/home/mnicolas/smartsense-node-app/main_app.py", line 570, in main
    my_app.send_alive_message()
  File "/home/mnicolas/smartsense-node-app/main_app.py", line 479, in send_alive_message
    self.mqtt_client_publisher_queue_in.put([topic, payload])

AttributeError: 'NoneType' object has no attribute "put"
      #+END_PLAIN
    - Comme on est en mode autonome, je n'instancie pas de queue pour MQTT
    - Mais j'ai pas désactivé les appels à différentes méthodes l'utilisant
      - `send_alive_message()`
      - `send_ip_adress()`
    - Et les timers associés
      - alive_timer
      - ip_timer
  - Plus de bug d'indiqué, mais aucune donnée n'apparait dans Influx
    - Juste oublié l'instruction pour démarrer le process gérant `InfluxDBInterface`
    - Ajout du snippet correspondant dans `main_app`
  - Fonctionne !
  - A pu consulter les premières entrées dans la base de données
  - A noté par contre la présence de timeout lors de l'envoi des données de l'application à la BD
    - J'ai ressenti aussi des lenteurs au niveau du fonctionnement du système depuis sa création
    - Problème de carte SD ?
    - A passé la taille du buffer avant écriture à 5k points pour ne pas trop tirer sur la SD
  - A lancé le programme <2024-04-17 Wed> ~17h00
    - Consommation initiale en RAM : ~300Mo (12,9%)
  - Le <2024-04-18 Thu> à 8h48
    - Application tourne toujours
    - Consommation en RAM : ~447Mo (36,6%)
    - Utilisation du swap : 97,3Mo
    - Quelques timeouts cependant
      - [DataToLocalInfluxDB - storageController] : data not saved, timeout ! {"error":"timeout"}
    - Rejoint un log distinct, émis périodiquement par influxdb
      - influxd-systemd-start.sh: ts=<timestamp> lvl=info msg="failed to store statistics" log_id=<id> service=monitor error=timeout
  - Il s'agissait par contre d'une version de l'application vidant/exportant régulièrement la base de données
  - Devrait tester une version alternative n'utilisant qu'une BD unique
- SmartSense: Réunion démo
  - Préparation
    - As-tu déjà présenté le noeud dans ce cadre d'event ?
    - Y a-t-il un scénario établi ?
    - Motivations derrière SmartSense autonome ?
      - Orientation initiale Smart Building
      - But est de pouvoir viser de nouvelles problématiques
    - Pourquoi équiper un noeud de tous ces capteurs ?
      - Scientifiques utilisent capteurs très précis (très cher)
      - Là tente d'amener un paradigme low-cost, mais tout de même essayer des données collectées
      - Plus simple de déployer board avec multitude
    - Qu'est-ce qu'on veut montrer/sur quoi insister ?
  - Réunion
    - Dans le cadre d'un café-bazar
    - Scénario
      - Faire tourner le noeud SmartSense
      - Montrer la collecte de données sur Grafana
    - Expliquer qu'en cours de dev, notamment sur stockage de données en local
    - Déploiement proche sur le campus de Beaulieu
      - Tester le logiciel
      - Tester le hardware
      - Valider les résultats
    - Discuter avec elles et eux pour voir si ça peut les intéresser
      - Collecter cartes de visites
      - Pourra les recontacter lorsque ça sera prêt
    - 10 noeuds autonomes fabriqués
      - 1 ici, 1 à Lannion, le reste chez 3DOuest
    - Combien ça coûte ?
      - Sur mesure
      - 50k conception + fabrication des 10 premiers
      - Estime à 3k le noeud autonome
        - 2k la partie autonome (batterie, boitier, carte capteurs externe)
        - 1k pour carte SmartSense + carte capteurs
    - Microphone
      - Trop coûteux d'enregistrer le flux brut
      - Traitements à base d'enveloppe sonore
        - Récupère uniquement les basses fréquences actuellement
        - Des travaux en cours pour obtenir une enveloppe conservant les hautes fréquences
      - But serait d'identifier l'origine du bruit
      - 4 micro positionnés de manière à pouvoir faire de la spatialisation sonore
    - Présence d'un télémètre optique
      - Détecte et indique la distance avec des obstacles, jusqu'à 4m, précision au cm
    - Connexion avec le noeud fog
      - Serveur de calculs plus puissant
      - Récupère et aggrège les données des différents capteurs
      - Peut effectuer des traitements avant de lui-même remonter les données/stocker les données
      - Questions de la localisation des calculs
        - Sur le noeud SmartSense, le noeud fog et le cloud/chez soi
    - Autonomie
      - Actuellement une autonomie d'une dizaines d'heures
        - Consomme ~10 Watt/h
      - Perspective
        - Ajout d'une batterie externe
        - Ajout d'un panneau solaire
    - Public pas forcément d'informaticiens
      - Adapter le discours en conséquence
      - i.e. éviter les termes un peu trop spécialisés tels que fog computing
- SmartSense: Incorporer les correctifs de l'application au repo
  - [X] Modification de l'appel au constructeur INA dans ina219_manager
    - Me rend compte en relisant la doc que la valeur pour le paramètre `busnum` n'est pas forcément 1
      #+BEGIN_PLAIN
*I2C Bus number*

In most cases this will be determined automatically, however if this fails you will see the exception:
`Could not determine default I2C bus for platform`

In this case just set the bus number in the INA219 constructor, for example:
`ina = INA219(SHUNT_OHMS, busnum=1)`

This is known to be required with Raspberry Pi 4 and the ‘Bullseye’ (October 2021) Raspberry Pi OS.
      #+END_PLAIN
    - Possède 2 bus dans notre cas, les bus 1 et 2
    - Les deux possèdent des devices connectés
      - Listes obtenus avec la commande i2cdetect -y <busnum>
      - Bus 1 : Devices sur les adresses (ports?) 40, 41, 44, 45, 46, 48 et 4b
      - Bus 2 : Devices sur les adresses 37, 3a, 50, 51, 52, 53, 54, 55, 56, 57
    - Comment déterminer quel bus utiliser ?
    - Les adresses auxquelles on s'adresse dans le cadre de ina219_manager correspondent à celles des devices sur le bus 1
      - https://gitlab.inria.fr/smartsense/3douest/node-app/smartsense-node-app/-/blob/main/ina219_manager.py?ref_type=heads#L24-30
    - Faudra que je fasse vérifier cette modif tout de même
  - [X] Modification de l'API pyftdi dans ftdijtag_manager
  - [X] Modification de l'API de Thread dans ina219_manager
  - [X] Modification de la méthode `__produce_msgs()` de usb_board_manager
  - [X] Modification du constructeur de MainApp dans le fichier éponyme
  - [X] Modification de `main()` dans main_app
  - [X] Modification de pyproject.toml
- SmartSense: Identifier l'origine du log "failed to store statistics"
  - Issue à ce sujet
    - https://github.com/influxdata/influxdb/issues/8036
  - Utilisateur-rices indiquent que le problème survient de manière inopinée chez eux
  - Et disparaît après un redémarrage
    - Après plusieurs redémarrage, pas de changements de mon côté
  - Mention d'un fix ayant été intégré dans 1.7.6
  - Sauf que j'ai installé la version 1.8.10 là
  - Commentaire ayant reçu plusieurs validations conseille de modifier la configuration par défaut d'InfluxDB
    - [data]
      - wal-fsync-delay = "10s"
    - [coordinator]
      - write-timeout = "30s"
  - Effectivement, ne constate plus le problème après modifications et redémarrage
- SmartSense: Rendre optionnel le mécanisme du workaround pour les crashs d'InfluxDB
  - Pars sur l'ajout d'un paramètre booléen `is_periodic_export_enabled` au constructeur `DataToLocalInfluxDB`
  - À partir de cette nouvelle propriété, détermine si exécute les méthodes
    - __export_db()
    - __drop_db()
    - __create_db()
  - Warning
    - Doit exécuter au moins à l'initialisation la méthode `__create_db()`
    - Pour créer la database SensorData, si elle n'existe pas déjà
    - Et configurer le client InfluxDB pour écrire dans cette base
  - Fonctionne
  - Me pose maintenant la question de s'il faudrait remanier le fichier config.ini
  - On pourrait mettre en place une catégorie [InfluxDB] avec les différentes valeurs à renseigner
    - i.e. influxDB_embedded, username, password et  periodic_export
  - On pourrait faire la même pour [MQTT] en vrai
  - Mérite sa tâche dédiée
  - Par contre, à quoi sert `backup_index` ?
  - Reset à 0 à chaque démarrage de l'application
    - Donc on ne peut pas s'y fier pour identifier et ordonner les exports
  - Présence d'un timestamp dans le nom de l'export
    - Qui lui devrait être strictement croissant par noeud
  - Remplacer `backup_index` par l'identifiant du noeud plutôt ?
- SmartSense: Monitorer fonctionnement de l'application sans workaround sur nouveau OS
  - Démarré le <2024-04-18 Thu> à ~15h46 (<2024-04-17 Wed> 23h56, heure du noeud)
  - Consommation en RAM initiale : ~215Mo
  - À ~16h43, consommation en RAM : ~396Mo (23,0%)
  - À ~17h53, consommation en RAM : ~430Mo (27,5%)
  - Le <2024-04-19 Fri> à ~08h55
    - Application toujours fonctionnelle
    - Consommation en RAM : ~412Mo (24,6%)
  - Vois néanmoins passer dans les logs des timeouts de temps en temps
    - Que ça soit lors de l'envoi périodique des données collectées
    - Ou juste un log d'InfluxDB itself concernant l'échec de du stockage de ses statistiques
- SmartSense: Mise en place du dongle Wi-Fi TP-Link TL-WN823N
  - J'ai récupéré via le SI un dongle Wi-Fi
    - https://wikidevi.wi-cat.ru/TP-LINK_TL-WN823N_v1 ou https://wikidevi.wi-cat.ru/TP-LINK_TL-WN823N_v2
    - La commande `lsusb` m'indique que c'est la v2/v3 utilisant le chipset Realtek RTL8192EU
  - Étonnament, plug-and-play
  - Noeud s'est connecté automatiquement à eduroam en utilisant la config que j'avais mis en place sur la RPi 3 de Guillermo
  - Pourra donc l'utiliser pour réaliser des tests avec l'antenne
  - Rencontre par contre des problèmes d'instabilité au fur et à mesure de l'exécution de l'application
  - A perdu la connexion et noeud n'arrive pas à se reconnecter
  - Raison évoquée dans les logs est que l'authentification prendrait trop de temps
  - Raccord avec la lenteur du système/les timeouts que l'on rencontre par intermittence
  - A réussi à me reconnecter sans encombre après redémarrage du système
- SmartSense: Corriger le formatage des entrées pour InfluxDB
  - leptonVersion=unknow
    - Donnée provient de `usb_board_manager`
    - Concerne le capteur thermalImgHd
    - De ce que j'observe, caméra rencontre un problème de temps en temps
    - Lorsque son status="NOK", leptonVersion=unknow
    - Le problème étant qu'on a potentiellement déjà enregistré des données pour leptonVersion
      - e.g. leptonVersion=2.0 (float)
    - Tenter d'enregistrer des entrées avec leptonVersion="unknow" échoue alors car type ne concorde pas
    - Propose de retirer cette valeur particulière pour ce champ
    - On pourra toujours l'inférer de status="NOK" si nécessaire
  - meta_consumption
    - Obtient aussi des erreurs d'écritures dans Influx pour les entrées de ce measurement
    - En jetant un oeil, observe que c'est moi qui a cassé le format de ces entrées
      - Ajout d'un espace de trop entre deux champs
** Semaine du <2024-04-08 Mon> au <2024-04-12 Fri>
*** Planned
**** DONE SmartSense: Monitorer noeud SmartSense sans stockage des entrées lourdes
CLOSED: [2024-04-10 Wed 15:00]
- Plusieurs types d'entrées sont bien plus volumineux que les autres
  - De plusieurs ordres de grandeur (jusqu'au ko par entrée)
- Entrées concernées
  - event_thermalImgHd
  - event_stream
- Et dans une moindre mesure
  - event_thermalImgLd
  - event_rfGiga
- Tester si ne pas stocker ces entrées stabilise le système
**** DONE SmartSense: Identifier un adapteur Wi-Fi adapté
CLOSED: [2024-04-10 Wed 15:03]
**** DONE SmartSense: Créer repos pour ADT SmartObs
CLOSED: [2024-04-10 Wed 15:10]
- Besoin d'un repo pour conserver les différents CRs de réunions
- Mickäel suggère aussi de mettre en place un miroir de mon reporting au même endroit
  - De façon à centraliser ce qui a trait au projet
- Permettra si besoin de rajouter de nouveaux repos
**** DONE SmartSense: Écrire un script permettant de rejouer une trace du noeud SmartSense
CLOSED: [2024-04-10 Wed 17:45]
- inch ne permet pas de reproduire correctement la charge générée par le noeud SmartSense sur son instance InfluxDB
  - Notamment, ne permet pas de reproduire le flux de blob
- Au cours de mes essais visant à remplacer l'instance InfluxDB par un fichier, j'ai produit
  - Un fichier txt de 1.7Go de données collectées au format line protocol
  - Un fichier txt.gz de 2.8Go similaire
- Pourrait mettre en place un script feedant ces données à une instance InfluxDB
- Permettrait p-e d'avoir un moyen automatisé de faire planter le noeud et les VMs
**** DONE SmartSense: Exécuter le script sur le noeud SmartSense
CLOSED: [2024-04-11 Thu 10:42]
- Avant toute chose, il serait bien de valider que le script et la trace obtenu produit le crash sur le noeud
- Exécuter le script avec le noeud SmartSense comme destinataire pour confirmer cela
  - ssh -L 8086:192.168.1.2:8086 pi@192.168.1.2
**** DONE SmartSense: Créer issue au sujet de la dépréciation de telnetlib
CLOSED: [2024-04-11 Thu 14:35]
- Librairie dépréciée
  - c.f. https://peps.python.org/pep-0594/#deprecated-modules
- Nécessaire de passer à une des librairies suggérées
- Créer l'issue correspondante
**** DONE SmartSense: Monitorer noeud SmartSense sous workload par inch
CLOSED: [2024-04-12 Fri 09:02]
- En théorie, devrait se comporter de manière similaire à la VM 32-bits
- Serait intéressant de le confirmer
**** DONE SmartSense: Exécuter le script sur VM 64-bits
CLOSED: [2024-04-15 Mon 08:43]
- Maintenant que j'ai observé le script produire le crash sur le noeud SmartSense
- Peut l'exécuter en utilisant l'instance InfluxDB d'une VM comme destination
- Permettra ainsi de confirmer que l'on observe le même comportement
**** DONE SmartSense: Identifier un SGBD pertinent pour le noeud
CLOSED: [2024-04-15 Mon 08:44]
- Serait intéressant IMO de proposer un autre SGBD qu'InfluxDB pour le noeud SmartSense autonome
- Dans l'idée, pense à SQLite
- Mais on a pas vraiment de relations entre nos données
- Identifier un SGBD pour embedded systems, document-oriented, et disponible sur RPi
**** IN-PROGRESS SmartSense: Traiter les problèmes relevés lors de l'uniformisation du code
- J'ai noté plusieurs points problématiques au cours de mon passage sur l'ensemble des modules
  - Le module *sensor_board* est inutilisé
    - Code obsolète à supprimer ?
  - La librairie *telnetlib* est dépréciée
    - Passer à https://pypi.org/project/telnetlib3/ ?
  - Une conditionnelle étrange est effectuée dans le module *tlc59108_manager*
    - https://gitlab.inria.fr/smartsense/3douest/node-app/smartsense-node-app/-/blob/3feb5c56dc5e74dcea9eb2f2ba98bec40029acdf/TLC59108Manager.py#L370
    - Revient à `if data[0]`
    - `and` est-il l'opérateur désiré ?
    - Simplifier/corriger
  - Les appels à *OpenOCDManager.update()* dans *test_ocd* manquent d'un paramètre
    - J'ai l'impression que c'est le paramètre *board*
    - Ajouter les valeurs correctes ?
  - *USBBoardManager.read_data()* et *parse_to_mqtt()* sont particulièrement sujets aux retours de Pylint
    - Trop d'instructions
    - Trop de branchements
    - Trop de blocs imbriqués
    - Refactorer pour améliorer la lisibilité du code ?
    - Voir si on peut aussi factoriser du code à l'aide de fonctions
  - Corriger ces problèmes
    - Ou si pas de solution claire et rapide, créer une issue correspondante
**** IN-PROGRESS SmartSense: Mise en place d'un processus de CI
- Mettre en place des tests, c'était un bon début
- Mettre en place un processus de CI, ça serait mieux
  - Garantirait la reproductibilité de l'environnement de tests
  - Et que j'oublie pas de les faire tourner avant l'intégration
- Voir comment GitLab gère ça
**** IN-PROGRESS LivingFog: Automatiser la création d'un cluster de Raspberry virtualisé
- Plutôt que de travailler avec le cluster physique
- Serait intéressant d'apprendre à provisionner un cluster virtuel de Raspberry
- Permettrait de pouvoir créer un environnement pour dev et effectuer des tests
  - Notamment comment setup k8s ou équivalent sur un tel système
*** Done
- Réunion de suivi du <2024-04-12 Fri>
  - Préparation
    - Tentative de reproduction du bug sur Raspberry virtualisée
      - Plutôt que de travailler directement sur le noeud, a virtualisé des Raspberry avec dernière version de l'OS
        - En 32bits et 64bits
      - A utilisé inch, outil de stress proposé par InfluxDB, pour reproduire la workload du noeud
      - Différences par rapport aux conditions réelles
        - Inch ne permet pas de reproduire correctement notre workload
        - Peut seulement insérer des valeurs numériques randoms, pas des strings par ex
        - A pu reproduire à peu près le volume et débit en termes de points
          - Nombre de measurements (i.e. tables)
          - Nombre de séries (i.e. ensemble de timestamp pour un measurement, des tags donnés)
        - Mais sur un bien plus grand nombre de valeurs par points
      - Pas réussi à reproduire le bug dans ces conditions sur VMs
      - Par contre, reproduit le bug sur noeud SmartSense
        - Pas un crash complet
        - Mais une impossibilité de compacter par manque de RAM
      - Observations
        - Augmentation de la consommation en RAM d'InfluxDB au début de la simulation
        - Puis atteint un plateau
          - À ~250Mo pour la version 32bits, ~320Mo pour la version 64bits
      - Réflexions
        - P-e la même chose que l'on observe dans notre cas
        - Mais que le plateau est trop élevé par rapport à notre matériel
      - Pistes suivies
        - Faire tourner une trace réelle sur VMs
      - A fait tourner la trace réelle sur VM 64bits
        - Sur ~24h
        - 1Go de données
        - 1 800 000 de points
        - Atteint 500Mo de RAM sans rencontrer de problème
      - Warning
        - Lors du redémarrage d'InfluxDB, a des messages d'avertissement sur sa durée de rédémarrage
        - Retrouve plus ma note à ce sujet, mais persuadé d'avoir eu un crash car démarrage trop long
        - Si durée dépend du nombre de shards, pourrait rencontrer ce problème
      - Conclusions
        - Upgrade le système
        - InfluxDB me paraît inadapté à notre utilisation actuelle sur le noeud SmartSense
          - Souhaite stocker des données de façon pérenne
            - Hors stocker un volume important de données provoque un crash au redémarrage d'InfluxDB
          - Majorité de nos données sont des blobs (38Mo sur 40Mo)
            - Utilité d'une timeseries DB pour ce type de données ?
          - Pas de requêtes sur les données numériques à l'heure actuelle
        - Trouve pertinent de dissocier stockage des données et exploitation des données
          - Pas les mêmes contraintes, pas les mêmes outils, pas les mêmes formats
          - Me parait plus adapté d'utiliser un équivalent Document-oriented de SQLite
            - SQLite permet de stocker du JSON
            - Ou sinon peut stocker les trames MQTT ou les entrées au format line protocol (InfluxDB) dans un/des fichiers txt
      - Questions
        - Serais curieux de la configuration InfluxDB pour le serveur
          - Quelle est la durée de rétention des données ?
          - Quelle est la conso en RAM ?
          - Quel est le volume de données stocké ?
        - Serais curieux d'avoir les caractéristiques techniques du micro
          - Pourquoi un flux de données si important ?
          - Possibilité de le diminuer ?
    - Wi-Fi
      - HaLow (SubGHz) me parait la techno la plus adaptée à notre cas d'usage
        - Consommation d'énergie, débit (ko/s à Mo/s), range (500m-1.5km)
      - Mais difficile de trouver du matériel
        - Les posts que je vois sont de début d'année
      - Mickaël propose de partir sur une antenne 5GHz AC
      - A regardé pour un dongle Wi-Fi adapté
        - 2 problèmes
        - Disponibilité de l'équipement
        - Existence des drivers
  - Notes
    - Voir CR : https://gitlab.inria.fr/smartsense/smartobs/crs-reunions/-/blob/main/2024-04-12-reunion-suivi.md
- SmartSense: Monitorer noeud SmartSense sans stockage des entrées lourdes
  - J'ai relancé le noeud à mon retour
  - i.e. <2024-04-08 Mon> à ~09:00
  - Restait des artefacts de ma session d'avant les congés
    - 10 shards
    - De ~2Mo chacun
    - Mais n'a eu aucun avertissement quant à la vitesse du démarrage
  - Au bout de ~40min, consommation en RAM de ~250Mo
  - Au bout de ~1h20, consommation en RAM de ~265Mo
  - Au bout de ~3h00, consommation en RAM de ~285Mo
  - Au bout de ~4h30, consommation en RAM de ~285Mo
  - Au bout de ~5h30, consommation en RAM de ~297Mo
  - Au bout de ~7h00, consommation en RAM de ~350Mo (27,7%)
  - Au bout de ~24h00, consommation en RAM de ~388Mo (29,3%)
  - Au bout de ~29h30, consommation en RAM de ~382Mo (29,0%)
  - Au bout de ~34h00, consommation en RAM de ~424Mo (32,7%)
  - Au bout de ~48h00, consommation en RAM de ~441Mo (34,1%)
  - Au bout de ~51h00, consommation en RAM de ~427Mo (33,4%)
  - Au bout de ~53h30, consommation en RAM de ~432Mo (34,4%)
  - A coupé l'expérience au bout de ~53h30
    - 63 shards, 134Mo de données
  - A redémarré InfluxDB ensuite, avec succès
    - Malgré 29 attempts et ~27s de démarrage
    - Consomme ~457Mo (42,6%)
- SmartSense: Identifier un SGBD pertinent pour le noeud
  - [X] SQLite
    - Au cas où, SQLite supporte un type JSON
      - https://sqlite.org/json1.html
      - https://sqldocs.org/sqlite/sqlite-json-data/
    - Et est installable sur RPi
  - [-] MongoDB +est un bon candidat+ n'est pas un bon candidat
    - Dispose d'une version pour RPi depuis la RPi 3
      - https://www.mongodb.com/developer/products/mongodb/mongodb-on-raspberry-pi/
    - J'ai rien dit
      - https://www.mongodb.com/docs/manual/administration/production-notes/#std-label-prod-notes-supported-platforms-ARM64
      - https://www.mongodb.com/docs/manual/release-notes/5.0-compatibility/#removed-raspberry-pi-support
    - Depuis la version 5 de Mongo (actuellement à la version 7), a drop le support des ARMs antérieurs aux ARMs v8.2
      - De mémoire, la RPi 3 a des CPUs ARM v7.x
  - [ ] RethinkDB
  - [ ] CouchDB
  - [X] Matthieu suggère tout simplement des fichiers
    - Avec du logrotate pour gérer les fichiers et les compresser
    - À voir si c'est l'outil le plus approprié
- SmartSense: Créer repos pour ADT SmartObs
  - Me suis rendu compte que Guillermo avait pris des notes pour un CR de notre réunion technique
  - Mais il n'avait pas pu suivre toute la réunion, notamment quand on parlait de méthodologie de travail avec Mickaël
  - Complétion du CR
  - Création du repo pour CR en local
  - J'ai par contre pas les droits pour créer le sous-groupe *SmartObs* du groupe *SmartSense*
  - A demandé à Mickaël de s'en charger
  - Mickaël s'en est occupé
- SmartSense: Identifier un adapteur Wi-Fi adapté
  - Mickaël propose comme antenne : https://www.inmac-wstore.com/tp-link-cpe710-antenne/p7310809.htm
    - Antenne 5GHz
  - Références utilisées
    - https://github.com/morrownr/USB-WiFi/
    - https://wikidevi.wi-cat.ru/
  - Pour le moment, regarde sur : https://www.inmac-wstore.com/
  - [~] TP-Link Archer T2U Nano
    - Chip: RTL8811AU
    - https://www.inmac-wstore.com/tp-link-archer-t2u-nano-adaptateur-reseau-usb-2-0/p7211994.htm
    - Semble non plug-&-play
    - Mais a l'air d'y avoir des threads sur comment le faire fonctionner
    - https://forums.raspberrypi.com/viewtopic.php?t=128089
    - Mais le site avec le driver a l'air down de nos jours
    - wikidevi pointe vers 2 drivers
      - Un par Edimax : https://www.edimax.com/edimax/download/download/data/edimax/global/download/product/smb_embedded_wireless_adapter/smb_embedded_wireless_adapter_ac650_ac600/ew-7811uac/
      - Un communautaire : https://github.com/aircrack-ng/rtl8812au
    - Ce dernier est fait pour le chip RLT8812AU
      - Si c'est compatible, alors on peut se pencher sur cet autre driver communautaire : https://github.com/morrownr/8812au-20210820
  - [X] TP-Link Archer T3U
    - https://www.inmac-wstore.com/tp-link-archer-t3u-adaptateur-reseau-usb-3-0/p7212232.htm
    - Semble supporté d'après ces infos avec un driver de la communauté
      - https://forums.raspberrypi.com/viewtopic.php?t=342665
    - Chip: RTL8812BU
      - https://wikidevi.wi-cat.ru/TP-LINK_Archer_T3U
    - Driver : https://github.com/morrownr/88x2bu-20210702?tab=readme-ov-file
  - [~] NETGEAR A6100 WiFi USB Mini Adapter
    - Chip: RTL8811AU
    - https://www.inmac-wstore.com/netgear-a6100-wifi-usb-mini-adapter-adaptateur-reseau-usb/p2798933.htm
  - [X] TRENDnet TEW-808UBM
    - Chip: RTL8812BU
  - Ce qui ressort de ma lecture de https://github.com/morrownr/USB-WiFi/blob/main/home/USB_WiFi_Adapter_Information_for_Linux.md
    - 2 sociétés qui font des chips pour clés Wi-Fi
      - Realtek
      - Mediatek
    - Realtek ne suit pas le standard Linux pour les drivers
      - Drivers ne sont pas intégrés au noyau Linux
      - Drivers ne sont pas forcément publiés non plus
      - Va dépendre du bon vouloir des fabriquants
      - Communauté propose des drivers, mais aux utilisateur-rices de mettre à jour leur système régulièrement
    - Les drivers de Mediatek eux sont intégrés au kernel
    - Donc plutôt partir sur un adapteur basé sur une chip Mediatek
      - Liste des chips Mediatek supportés : https://wireless.wiki.kernel.org/en/users/drivers/mediatek
    - TP-Link et D-Link ont l'air d'avoir la facheuse tendance de MàJ leurs adapteurs Wi-Fi en gardant la même référence
      - MàJ pouvant impliquer un changement de chip
      - Et donc un changement des drivers adaptés
    - Panda et Alfa eux se démarquent par la qualité de leur matériel et le support natif de Linux
    - Peut se pencher sur l'utilisation d'un cable d'extension USB pour sortir le dongle du boitier et ainsi ne pas manquer de place
      - Utiliser un adapteur Wi-Fi avec antenne par exemple
      - Curieux de si la RPi peut alimenter un adapteur avec antenne par contre
  - À titre perso, recommanderais
    - Tirés de : https://github.com/morrownr/USB-WiFi/blob/main/home/The_Short_List.md et https://elinux.org/RPi_USB_Wi-Fi_Adapters
    - Avec antenne:
      - ALFA AWUS036ACM : https://www.amazon.com/Network-AWUS036ACM-Long-Range-Wide-Coverage-High-Sensitivity/dp/B08BJS8FXD
    - Format mini:
      - Linksys AE6000: https://www.amazon.com/Linksys-AE6000-Dual-Band-Wireless-Adapter/dp/B00BFW8KIG
    - Format nano:
      - Panda PAU0A AC600: https://www.amazon.com/dp/B07C9TYDR4/?tag=pandaw-20
  - Voir si on a d'autres revendeurs référencés sur lesquels ces modèles seraient dispos
- SmartSense: Écrire un script permettant de rejouer une trace du noeud SmartSense
  - Infos supplémentaires
    - Taille du fichier de traces : 1.7Go
    - Taille du fichier de traces sans les données du capteur stream : 1.1Go
    - Taille du fichier de traces dans les données des capteurs stream et thermal : 252Mo
    - Donc les blobs représentent ~85% des données collectées et stockées
      - Sans compter qu'on a le format du line protocol
  - Peut utiliser du portforwarding pour ne pas m'embêter à gérer les différents cas de figures
    - La commande permet d'interagir depuis ma machine avec l'instance InfluxDB du noeud SmartSense
      - ssh -L 8086:192.168.1.2:8086 pi@192.168.1.2
  - Script
    - [X] Créer un client InfluxDB
    - [X] Ouvrir le fichier
    - [X] Pour chaque ligne
      - [X] Bufferiser la ligne
      - [X] Si buffer atteint 100 lignes
        - [X] Envoyer buffer via client
        - [X] Vider buffer
        - [X] Attendre 5s
  - Problème rencontré à cause de "leptonVersion=unknow"
    - A gunzip le fichier
      - gunzip -v test-storage.txt.gz
    - Remplacé les occurrences de l'erreur de formatage
      - sed -i 's/leptonVersion=unknow/leptonVersion="unknown"/g' test-storage.txt
    - A gzippé le fichier
      - gzip test-storage.txt
  - Problème rencontré à cause de 'leptonVersion="unknown"'
    - influxdb.exceptions.InfluxDBClientError: 400: {"error":"partial write: field type conflict: input field \"leptonVersion\" on measurement \"meta_thermalImgHd\" is type float, already exists as type string dropped=1"}
    - A décidé
  - Problème rencontré à cause d'une erreur de formatage
    - influxdb.exceptions.InfluxDBClientError: 400: {"error":"partial write: unable to parse 'event_gyroscope,admin=p,mode=production,location=External,sector=1,subsector=1,node=b8:27:eb:8c:67:48,board=sensorBoard,mcu=sensor x=0,y=0,z=0 1684598296169874816event_magnetometer,admin=p,mode=production,location=External,sector=1,subsector=1,node=b8:27:eb:8c:67:48,board=sensorBoard,mcu=sensor x=4,y=53,z=26 1684598296169874816': bad timestamp dropped=0"}
    - Tous les 10 buffers, de manière régulière
    - i.e. toutes les 1000 entrées
    - J'ai dû tout simplement me foirer lors de la création de ce fichier de traces
    - Doit manquer un \n entre chaque buffer de traces que j'ajoutais au fichier
    - Ajout d'un try/catch de cette exception
- SmartSense: Exécuter le script sur le noeud SmartSense
  - A lancé le script pour feed les données au noeud SmartSense
    - Démarré à ~17h30
  - À voir si cela va provoquer le crash
    - Consommation initiale en RAM: 286Mo
  - Obtenu un crash du script à ~02h43 (~17h20, heure du noeud)
  - Dernier log produit par le script à ce moment
    - [[file:logs/2024-04-11-test_saturate_influx.log]]
  - Puis rencontre une exception non-gérée interrompant le script
    #+BEGIN_PLAIN
Traceback (most recent call last):
  File "/home/mnicolas/.cache/pypoetry/virtualenvs/smartsense-node-app-d-qYo1ZE-py3.12/lib/python3.12/site-packages/urllib3/connectionpool.py", line 790, in urlopen
    response = self._make_request(
               ^^^^^^^^^^^^^^^^^^^
  File "/home/mnicolas/.cache/pypoetry/virtualenvs/smartsense-node-app-d-qYo1ZE-py3.12/lib/python3.12/site-packages/urllib3/connectionpool.py", line 536, in _make_request
    response = conn.getresponse()
               ^^^^^^^^^^^^^^^^^^
  File "/home/mnicolas/.cache/pypoetry/virtualenvs/smartsense-node-app-d-qYo1ZE-py3.12/lib/python3.12/site-packages/urllib3/connection.py", line 461, in getresponse
    httplib_response = super().getresponse()
                       ^^^^^^^^^^^^^^^^^^^^^
  File "/usr/lib64/python3.12/http/client.py", line 1423, in getresponse
    response.begin()
  File "/usr/lib64/python3.12/http/client.py", line 331, in begin
    version, status, reason = self._read_status()
                              ^^^^^^^^^^^^^^^^^^^
  File "/usr/lib64/python3.12/http/client.py", line 292, in _read_status
    line = str(self.fp.readline(_MAXLINE + 1), "iso-8859-1")
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/lib64/python3.12/socket.py", line 707, in readinto
    return self._sock.recv_into(b)
           ^^^^^^^^^^^^^^^^^^^^^^^
ConnectionResetError: [Errno 104] Connection reset by peer

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/mnicolas/.cache/pypoetry/virtualenvs/smartsense-node-app-d-qYo1ZE-py3.12/lib/python3.12/site-packages/requests/adapters.py", line 486, in send
    resp = conn.urlopen(
           ^^^^^^^^^^^^^
  File "/home/mnicolas/.cache/pypoetry/virtualenvs/smartsense-node-app-d-qYo1ZE-py3.12/lib/python3.12/site-packages/urllib3/connectionpool.py", line 844, in urlopen
    retries = retries.increment(
              ^^^^^^^^^^^^^^^^^^
  File "/home/mnicolas/.cache/pypoetry/virtualenvs/smartsense-node-app-d-qYo1ZE-py3.12/lib/python3.12/site-packages/urllib3/util/retry.py", line 470, in increment
    raise reraise(type(error), error, _stacktrace)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mnicolas/.cache/pypoetry/virtualenvs/smartsense-node-app-d-qYo1ZE-py3.12/lib/python3.12/site-packages/urllib3/util/util.py", line 38, in reraise
    raise value.with_traceback(tb)
  File "/home/mnicolas/.cache/pypoetry/virtualenvs/smartsense-node-app-d-qYo1ZE-py3.12/lib/python3.12/site-packages/urllib3/connectionpool.py", line 790, in urlopen
    response = self._make_request(
               ^^^^^^^^^^^^^^^^^^^
  File "/home/mnicolas/.cache/pypoetry/virtualenvs/smartsense-node-app-d-qYo1ZE-py3.12/lib/python3.12/site-packages/urllib3/connectionpool.py", line 536, in _make_request
    response = conn.getresponse()
               ^^^^^^^^^^^^^^^^^^
  File "/home/mnicolas/.cache/pypoetry/virtualenvs/smartsense-node-app-d-qYo1ZE-py3.12/lib/python3.12/site-packages/urllib3/connection.py", line 461, in getresponse
    httplib_response = super().getresponse()
                       ^^^^^^^^^^^^^^^^^^^^^
  File "/usr/lib64/python3.12/http/client.py", line 1423, in getresponse
    response.begin()
  File "/usr/lib64/python3.12/http/client.py", line 331, in begin
    version, status, reason = self._read_status()
                              ^^^^^^^^^^^^^^^^^^^
  File "/usr/lib64/python3.12/http/client.py", line 292, in _read_status
    line = str(self.fp.readline(_MAXLINE + 1), "iso-8859-1")
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/lib64/python3.12/socket.py", line 707, in readinto
    return self._sock.recv_into(b)
           ^^^^^^^^^^^^^^^^^^^^^^^
urllib3.exceptions.ProtocolError: ('Connection aborted.', ConnectionResetError(104, 'Connection reset by peer'))

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/mnicolas/Documents/smartsense-node-app/test_saturate_influxdb.py", line 45, in <module>
    main()
  File "/home/mnicolas/Documents/smartsense-node-app/test_saturate_influxdb.py", line 36, in main
    influxdb_client.write_points(buffer, protocol="line", batch_size=None)
  File "/home/mnicolas/.cache/pypoetry/virtualenvs/smartsense-node-app-d-qYo1ZE-py3.12/lib/python3.12/site-packages/influxdb/client.py", line 603, in write_points
    return self._write_points(points=points,
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mnicolas/.cache/pypoetry/virtualenvs/smartsense-node-app-d-qYo1ZE-py3.12/lib/python3.12/site-packages/influxdb/client.py", line 681, in _write_points
    self.write(
  File "/home/mnicolas/.cache/pypoetry/virtualenvs/smartsense-node-app-d-qYo1ZE-py3.12/lib/python3.12/site-packages/influxdb/client.py", line 413, in write
    self.request(
  File "/home/mnicolas/.cache/pypoetry/virtualenvs/smartsense-node-app-d-qYo1ZE-py3.12/lib/python3.12/site-packages/influxdb/client.py", line 332, in request
    response = self._session.request(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mnicolas/.cache/pypoetry/virtualenvs/smartsense-node-app-d-qYo1ZE-py3.12/lib/python3.12/site-packages/requests/sessions.py", line 589, in request
    resp = self.send(prep, **send_kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mnicolas/.cache/pypoetry/virtualenvs/smartsense-node-app-d-qYo1ZE-py3.12/lib/python3.12/site-packages/requests/sessions.py", line 703, in send
    r = adapter.send(request, **kwargs)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mnicolas/.cache/pypoetry/virtualenvs/smartsense-node-app-d-qYo1ZE-py3.12/lib/python3.12/site-packages/requests/adapters.py", line 501, in send
    raise ConnectionError(err, request=request)
requests.exceptions.ConnectionError: ('Connection aborted.', ConnectionResetError(104, 'Connection reset by peer'))
    #+END_PLAIN
  - Correspond à une perte de connexion avec la BD
  - Log du noeud
    - [[file:logs/2024-04-11-test_saturate_influx.log]]
  - Indique une première erreur de compaction par manque de mémoire ~7min avant le crash
    #+BEGIN_PLAIN
juin 06 17:15:14 raspberrypi influxd-systemd-start.sh[2464]: ts=2023-06-06T15:15:14.898289Z lvl=info msg="TSM compaction (start)" log_id=0iFrKq5l000 engine=tsm1 tsm1_strategy=full tsm1_optimize=false trace_id=0iGKGgKW000 op_name=tsm1_compact_group db_shard_id=128 op_event=start
juin 06 17:15:14 raspberrypi influxd-systemd-start.sh[2464]: ts=2023-06-06T15:15:14.898416Z lvl=info msg="Beginning compaction" log_id=0iFrKq5l000 engine=tsm1 tsm1_strategy=full tsm1_optimize=false trace_id=0iGKGgKW000 op_name=tsm1_compact_group db_shard_id=128 tsm1_files_n=4
juin 06 17:15:14 raspberrypi influxd-systemd-start.sh[2464]: ts=2023-06-06T15:15:14.898469Z lvl=info msg="Compacting file" log_id=0iFrKq5l000 engine=tsm1 tsm1_strategy=full tsm1_optimize=false trace_id=0iGKGgKW000 op_name=tsm1_compact_group db_shard_id=128 tsm1_index=0 tsm1_file=/data/influxdb/data/SensorData/short_duration/128/000000001-000000001.tsm
juin 06 17:15:14 raspberrypi influxd-systemd-start.sh[2464]: ts=2023-06-06T15:15:14.898519Z lvl=info msg="Compacting file" log_id=0iFrKq5l000 engine=tsm1 tsm1_strategy=full tsm1_optimize=false trace_id=0iGKGgKW000 op_name=tsm1_compact_group db_shard_id=128 tsm1_index=1 tsm1_file=/data/influxdb/data/SensorData/short_duration/128/000000002-000000001.tsm
juin 06 17:15:14 raspberrypi influxd-systemd-start.sh[2464]: ts=2023-06-06T15:15:14.898568Z lvl=info msg="Compacting file" log_id=0iFrKq5l000 engine=tsm1 tsm1_strategy=full tsm1_optimize=false trace_id=0iGKGgKW000 op_name=tsm1_compact_group db_shard_id=128 tsm1_index=2 tsm1_file=/data/influxdb/data/SensorData/short_duration/128/000000003-000000001.tsm
juin 06 17:15:14 raspberrypi influxd-systemd-start.sh[2464]: ts=2023-06-06T15:15:14.898616Z lvl=info msg="Compacting file" log_id=0iFrKq5l000 engine=tsm1 tsm1_strategy=full tsm1_optimize=false trace_id=0iGKGgKW000 op_name=tsm1_compact_group db_shard_id=128 tsm1_index=3 tsm1_file=/data/influxdb/data/SensorData/short_duration/128/000000004-000000001.tsm
juin 06 17:15:16 raspberrypi influxd-systemd-start.sh[2464]: [httpd] 192.168.1.2 - admin [06/Jun/2023:17:15:16 +0200] "POST /write?db=SensorData HTTP/1.1 " 204 0 "-" "python-requests/2.31.0" f129e0ec-047c-11ee-bad5-b827eb8c6748 258495
juin 06 17:15:18 raspberrypi influxd-systemd-start.sh[2464]: [httpd] 192.168.1.2 - admin [06/Jun/2023:17:15:18 +0200] "POST /write?db=SensorData HTTP/1.1 " 400 436 "-" "python-requests/2.31.0" f285375a-047c-11ee-bad6-b827eb8c6748 85147
juin 06 17:15:21 raspberrypi influxd-systemd-start.sh[2464]: [httpd] 192.168.1.2 - admin [06/Jun/2023:17:15:20 +0200] "POST /write?db=SensorData HTTP/1.1 " 204 0 "-" "python-requests/2.31.0" f3c508e9-047c-11ee-bad7-b827eb8c6748 586502
juin 06 17:15:24 raspberrypi influxd-systemd-start.sh[2464]: [httpd] 192.168.1.2 - admin [06/Jun/2023:17:15:23 +0200] "POST /write?db=SensorData HTTP/1.1 " 204 0 "-" "python-requests/2.31.0" f553d32b-047c-11ee-bad8-b827eb8c6748 908366
juin 06 17:15:24 raspberrypi influxd-systemd-start.sh[2464]: ts=2023-06-06T15:15:24.250020Z lvl=info msg="Error replacing new TSM files" log_id=0iFrKq5l000 engine=tsm1 tsm1_strategy=full tsm1_optimize=false trace_id=0iGKGgKW000 op_name=tsm1_compact_group db_shard_id=128 error="cannot allocate memory"
juin 06 17:15:25 raspberrypi influxd-systemd-start.sh[2464]: ts=2023-06-06T15:15:25.269532Z lvl=info msg="TSM compaction (end)" log_id=0iFrKq5l000 engine=tsm1 tsm1_strategy=full tsm1_optimize=false trace_id=0iGKGgKW000 op_name=tsm1_compact_group db_shard_id=128 op_event=end op_elapsed=10371.259ms
    #+END_PLAIN
  - Puis un crash en bonne et due forme par manque de mémoire
  - Chose étonnante, au bout de 3h, InfluxDB a réussi à redémarrer proprement
    - Assez de RAM disponible ?
- SmartSense: Exécuter le script sur VM 64-bits
  - A créé une image dédiée, pi-bookworm-64bits-test-saturate.qcow2
  - A démarré le script à ~09h27
  - Consommation initiale en RAM: 93Mo
  - Au bout de ~1h, consommation en RAM: 288Mo
  - Le script a crash quelques minutes après
  - Erreur côté script
    #+BEGIN_PLAIN
Traceback (most recent call last):
  File "/home/mnicolas/Documents/smartsense-node-app/test_saturate_influxdb.py", line 45, in <module>
    main()
  File "/home/mnicolas/Documents/smartsense-node-app/test_saturate_influxdb.py", line 36, in main
    influxdb_client.write_points(buffer, protocol="line", batch_size=None)
  File "/home/mnicolas/.cache/pypoetry/virtualenvs/smartsense-node-app-d-qYo1ZE-py3.12/lib/python3.12/site-packages/influxdb/client.py", line 603, in write_points
    return self._write_points(points=points,
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mnicolas/.cache/pypoetry/virtualenvs/smartsense-node-app-d-qYo1ZE-py3.12/lib/python3.12/site-packages/influxdb/client.py", line 681, in _write_points
    self.write(
  File "/home/mnicolas/.cache/pypoetry/virtualenvs/smartsense-node-app-d-qYo1ZE-py3.12/lib/python3.12/site-packages/influxdb/client.py", line 413, in write
    self.request(
  File "/home/mnicolas/.cache/pypoetry/virtualenvs/smartsense-node-app-d-qYo1ZE-py3.12/lib/python3.12/site-packages/influxdb/client.py", line 373, in request
    raise InfluxDBServerError(reformat_error(response))
influxdb.exceptions.InfluxDBServerError: {"error":"timeout"}
    #+END_PLAIN
  - Erreur côté InfluxDB
    #+BEGIN_PLAIN
Apr 11 09:21:33 raspberrypi influxd-systemd-start.sh[691]: [httpd] 10.0.2.2 - admin [11/Apr/2024:09:21:31 +0100] "POST /write?db=SensorData HTTP/1.1 " 204 0 "-" "python-requests/2.31.0" 80b8eae0-f7dc-11ee-851d-e2c03c540c32 1311065
Apr 11 09:21:36 raspberrypi influxd-systemd-start.sh[691]: [httpd] 10.0.2.2 - admin [11/Apr/2024:09:21:35 +0100] "POST /write?db=SensorData HTTP/1.1 " 204 0 "-" "python-requests/2.31.0" 82b46d45-f7dc-11ee-851e-e2c03c540c32 918502
Apr 11 09:21:38 raspberrypi influxd-systemd-start.sh[691]: [httpd] 10.0.2.2 - admin [11/Apr/2024:09:21:38 +0100] "POST /write?db=SensorData HTTP/1.1 " 204 0 "-" "python-requests/2.31.0" 8472f3e3-f7dc-11ee-851f-e2c03c540c32 420254
Apr 11 09:21:41 raspberrypi influxd-systemd-start.sh[691]: [httpd] 10.0.2.2 - admin [11/Apr/2024:09:21:40 +0100] "POST /write?db=SensorData HTTP/1.1 " 400 438 "-" "python-requests/2.31.0" 85e674e7-f7dc-11ee-8520-e2c03c540c32 676701
Apr 11 09:21:44 raspberrypi influxd-systemd-start.sh[691]: [httpd] 10.0.2.2 - admin [11/Apr/2024:09:21:43 +0100] "POST /write?db=SensorData HTTP/1.1 " 204 0 "-" "python-requests/2.31.0" 8781a0cc-f7dc-11ee-8521-e2c03c540c32 1005047
Apr 11 09:22:13 raspberrypi influxd-systemd-start.sh[691]: ts=2024-04-11T08:22:12.426241Z lvl=info msg="failed to store statistics" log_id=0oV0TPbW000 service=monitor error=timeout
Apr 11 09:22:26 raspberrypi influxd-systemd-start.sh[691]: ts=2024-04-11T08:22:24.738496Z lvl=info msg="failed to store statistics" log_id=0oV0TPbW000 service=monitor error=timeout
Apr 11 09:22:41 raspberrypi influxd-systemd-start.sh[691]: [httpd] 10.0.2.2 - admin [11/Apr/2024:09:21:46 +0100] "POST /write?db=SensorData HTTP/1.1 " 500 15 "-" "python-requests/2.31.0" 894e9a3a-f7dc-11ee-8522-e2c03c540c32 43834722
Apr 11 09:22:41 raspberrypi influxd-systemd-start.sh[691]: ts=2024-04-11T08:22:30.191672Z lvl=error msg="[500] - \"timeout\"" log_id=0oV0TPbW000 service=httpd
Apr 11 09:22:41 raspberrypi influxd-systemd-start.sh[691]: ts=2024-04-11T08:22:37.326121Z lvl=info msg="failed to store statistics" log_id=0oV0TPbW000 service=monitor error=timeout
Apr 11 09:22:57 raspberrypi influxd-systemd-start.sh[691]: ts=2024-04-11T08:22:56.785783Z lvl=info msg="failed to store statistics" log_id=0oV0TPbW000 service=monitor error=timeout
Apr 11 09:23:12 raspberrypi influxd-systemd-start.sh[691]: ts=2024-04-11T08:23:12.767803Z lvl=info msg="Retention policy deletion check (start)" log_id=0oV0TPbW000 service=retention trace_id=0oV3uOVl000 op_name=retention_delete_check op_event=start
Apr 11 09:23:19 raspberrypi influxd-systemd-start.sh[691]: ts=2024-04-11T08:23:13.832220Z lvl=info msg="Retention policy deletion check (end)" log_id=0oV0TPbW000 service=retention trace_id=0oV3uOVl000 op_name=retention_delete_check op_event=end op_elapsed=1064.583ms
Apr 11 09:23:19 raspberrypi influxd-systemd-start.sh[691]: ts=2024-04-11T08:23:13.832767Z lvl=info msg="failed to store statistics" log_id=0oV0TPbW000 service=monitor error=timeout
Apr 11 09:23:31 raspberrypi influxd-systemd-start.sh[691]: ts=2024-04-11T08:23:30.903172Z lvl=info msg="failed to store statistics" log_id=0oV0TPbW000 service=monitor error=timeout
Apr 11 09:23:49 raspberrypi influxd-systemd-start.sh[691]: ts=2024-04-11T08:23:48.991732Z lvl=info msg="failed to store statistics" log_id=0oV0TPbW000 service=monitor error=timeout
Apr 11 09:24:05 raspberrypi influxd-systemd-start.sh[691]: ts=2024-04-11T08:24:03.392276Z lvl=info msg="failed to store statistics" log_id=0oV0TPbW000 service=monitor error=timeout
Apr 11 09:24:19 raspberrypi influxd-systemd-start.sh[691]: ts=2024-04-11T08:24:18.952007Z lvl=info msg="failed to store statistics" log_id=0oV0TPbW000 service=monitor error=timeout
    #+END_PLAIN
  - Semble avoir surchargé la VM d'un point de vue calculs avant de l'avoir surchargé en RAM
  - Ralentir la vitesse à laquelle le script envoie les données à l'instance InfluxDB permettra p-e d'éviter ce problème
  - Modification du script pour écrire toutes les 4s au lieu de 2s
  - Relance le script à ~10h30
  - Au bout de ~4h, consommation en RAM: 431Mo
  - Étonnament, au bout de ~6h, consommation en RAM: 353Mo
  - Au bout de ~22h00, consommation en RAM: 540Mo
  - Mais fin manuellement à l'expérience au bout de ~24h
    - Consommation en RAM: ~500Mo
    - 1Go de données stockées
    - A réussi à redémarrer ~8s
      - 6 warnings/attempts
- SmartSense: Créer issue au sujet de la dépréciation de telnetlib
  - Création de l'issue correspondante : https://gitlab.inria.fr/smartsense/3douest/node-app/smartsense-node-app/-/issues/3
- SmartSense: Monitorer noeud SmartSense sous workload par inch
  - Commande executée
    - Après avoir mis en place le port forwarding : ssh -L 8087:192.168.1.2:8086 pi@192.168.1.2
    - ~/go/bin/inch -v -host localhost:8087 -db SensorData -delay 1s -m 5 -t 1,2,1,2,1,1,1,1 -f 84 -p 86400 -b 50 -consistency any
  - Expérience démarrée à ~16h55 (~07h34, heure du noeud)
  - Consommation mémoire initiale: ~200Mo (17,5%)
  - A tenu l'expérience
  - Consommation mémoire finale: ~230Mo (20%)
  - Mais, rencontre au bout de ~08h d'expérience (15h48, heure du noeud) un premier problème de compaction
    - juin 07 15:48:40 raspberrypi influxd-systemd-start.sh[16898]: ts=2023-06-07T13:48:39.930693Z lvl=info msg="Error replacing new TSM files" log_id=0iH54c9G000 engine=tsm1 tsm1_strategy=full tsm1_optimize=false trace_id=0iHXhMvW000 op_name=tsm1_compact_group db_shard_id=149 error="cannot allocate memory"
  - Alors que la consommation de RAM approchait les ~400Mo à ce moment
    - cf. [[file:img/2024-04-12-grafana-hardware-monitoring.png]]
    - Quoique, on avait 320Mo de RAM cached à ce moment aussi
    - Globalement Grafana indique qu'on avait que 100Mo de dispo
    - Le cache n'est pas automatiquement récupéré si nécessaire ?
  - Reproductible ?
  - Est-ce que ça confirme que InfluxDB tourne mieux sur version récente ?
- LivingFog: Automatiser la création d'un cluster de Raspberry virtualisé
  - Serait bien de prendre en main virt-manager à cette occasion
    - Pour le moment, je créé mes images et VMs à la main
    - Est-ce que virt-manager me permettrait de gérer cela plus simplement ?
  - Tentative de créer une RPi3b virtuelle avec virt-manager
    - À la création de la VM, obtient l'erreur suivante
      - Unable to complete install: 'unsupported configuration: machine type 'raspi3b' does not support ACPI'
    - Semble être lié à une erreur documentée de libvirt
      - https://gitlab.com/libvirt/libvirt/-/issues/297
    - Je retrouve justement l'erreur à l'origine de cette issue si je retire la feature ACPI de l'XML
      - Unable to complete install: 'internal error: process exited while connecting to monitor: 2024-04-11T13:23:29.859513Z qemu-system-aarch64: Property 'raspi3b-machine.pflash0' not found'
      - https://stackoverflow.com/questions/64698385/libvirt-qemu-system-aarch64-property-pflash0-not-found
    - Mais de ce que je comprends, le problème est fixed
    - Quel est donc ce problème que je rencontre ?
    - À identifier et creuser
  - Peut en parallèle réfléchir en matière de scripts
** Semaine du <2024-03-18 Mon> au <2024-03-22 Fri>
*** Planned
**** DONE SmartSense: Intégrer la branche *refactor/main-app*
CLOSED: [2024-03-18 Mon 14:21]
- MàJ par rapport à *refactor/mqtt-interface-remove-inheritance*
- Faire la MR
**** DONE SmartSense: Intégrer la branche *refactor/naming-conventions*
CLOSED: [2024-03-18 Mon 14:21]
- Préparer la MR, en attendant l'intégration de *refactor/main-app*
**** DONE SmartSense: Intégrer la branche *refactor/usb-board-manager*
CLOSED: [2024-03-18 Mon 14:21]
- Faire la MR
**** DONE SmartSense: Recréer une image QEMU RPi 64-bits
CLOSED: [2024-03-19 Tue 09:26]
- J'ai quelque peu pollué l'image que j'ai créé la semaine dernière pour tester inch et essayer de faire planter la VM
- C'était avant que je me pose la question de créer des snapshots
- Refaire l'image au propre
**** DONE SmartSense: Mesurer les différentes métriques des données produites par le noeud SmartSense
CLOSED: [2024-03-19 Tue 16:09]
- Pour le moment, ai utilisé inch pour générer un volume de données similaire
  - i.e. la taille d'un shard grossièrement
- Afin de reproduire le plus fidèlement possible notre scénario de crash, serait pertinent de mesurer les différentes caractéristiques du volume de données généré
  - Et même pour mieux connaître de manière générale le comportement de notre système
- Métriques à observer
  - Nombre de measurements
  - Nombre de series
  - Nombre de tags
  - Nombre de valeurs
  - Débit en écriture
  - Volume des valeurs
**** DONE SmartSense: Configurer inch pour simuler le plus fidèlement le workload du RPi
CLOSED: [2024-03-21 Thu 10:50]
- Pour le moment, utilise inch avec la commande suivante
  - ~/go/bin/inch -v -db SensorData -delay 15s -m 20 -t 2,5000,1 -f 4 -consistency any
- Comparer ces paramètres avec les métriques observées
- MàJ la commande pour s'approcher de ces dernières
  - Si possible
  - Notamment, comment reproduire les events générés par la caméra et le micro ?
**** DONE SmartSense: Créer une image QEMU RPi 32-bits
CLOSED: [2024-03-21 Thu 14:30]
- Pour comparer le comportement d'InfluxDB sur un système 64-bits à celui sur 32-bits
- Devrait disposer d'une image similaire
- Suivre les mêmes étapes
**** DONE SmartSense: Monitorer InfluxDB OS 64-bits sous workload par inch
CLOSED: [2024-03-22 Fri 08:58]
**** DONE SmartSense: Monitorer InfluxDB OS 32-bits sous workload par inch
CLOSED: [2024-03-22 Fri 08:58]
**** DONE SmartSense: Analyser résultats des simulations menées sur les VMs pour reproduire le bug d'InfluxDB
CLOSED: [2024-03-22 Fri 10:26]
- Maintenant que j'ai pu faire tourner les simulations sur chaque VM
- Voir ce qu'on peut tirer de ces résultats
**** IN-PROGRESS SmartSense: Traiter les problèmes relevés lors de l'uniformisation du code
- J'ai noté plusieurs points problématiques au cours de mon passage sur l'ensemble des modules
  - Le module *sensor_board* est inutilisé
    - Code obsolète à supprimer ?
  - La librairie *telnetlib* est dépréciée
    - Passer à https://pypi.org/project/telnetlib3/ ?
  - Une conditionnelle étrange est effectuée dans le module *tlc59108_manager*
    - https://gitlab.inria.fr/smartsense/3douest/node-app/smartsense-node-app/-/blob/3feb5c56dc5e74dcea9eb2f2ba98bec40029acdf/TLC59108Manager.py#L370
    - Revient à `if data[0]`
    - `and` est-il l'opérateur désiré ?
    - Simplifier/corriger
  - Les appels à *OpenOCDManager.update()* dans *test_ocd* manquent d'un paramètre
    - J'ai l'impression que c'est le paramètre *board*
    - Ajouter les valeurs correctes ?
  - *USBBoardManager.read_data()* et *parse_to_mqtt()* sont particulièrement sujets aux retours de Pylint
    - Trop d'instructions
    - Trop de branchements
    - Trop de blocs imbriqués
    - Refactorer pour améliorer la lisibilité du code ?
    - Voir si on peut aussi factoriser du code à l'aide de fonctions
  - Corriger ces problèmes
    - Ou si pas de solution claire et rapide, créer une issue correspondante
**** IN-PROGRESS SmartSense: Mise en place d'un processus de CI
- Mettre en place des tests, c'était un bon début
- Mettre en place un processus de CI, ça serait mieux
  - Garantirait la reproductibilité de l'environnement de tests
  - Et que j'oublie pas de les faire tourner avant l'intégration
- Voir comment GitLab gère ça
**** IN-PROGRESS SmartSense: Monitorer noeud SmartSense sans stockage des entrées lourdes
- Plusieurs types d'entrées sont bien plus volumineux que les autres
  - De plusieurs ordres de grandeur (jusqu'au ko par entrée)
- Entrées concernées
  - event_thermalImgHd
  - event_stream
- Et dans une moindre mesure
  - event_thermalImgLd
  - event_rfGiga
- Tester si ne pas stocker ces entrées stabilise le système
**** TODO SmartSense: Monitorer noeud SmartSense sous workload par inch
- En théorie, devrait se comporter de manière similaire à la VM 32-bits
- Serait intéressant de le confirmer
*** Done
- SmartSense: Recréer une image QEMU RPi 64-bits
  - [X] Obtenir l'image système
  - [X] Ajouter l'utilisateur pi
  - [X] Activer le SSH
  - [X] Convertir l'image OS en image qcow2
  - [X] Redimensionner l'image
  - [X] Redimensionner la partition
    - sudo modprobe nbd max_part=10
    - sudo qemu-nbd -c /dev/nbd0 pi-bookworm-64.qcow2
    - gparted /dev/nbd0
    - sudo qemu-nbd -d /dev/nbd0
  - [X] MàJ l'OS
  - [X] Installer InfluxDB
    - wget https://dl.influxdata.com/influxdb/releases/influxdb_1.8.10_arm64.deb
    - sudo apt install ./influxdb_1.8.10_arm64.deb
  - [X] Installer golang
    - sudo apt install golang
  - [X] Installer inch
    - go install github.com/influxdata/inch/cmd/inch@latest
  - [X] Mettre en place la config d'InfluxDB
    - Dans /etc/influxdb/influxdb.conf
    - index-version = "tsi1"
    - compact-full-write-cold-duration = "1h"
    - max-concurrent-compactions = 1
  - [X] Désactiver le redémarrage automatique d'InfluxDB
    - Histoire de ne pas polluer les logs avec des redémarrages et plantages en boucle
    - Dans /etc/systemd/system/influxdb.service.d/override.conf
      #+BEGIN_PLAIN
[System]
Restart=no
      #+END_PLAIN
    - Désactiver lancement au démarrage : sudo systemctl disable influxdb
  - [X] Créer la base de données de test avec la bonne retention policy
    - CREATE DATABASE SensorData WITH SHARD DURATION 1h NAME short_duration
  - [X] Créer snapshot
    - base-image : Image de l'OS MàJ
    - influxdb-inch : Image avec influxdb et inch d'installés et configurés
    - Sinon, image de travail actuelle
- SmartSense: Mesurer les différentes métriques des données produites par le noeud SmartSense
  - Nombre de measurements : ~20 events, ~20 meta
  - Nombre de series : 68
  - Nombre de tags : 1, 2, 1, 2, 1, 1, 1, 1
  - Nombre de valeurs : 1 à 4 valeurs
    - Généralement des entiers, floats et strings courtes
    - Mais des strings longues (~900, ~12k, ~64k characters) pour caméra/micro
      - ~900 pour rfGiga
      - ~64k pour stream
      - ~12k pour thermalImgHd
      - ~150 pour thermalImgLd
  - Nombre de points par série : 86 400
    - Considère un par seconde
    - Part sur une durée d'un jour
  - Débit en écriture
    - ~100 points/5s
    - ~20 points/s
    - Notes
      - 1/s pour thermalImgHd et thermalImgLd
      - la périodicité de stream a l'air de varier de la seconde à 30aine de secondes
  - Volume
    - ~40Mo/h
    - ~11ko/s
      - Raccord avec thermalImgHd à chaque seconde
- SmartSense: Configurer inch pour simuler le plus fidèlement le workload du RPi
  - inch n'a pas l'air de permettre de configurer les valeurs fournies pour les champs
  - Peut simplifier le système en prenant que thermalImgHd comme ordre de grandeur
    - i.e. un measurement qui fournit ~10ko/s de données
  - A lancé la commande suivante le <2024-03-19 Tue>, 15h27 (heure de la VM)
    - ~/go/bin/inch -v -db SensorData -delay 1s -m 5 -t 1,2,1,2,1,1,1,1 -f 1000 -p 86400 -b 100 -consistency any
  - VM freezed le lendemain matin
  - Avais saturé l'espace disque
    - 8Go est p-e un peu short
  - Jusque là, avait créé 6 shards de 600Mo
  - Et un 7ème shard était en cours de vie
  - Doit diviser par ~12 la charge de travail si veux m'approcher du volume de données de notre use case
  - Aurais tout de même un débit en terme de points supérieur à la workload réelle
  - A lancé la commande suivante le <2024-03-20 Wed>, 08:37 (heure de la VM)
    - ~/go/bin/inch -v -db SensorData -delay 1s -m 5 -t 1,2,1,2,1,1,1,1 -f 84 -p 86400 -b 100 -consistency any
  - Observations
    - 84 points/s
    - 7k valeurs/s
    - Cardinalité de séries de 20 (5 * 2 * 2)
  - En divisant le buffer par 2, i.e. -b 50, divise par 2 le nombre de points/s et de valeurs/s
  - Modifier le nombre de fields, i.e. -f 168, impacte le nombre de valeurs/s
  - Modifier le nombre de measurements, i.e. -m 20, n'a pas d'impact sur le débit, uniquement sur le nombre de séries
  - Est-ce que j'essaie d'approcher le nombre de measurements et de séries ?
    - Dans ce cas, doit réduire le nombre de fields par points pour maintenir le volume de données
  - Ou d'approcher le fait qu'on a surtout 4-5 measurements avec un nombre de valeurs conséquent ?
    - e.g. -m 5 -f 420 -b 10
    - Maintient le débit d'un point de vue valeurs
      - ~3k val/s
    - Mais le débit d'un point de vue points n'arrive à suivre
      - 7 points/s
  - A relancé avec les paramètres suivant à 08:56
    - ~/go/bin/inch -v -db SensorData -delay 1s -m 5 -t 1,2,1,2,1,1,1,1 -f 84 -p 86400 -b 50 -consistency any
  - Observations
    - 40 points/s
    - ~3500 valeurs/s
    - Cardinalité : 20
    - Volume : 80Mo/shard
  - À tester
    - ~/go/bin/inch -v -db SensorData -delay 200ms -m 5 -t 1,2,1,2,1,1,1,1 -f 420 -p 86400 -b 10 -consistency any~/go/bin/inch -v -db SensorData -delay 200ms -m 5 -t 1,2,1,2,1,1,1,1 -f 420 -p 86400 -b 10 -consistency any
  - Observations
    - 29.7 points/s
    - ~12500 valeurs/s
    - Cardinalité : 20
  - En remontant le delay, i.e. -delay 400ms, permet de concentrer les valeurs sur un nombre réduit de points
  - Commande
    - ~/go/bin/inch -v -db SensorData -delay 400ms -m 5 -t 1,2,1,2,1,1,1,1 -f 420 -p 86400 -b 10 -consistency any
  - Observations
    - 18.9 points/s
    - ~ 7800 valeurs/s
    - Cardinalité : 20
- Réflexions sur l'utilisation d'InfluxDB
  - Un point intéressant des discussions de la réunion du WP3 TF de <2024-03-19 Tue> fut sur la gestion des données
  - Un intervenant insistait sur l'utilité de découpler stockage des données et exploitation des données
  - On a effectivement pas les mêmes problématiques dans les 2 cas
  - Pour le stockage, on va souhaiter conserver de manière efficace l'intégralité des données de manière pérenne
  - Pour l'exploitation, on va plutôt s'intéresser qu'à un sous-ensemble des données
    - D'un point de vue temporel, e.g. ne s'intéresse qu'aux données des 30 derniers jours
    - D'un point de vue contenu, e.g. ne s'intéresse qu'aux données concernant la qualité de l'air
    - Et qu'on va potentiellement transformer pour atteindre nos objectifs en terme de coût, e.g. temps de réponse
  - Et en fonction de l'exploitation, du traitement désiré, on va préférer utiliser des outils différents
    - Pense pas qu'il y a un intérêt particulier à utiliser InfluxDB si on veut mettre en place un traitement sur les flux vidéo et audio
  - Me parait intéressant de réfléchir à nos données et leur gestion avec cette nouvelle perspective
    - Notamment du format des données
  - Et de se pencher sur les bonnes pratiques/outils appropriés de ce côté là
    - Se repencher sur le livre de Kleppmann
- SmartSense: Créer une image QEMU RPi 32-bits
  - [X] Obtenir l'image système
  - [X] Ajouter l'utilisateur pi
  - [X] Activer le SSH
  - [X] Convertir l'image OS en image qcow2
  - [X] Redimensionner l'image
  - [X] Redimensionner la partition
  - [X] MàJ l'OS
  - [X] Créer base image
  - [X] Créer snapshot influxdb-inch
  - [X] Installer InfluxDB
  - [X] Installer golang
  - [X] Installer inch
  - [X] Mettre en place la config d'InfluxDB
  - [X] Désactiver le redémarrage automatique d'InfluxDB
  - [X] Créer la base de données de test avec la bonne retention policy
  - [X] Créer la snapshot de travail
- Magellan: Présentation de Nix par Volodia
  - Problème de reproductibilité dans la recherche scientifique informatique
    - Docker, ça marche tant que l'image est disponible
    - Mais re-créer l'image s'avère déjà difficile des années après
  - Nix souhaite adresser ce problème
    - Propose un langage, NixDSL, pour décrire la construction de paquets
    - Propose un package manager, nixpkgs
    - Construit un OS, nixOS, à partir de là
  - Govind recommande Determinate Systems Nix Installer
- SmartSense: Monitorer InfluxDB OS 64-bits sous workload par inch
  - Commande
    - ~/go/bin/inch -v -db SensorData -delay 1s -m 5 -t 1,2,1,2,1,1,1,1 -f 84 -p 86400 -b 50 -consistency any
  - Initialement, conso mémoire de 80Mo
  - inch a inséré des données durant ~11h
  - VM a tenu
  - Conso mémoire atteint 320Mo finalement
  - Après redémarrage d'InfluxDB, diminue à 165Mo
  - Redémarrage avec 13 shards a nécessité ~20s et a déclenché 13 warnings
    - Rappel qu'au bout du 25ème warning, influxdb abort son lancement
  - A sauvegardé la snapshot sous le nom pi-bookworm-64bits-84-fields-completed.qcow2
- SmartSense: Monitorer InfluxDB OS 32-bits sous workload par inch
  - Rencontre l'erreur suivante au lancement de inch
    #+BEGIN_PLAIN
panic: unaligned 64-bit atomic operation

goroutine 9 [running]:
runtime/internal/atomic.panicUnaligned()
	/usr/lib/go-1.19/src/runtime/internal/atomic/unaligned.go:8 +0x24
runtime/internal/atomic.Xchg64(0x194e1fc, 0x0)
	/usr/lib/go-1.19/src/runtime/internal/atomic/atomic_arm.s:269 +0x14
github.com/influxdata/inch.(*Simulator).runMonitor(0x194e180, {0x34dc74, 0x19180f0})
	/home/pi/go/pkg/mod/github.com/influxdata/inch@v0.0.0-20230124130601-4ae8d5121d6e/inch.go:536 +0x1b4
github.com/influxdata/inch.(*Simulator).Run.func2()
	/home/pi/go/pkg/mod/github.com/influxdata/inch@v0.0.0-20230124130601-4ae8d5121d6e/inch.go:278 +0x70
created by github.com/influxdata/inch.(*Simulator).Run
	/home/pi/go/pkg/mod/github.com/influxdata/inch@v0.0.0-20230124130601-4ae8d5121d6e/inch.go:278 +0x1470
    #+END_PLAIN
  - M'en suis sorti en utilisant inch sur ma machine directement
    - Par défaut, transmet les données à localhost:8086
    - Ajout de la config correspondante dans QEMU pour permettre le port forwarding
      - -netdev ...,hostfwd=tcp::8086-:8086
  - A lancé la commande à 13:40 (heure VM)
    - ~/go/bin/inch -v -db SensorData -delay 1s -m 5 -t 1,2,1,2,1,1,1,1 -f 84 -p 86400 -b 50 -consistency any
  - Consommation mémoire : 80Mo au démarrage
  - Consommation mémoire : 213Mo au bout d'1h
  - VM a tenu
  - Consommation mémoire : 248Mo au final
  - On retrouve bien 13 shards, ~75Mo chacun
  - Surprenamment, ici n'a nécessité que 8s pour redémarrer
    - 5 warnings se sont déclenchés pendant ce temps
  - Après redémarrage d'InfluxDB, diminue à 91Mo
  - A sauvegardé la snapshot sous le nom pi-bookworm-32bits-84-fields-completed.qcow2
  - Remarque
    - Pas besoin d'installer inch dans nos VMs
    - Juste d'exposer la base InfluxDB
    - Simplifira le processus de création des prochaines, si nécessaire
- SmartSense: Mise en place d'un processus de CI
  - Plusieurs points à éclaircir
  - [X] Dispose-t-on de runners ?
    - De ce que je comprends, GitLab fonctionne avec un système de runners qui exécutent les jobs
    - Est-ce que le GitLab Inria en met à disposition ?
    - Ou faut-il les déployer soi-même ?
      - Dans ce cas, quelle machine utiliser ?
    - Quelques runners sont mis à dispostion
      - https://gitlab.inria.fr/siteadmin/doc/-/wikis/faq#how-to-use-the-continuous-integration-ci-service
      - https://ci.inria.fr/doc/page/gitlab/
    - Vu notre système, les smalls semblent fit nos besoins
  - [X] Comment déployer des service MQTT et InfluxDB ?
    - Des tests d'intégration interagissent directement avec le broker MQTT ou la base de données InfluxDB
    - Comment déployer des conteneurs proposant ces services ?
    - Existe une option dédiée à préciser dans le fichier de config
      - https://docs.gitlab.com/16.8/ee/ci/yaml/index.html#services
    - Dans notre cas, devrait le faire pour les tests d'intégration
  - [ ] Comment catégoriser nos tests ?
    - Serait intéressant IMO de décomposer notre pipeline en plusieurs étapes
    - Lint > Unit Tests > Integration Tests
    - Besoin dans ce cas de pouvoir distinguer/préciser quels tests sont des unit tests et lesquels sont des integration tests
  - [ ] Quand déclencher notre pipeline ?
    - À chaque MR vers main me paraît le minimum
    - Mais puisqu'on peut aussi push dans main directement, me paraît intéressant/nécessaire de remettre de la CI ici
      - Moyen d'éviter la redondance ?
- SmartSense: Analyser résultats des simulations menées sur les VMs pour reproduire le bug d'InfluxDB
  - Dans les 2 cas, n'a pas pu reproduire le bug
  - Ce que l'on observe, c'est une augmentation croissante de la consommation mémoire
  - Jusqu'à atteindre un palier, un consommation mémoire de croisière
    - ~200/300Mo en fonction de la version de l'OS
  - Observe néanmoins que redémarrer InfluxDB réinitialise l'augmentation de la consommation mémoire
  - A-t-on juste le même comportement sur le noeud SmartSense ?
    - Mais que le palier est trop élevé pour notre hardware étant donné notre workload ?
    - Dans ce cas, indiquerait que la workload simulée est trop faible par rapport à la workload réelle
  - A-t-on un bug dans notre version actuelle du système qui est corrigée dans la dernière version du système ?
    - Dans ce cas, indiquerait que rejouer la workload simulée sur le noeud SmartSense provoquerait ou permettrait de s'approcher du point de rupture
  - Est-ce qu'un redémarrage régulier d'InfluxDB permettrait de réinitialiser la consommation mémoire de l'application ?
    - Et ainsi éviter d'atteindre la valeur seuil où le mécanisme de compaction achève notre système
    - Pose par contre le problème de la multiplication des shards qui ralentisse le démarrage d'InfluxDB
    - Jusqu'au point où on ne puisse plus démarrer le SGDB
      - ~25 shards de ce que j'ai pu observer, dans le cas du noeud
  - Pistes
    - Ré-utiliser inch, mais sur le noeud et non pas une VM
      - Mon expérience avec la VM 32-bits m'a appris à faire tourner inch pour taper sur une DB distante
      - Peut réutiliser le même setup expérimental mais avec le noeud SmartSense
      - Permettrait d'observer si son comportement diffère de la VM 32-bits
      - Et d'établir si la workload simulée est suffisante pour reproduire le bug
    - Disable les points problématiques et refaire tourner mainApp
      - Par points problématiques, désigne les entrées de
        - event_thermalImgHd et Ld
        - event_stream
        - event_rfGiga
      - Serait intéressant de voir si l'application devient stable si on ignore/retire ces entrées
      - Indiquerait la nécessité de traiter ces données de manière particulière
- SmartSense: Monitorer noeud SmartSense sans stockage des entrées lourdes
  - A modifié `DataToLocalInfluxDB` pour filtrer les entrées concernées
    - event_thermalImgHd, event_thermalImgLd et event_stream
  - Au bout de 4h de fonctionnement, observe que la consommation mémoire s'est stabilisé depuis ~2h
  - Consommation mémoire a démarré à 204Mo
  - Au cours de la fenêtre 2h-4h, consommation mémoire stagne à 246Mo
  - Si cela se confirme
    - Indique que c'est notre utilisation d'InfluxDB le problème
    - Et qu'il faut envisager une alternative pour stocker les flux de données concernés
    - Faudrait creuser quel(s) intérêt(s) nous avions à stocker ces blobs dans une time series-oriented database
  - Consommation mémoire a augmenté jusqu'à 277Mo 2h plus tard
  - Stagne de nouveau
** Semaine du <2024-03-11 Mon> au <2024-03-15 Fri>
*** Planned
**** DONE SmartSense: Rédaction du CR de la réunion du <2024-03-13 Wed>
CLOSED: [2024-03-14 Thu 09:37]
- Olivier ne pouvant être présent, souhaite un compte rendu de la réunion
- Éric a expliqué que cela serait utile aussi pour le suivi du projet de conserver une trace écrite de nos réunions
- Rédiger et partager le CR de cette réunion
**** IN-PROGRESS SmartSense: Intégrer la branche *refactor/main-app*
- MàJ par rapport à *refactor/mqtt-interface-remove-inheritance*
- Faire la MR
**** IN-PROGRESS SmartSense: Intégrer la branche *refactor/naming-conventions*
- Préparer la MR, en attendant l'intégration de *refactor/main-app*
**** IN-PROGRESS SmartSense: Intégrer la branche *refactor/usb-board-manager*
- Faire la MR
**** IN-PROGRESS SmartSense: Traiter les problèmes relevés lors de l'uniformisation du code
- J'ai noté plusieurs points problématiques au cours de mon passage sur l'ensemble des modules
  - Le module *sensor_board* est inutilisé
    - Code obsolète à supprimer ?
  - La librairie *telnetlib* est dépréciée
    - Passer à https://pypi.org/project/telnetlib3/ ?
  - Une conditionnelle étrange est effectuée dans le module *tlc59108_manager*
    - https://gitlab.inria.fr/smartsense/3douest/node-app/smartsense-node-app/-/blob/3feb5c56dc5e74dcea9eb2f2ba98bec40029acdf/TLC59108Manager.py#L370
    - Revient à `if data[0]`
    - `and` est-il l'opérateur désiré ?
    - Simplifier/corriger
  - Les appels à *OpenOCDManager.update()* dans *test_ocd* manquent d'un paramètre
    - J'ai l'impression que c'est le paramètre *board*
    - Ajouter les valeurs correctes ?
  - *USBBoardManager.read_data()* et *parse_to_mqtt()* sont particulièrement sujets aux retours de Pylint
    - Trop d'instructions
    - Trop de branchements
    - Trop de blocs imbriqués
    - Refactorer pour améliorer la lisibilité du code ?
    - Voir si on peut aussi factoriser du code à l'aide de fonctions
  - Corriger ces problèmes
    - Ou si pas de solution claire et rapide, créer une issue correspondante
**** IN-PROGRESS SmartSense: Mettre en place une nouvelle version système du noeud
- Un retour effectué par plusieurs personnes est qu'influxdb fonctionne mieux sur un OS 64-bits
  - Plus précisément, InfluxDB a pris la décision à la version 1.8.x de ne supporter que ces systèmes
- Ça tombe bien, désormais il existe des versions de Raspbian 64-bits
- Permettrait au passage de mettre à jour l'OS
  - Utile pour les paquets/dépendances
  - Mais aussi les drivers pour périphériques WiFi
- Nécessite cependant d'établir le processus de setup d'un noeud
  - Quels logiciels sont requis ?
  - Quelles modifications de la configuration par défaut sont effectuées ?
- Peut être l'occasion de mettre ça au propre
- Et de paver la voie pour la mise en place d'un outil de déploiement et configuration des noeuds
  - Ansible ? Nix ?
**** IN-PROGRESS SmartSense: Identifier des devices WiFi pour le noeud
- Maintenant que je m'y retrouve un peu mieux dans les différentes normes WiFi
- Peut faire une première sélection de dongles
- Caractéristiques à prendre en considération
  - Norme
  - Antenne directionnelle
  - Bandes de fréquences utilisées
  - Débit
  - Portée
  - Consommation énergie
**** TODO SmartSense: Mise en place d'un processus de CI
- Mettre en place des tests, c'était un bon début
- Mettre en place un processus de CI, ça serait mieux
  - Garantirait la reproductibilité de l'environnement de tests
  - Et que j'oublie pas de les faire tourner avant l'intégration
- Voir comment GitLab gère ça
*** Done
- SmartSense: Réunion technique <2024-03-15 Fri>
  - Gestion des CRs
    - Eu des bons retours sur le CR que j'ai redigé
    - Peut être plus succint la prochaine fois si nécessaire
    - Mais pose la question de comment les gérer
    - Nous sommes mis d'accord sur la création repo au sein de l'organisation SmartSense dédié
      - Continuer à utiliser Notes pour rédiger le CR lors des réunions, le peaufiner ensuite
      - Mais ensuite l'exporter et le versionner dans ce repo
    - Permettra d'avoir une adresse unique et pérenne où retrouver les CRs
      - On a pas forcément de garanties sur la rétention des données sur Notes
    - Sur le même sujet, Mickaël a demandé à ce que je mette un miroir de mon journal de bord sur le gitlab SmartSense
      - Permettra de rassembler au même endroit les différents documents
  - InfluxDB
    - A présenté le problème
    - Explique que je suis en train de virtualiser une RPi avec un OS-64bits
      - Et de simuler une charge comparable en données à l'aide de l'outil inch
      - Apprécient l'idée de la virtualisation
    - Explique que je me rends compte à présent que le mécanisme de compression est le clou dans le cercueil
    - Mais que la conso en RAM d'InfluxDB augmente au fur et à mesure
      - Se pose la question de si atteint un plateau au bout d'un moment
    - Reste à trouver pourquoi
      - Principales raisons listées sont
        - L'utilisation de l'index en mémoire
          - Seulement lorsque configuration de index-version="inmem"
          - Nous on est utilise les time-series index (index-version="tsi1") depuis le début
          - Donc pas concernés
        - Le nombre trop important de séries
          - Séries : Un ensemble de points partageant les mêmes valeurs de measurement, tags et clés de champs
          - Comme on travaille sur un unique noeud, on a peu de valeurs distinctes pour un même tag
          - Sur le petit dataset que j'ai sur le noeud, possède que 64 séries
          - De plus, de ce que je comprends, ne devrait plus poser de problème depuis le passage au time-series index
            - https://www.influxdata.com/blog/how-to-overcome-memory-usage-challenges-with-the-time-series-index/
          - Donc pas concernés
    - Donc à creuser
    - On se donne une semaine supplémentaire
    - Après on verra pour une alternative/un workaround
    - Voir pour peaufiner la simulation de charge pour approcher la charge réelle
      - Notamment, les données transmises par la caméra et le micro
    - Reproduire le process sur une VM 32-bits
      - Voir si cela a impact sur le résultat
  - Wi-Fi
    - LTE-M & NB IoT nécessitent réseau 4G, donc bien en dehors de notre scope
    - LoRaWAN serait par contre un + dans le projet de leur PoV
      - Mais expliquais que Guillaume n'était pas fan de la techno
    - Wi-Fi HaLow est prometteur
    - Mais la carte shield trouvée n'est pas compatible avec notre CM3
    - Dongle USB trouvé, mais n'est plus en vente
    - Faute de mieux, partir sur un dongle Wi-Fi classique
    - Mickaël a plutôt en tête de partir sur un récepteur puissant
      - Récepteur avec une sensibilité élevée
      - Antenne active, qui améliore portée et sensibilité
      - Omni-directionnel, mais qu'on puisse si besoin cibler la zone d'écoute
        - Mentionnait la technologie MIMO
    - Guillermo propose d'aller discuter avec Laurent Morin au sujet des équipements réseau
      - S'y connaitrait et pourrait nous renseigner/orienter sur le sujet
  - Énergie
    - Noeud a une autonomie de 10h
    - Va encore prendre un coup avec l'ajout du Wi-Fi donc
    - Mais on ne s'intéresse pas particulièrement pour le moment à cette problématique dans le cadre du projet
  - Méthodologie
    - Avec Mickaël, avons rediscuté de comment procéder pour l'intégration de mes changements
    - Pour les modifications mineurs/simples
      - Genre refactoring, ajout de tests
      - Suis autorisé maintenant à merge dans main
    - Continuer à faire des MRs pour les nouvelles fonctionnalités/changements majeurs
      - Et pour les modifications où je souhaite un avis supplémentaire
- SmartSense: Mettre en place une nouvelle version système du noeud
  - Re-virtualiser une Raspberry via Qemu me parait un bon début
    - Tutos utilisés
      - https://github.com/dhruvvyas90/qemu-rpi-kernel/tree/master/native-emulation
      - https://blog.agchapman.com/using-qemu-to-emulate-a-raspberry-pi/
      - https://interrupt.memfault.com/blog/emulating-raspberry-pi-in-qemu
    - En clair
      #+BEGIN_PLAIN
aunpack 2023-12-11-raspios-bookworm-arm64-lite.img.xz

sudo losetup -f --show -P 2023-12-11-raspios-bookworm-arm64-lite.img
sudo mount /dev/loop0p01 /mnt/image
cp /mnt/image/bcm2710-rpi-cm3.dtb .
cp /mnt/image/kernel8.img .
sudo echo 'pi:$6$rMAjicIMHWanx8Gb$gE0zeXkz1aWQYloCjbHt58Ow35DK45uf10bRG0eum8APKbnnrPYv.eO81d.Abx6zJfVhhkbUwMtFedDglhiM//' | sudo tee /mnt/image/userconf.txt
sudo touch /mnt/image/ssh
sudo umount /mnt/image
sudo losetup -d /dev/loop0

qemu-img convert -f raw -O qcow2 2023-12-11-raspios-bookworm-arm64-lite.img 2023-12-11-raspios-bookworm-arm64-lite.qcow2
qemu-img resize 2023-12-11-raspios-bookworm-arm64-lite.qcow 8G

# Redimension des partitions avec gparted
sudo modprobe nbd max_part=10
sudo qemu-nbd -c /dev/nbd0 pi-bookworm-64.qcow2
gparted /dev/nbd0
sudo qemu-nbd -d /dev/nbd0

qemu-system-aarch64 \
    -M raspi3b \
    -cpu cortex-a72 \
    -m 1G -smp 4 \
    -nographic \
    -dtb bcm2710-rpi-cm3.dtb \
    -sd 2023-12-11-raspios-bookworm-arm64-lite.qcow \
    -kernel kernel8.img \
    -append "rw earlyprintk loglevel=8 console=ttyAMA0,115200 dwc_otg.lpm_enable=0 root=/dev/mmcblk0p2 rootdelay=1" \
    -device usb-net,netdev=net0  \
    -netdev user,id=net0,hostfwd=tcp::2222-:22
      #+END_PLAIN
  - A démarré le système et l'a MàJ
  - Avait en tête que le système était réinitialisé à chaque lancement, mais dans le cas présent non
  - Installation d'InfluxDB
    - https://github.com/influxdata/influxdb/releases/tag/v1.8.10
    - sudo dpkg -i influxdb_1.8.10_arm64.deb
  - Installation de Inch
    - https://docs.influxdata.com/influxdb/v1/tools/inch/
    - Installation au préalable de go
      - sudo apt install golang
    - go install github.com/influxdata/inch/cmd/inch@latest
  - Démarrage d'Influx
    - sudo service start influxdb
  - Démarrage d'Inch
    - ~/go/bin/inch -v -b 10000 -t 2,5000,1 -p 100000 -consistency any
    - Au bout d'une 20aine de secondes, a déjà généré 7Mo de données
      - Correspond à 2M de points
    - ~/go/bin/inch -v -db SensorData -delay 15s -m 20 -t 2,5000,1 -f 4 -consistency any
      - Parait un débit correspondant plus à ce que l'on obtient sur le noeud SmartSense
- SmartSense: Monitoring d'InfluxDB sur OS 64-bits
  - Sur la VM de Raspberry CM3
  - A laissé tourner le système avec InfluxDB et Inch toute la nuit du <2024-03-14 Thu> au <2024-03-15 Fri>
  - Fonctionne toujours au matin
    - 18 shards à ce moment
  - Mais remarque augmentation de la RAM utilisée
    - Me semble qu'a commencé vers ~200Mo
    - Actuellement à ~580Mo
  - Donc oui, on a bien une fuite mémoire
  - Qui aboutit au OoM lors d'une compression
  - Solution serait de redémarré périodiquement InfluxDB
  - Mais repose le problème de la multiplication des shards
  - Faudrait tester le fait de redémarrer périodiquement InfluxDB tout en gardant la retention policy de base
  - Mais si non, besoin de multiplier les shards
  - Et à ce moment là, problème possible au redémarrage
  - Concernant l'augmentation de la RAM utilisée InfluxDB, mentionne la cardinalité des séries
    - i.e. le nombre de combinaison uniques de valeurs à partir des measurements et tags
    - Sur le dataset que j'ai actuellement sur le noeud SmartSense, a 68 séries
    - Me parait pas si élevé
  - Mentionne le passage à l'index tsi1 plutôt qu'en mémoire
    - Ce que l'on a déjà fait
  - Léo m'a donné la commande pour créer des snapshots à partir d'une image système
    - qemu-img create -f qcow -F qcow2 -o backing_file=2023-12-11-raspios-bookworm-arm64-lite.qcow test.qcow2
  - Observe abort du démarrage d'InfluxDB au bout de 25 attempts/~6s
    - Malheureusement, j'ai pas noté le nombre de shards auquel cela correspond
    - J'ai réussi à redémarré l'image, a 25 shards à ce stade
- SmartSense: Rédaction du CR de la réunion du <2024-03-13 Wed>
  - Me rend compte que je n'ai pas rédigé de CR auparavant
  - Pars sur le format des réunions Coast
  - A partagé la 1ère version du CR : https://notes.inria.fr/o2KRXqWIQQmdlrZdVVmTPQ
- Récapitulatif
  - Équipe Taran m'a donné accès au projet SmartSense
  - Plus particulièrement au code tournant sur les noeuds
  - Objectif initial a été de prendre en main le programme
  - Problèmes
    - Aucune documentation du code
      - Documentation haut niveau, e.g. description du protocole de message
      - Mais à pas jour
    - Aucun outil de qualité du code
      - Linter, autoformat
    - Aucun test
    - Aucun environnement de dev
      - Programme ne pouvant démarrer que sur Rpi, avec MQTT et InfluxDB d'installé
    - Aucune documentation de l'env de dev
    - Et qualité du code variable
  - Mise en place d'outils
    - Linter, tests, codeformatter
  - Mise en place de tests unitaires et d'intégration
    - MQTT, InfluxDB
  - Refactoring de ces modules
  - Refactoring du module principal pour le rendre plus lisible et testable
  - Mise en place de tests unitaires pour ce module
  - Depuis, 2 tâches principales
  - InfluxDB
    - Instance locale d'InfluxDB plante au bout d'un moment
    - Problème
      - Aucune trace autour de ce bug
      - Quoi crash exactement ?
        - InfluxDB ? L'application ?
      - Pourquoi ?
        - Message de commit accuse mécanisme de compression des shards
        - Mais rien pour étayer cette accusation
      - Quel est l'état du système ensuite ?
        - InfluxDB redémarre ?
    - A confirmé que c'était bien le mécanisme de compression
      - Trop gourmand en RAM
    - Diverses tentatives pour borner son coût, sans succès
    - Voir la version 64-bits
    - À la recherche sinon d'alternative
      - File System
      - Telegraf?
  - Wi-Fi
- Récapitulatif du débit en données d'un noeud
  - InfluxDB : ~40Mo/h
  - Line protocol, fichier txt : 75Mo/h
    - 1.5Mo/min
    - 25Ko/s
  - Line protocol, fichier gzip : 25Mo/h
    - 0.5Mo/min
    - ~9Ko/s
  - MQTT, fichier gzip : 36Mo/h
    - 0.58Mo/min
    - 10Ko/s
  - MQTT, fichier txt : 108Mo/h (je suppose x3 par rapport au gzip)
    - 1.8Mo/min
    - 30Kos/s
- Réunion du <2024-03-12 Tue>
  - Réunion avec Guillaume en amont de la réunion avec les autres
  - Objectifs
    - Convenir de la suite
      - Détails supplémentaires sur sa reprise du travail ?
      - Continuer SmartSense à 100% pour le moment ou recommencer à me pencher sur LivingFog ?
    - Parler de l'antenne
      - Wi-Fi HaLow a l'air suited to our needs
      - Mais nécessiterait de rendre compatible les noeuds mais aussi l'antenne
      - Présenté comme ça, Guillaume est intéressé par la techno
      - Prêt à commander le matos pour qu'on expérimente avec
      - Rien de défini concernant l'antenne
- Réunion du <2024-03-13 Wed>
  - Points à aborder
    - Guillaume
      - Toujours en arrêt
      - Mais travaille ponctuellement en visio
      - Pouvait pas être présent ce coup-ci, mais pas hésiter à l'inclure à présent
      - Pense pouvoir revenir fin avril
      - Vais reprendre mes tâches sur la partie fog (~20%)
    - Repo
      - Méthodologie de travail ?
        - Continue de collaborer via MR ?
    - InfluxDB
      - Quel est le rôle de l'instance locale ?
      - Si c'est juste stocker, pourquoi utiliser InfluxDB et pas un outil plus léger ?
        - Notamment, si on fait évoluer la version utilisée par le serveur, est-ce qu'on pourra mettre à jour celle du noeud autonome ?
    - WiFi
      - Quelles sont les spécifications de l'antenne ?
        - Distance par rapport à la Croix Verte ?
        - Norme WiFi ?
      - Savoir si ça vaut le coup de regarder du côté de HaLow
      - Rien de défini actuellement
      - Antenne à définir/concevoir nous même
    - Autonomie
      - Combien de temps survivait le noeud sur batterie ?
      - 10h d'après Mickaël
    - Paradigme
      - Conçoit-on une application pour environnement contraint de la même manière ?
        - Mise en veille du système ?
- SmartSense: Monitoring du noeud SmartSense
  - J'ai lancé tourné de <2024-03-07 Thu> après-midi jusqu'à <2024-03-11 Mon> le noeud SmartSense
  - En stockant les données à l'aide d'un fichier.gz plutôt que dans Influx
  - InfluxDB et mainApp ont tourné correctement tout le week-end
  - Observe que la taille du fichier augmente à environ 0,8Go/j
    - Parait raccord avec les estimations que j'avais fait
  - Reste à voir comment se passe l'import d'un tel volume de données
    - 2.7Go gzippé
    - Donc environ le triple en réalité
  - Pour le moment, a récupéré le fichier
    - [[file:~/Downloads/test-storage.txt.gz]]
  - Pose par contre la question de l'utilité de passer par un import manuel
    - Le système gère déjà l'enregistrement en BD des messages via le broker
  - Est-ce que ça serait pas plus simple que le noeud enregistre ses messages
  - Et qu'un script les diffusent ensuite au broker
  - P-e plus coûteux
    - Au moment de l'exécution du script, aurait un pic de messages
    - Est-ce que cela pourrait saturer le message broker ?
      - Mickaël mentionnait le besoin de mettre en place plusieurs brokers car un seul saturait
      - Ce qui en soit paraît étrange/suspect que le broker soit le premier bottleneck du système
  - A relancé l'application, mais en skippant /parseMqtt()/
- SmartSense: Identifier des devices WiFi pour le noeud
  - ALFA Network WiFi HaLow HAT
    - https://www.sparkfun.com/products/19956
    - Norme : 802.11ah
    - Bandes de fréquence : < 1GHz
    - Range : Longue
    - Débit : 150Kbps à 15Mbps
    - Antenne : Port dispo
      - Attention à la fréquence, pas la même en fonction de la région
      - Pour EU, besoin de : https://alfa-network.eu/aoa-868-5acm
    - Out of the box : Pense pas
      - Probablement besoin de mettre en place notre propre antenne de réception si on part sur du HaLow
    - Chipset : Newracom™ NRC7292
  - Panda Wireless PAU0B
    - https://www.amazon.com/dp/B08NPX2X4Z/?tag=pandaw-20
    - Norme : 802.11ac
    - Bandes de fréquence : 2.4GHz et 5GHz
    - Range : Longue
      - Pas de précisions supplémentaires sur le site
    - Débit : Jusqu'à 433Mbps en 5GHz, probablement 150Mbps en 2.4GHz
    - Antenne : Oui
    - Gain : Parle de /high-gain antenna/, sans précisions supplémentaires
    - Out of the box : Semblerait
    - Modes de fonctionnement : AP (seulement?)
    - Possèdent aussi d'autres devices du genre, correspondant aux normes précédentes
      - http://www.pandawireless.com/Specs%20|%20Panda%20Wireless.html
    - Chipset : MediaTek MT7610U
  - Alfa AWUS036ACHM
    - https://www.amazon.com/AWUS036ACHM-802-11ac-Range-Boost-Adapter/dp/B08SJBV1N3
    - Norme : 802.11ac
    - Bandes de fréquence : 2.4GHz et 5GHz
    - Range : Longue
      - Pas de précisions supplémentaires sur le site
    - Débit : 150Mbps en 2.4GHz, 433Mbps en 5GHz
    - Antenne : Oui
    - Gain : Parle de /high-gain antenna/, sans précisions
    - Out of the box : Commentaire confirme fonctionnement sur versions récentes de Raspberry
    - Modes de fonctionnement : AP (seulement?)
    - Chipset : MediaTek MT7610U
- Discussion avec François
  - Martin m'a recommandé de parler avec François au sujet du WiFi Long Range
  - Celui-ci m'a expliqué les choses suivantes
  - WiFi Halow correspond bien à ce qu'on veut
    - Mais nécessite du travail
    - Peu de matos actuellement
      - Notamment pas avec dongle usb
      - Et besoin de faire l'antenne
    - Mais s'accompagne de features intéressantes pour notre use case
      - Notamment Target Wake-up Time (TWT) qui permet de réduire la conso électrique
  - Le reste des normes WiFi risque d'être limitant/pas adapté pour nous
  - A mentionné sinon des alternatives
    - Narrowband IOT
      - Low Pover WAN radio technology
      - Cellular Network
        - A-t-on le droit de déployer notre propre infrastructure ?
        - J'en doute
      - Focuses on indoor coverage, low cost, long battery life, high connection density
      - Parcouru *NB-IoT Network Field Trial: Indoor, Outdoor and Underground Coverage Campaign*
        - Dispo ici : https://research.edgehill.ac.uk/ws/portalfiles/portal/29075498/Camera_ready_IWCMC_NB_IoTDeployment.pdf
      - Ont expérimenté le déploiement dans un quartier de Talinn de cette techno
      - Et monitoré la bonne communication avec des noeuds disseminés en extérieur, en intérieur et en sous-sol
      - Portée : jusqu'à 700m en extérieur, sweet spot à 400m
      - Débit :
        - 16,9kbit/s pour LTE Cat NB1 single-tone
        - 66kbit/s pour LTE Cat NB1 multi-tone
        - 159kbit/s pour LTE Cat NB2
      - Qualité impactée négativement en cas d'activité parallèle
        - Leur expérience se déroulait en ville
        - En matinée, observait une baisse de qualité des communications
        - Baisse provoquée par les interférences du traffic des communications humaines sur le réseau LTE
    - LoRa
    - Pycom
- Discussion avec Anne-Cécile
  - On regarde actuellement pour ajouter un moyen de télécommunication au noeud SmartSense
  - LoRaWan exclut par Guillaume (complexité, débit)
  - S'orienterait vers du WiFi
    - Consommation énergétique des normes précédentes ?
    - A-t-elle joué avec HaLow et a des retours à effectuer ?
    - M'explique que non
      - Conseille de faire attention à la distance, notamment en cas de non-LOS
  - Des conseils sur concevoir des applications edge/fog efficientes ?
    - SmartSense tourne en continu, endort juste ses process en attendant prochains messages des capteurs
    - M'explique que dépend de l'OS utilisé
    - Et que c'est le domaine de compétence de l'équipe Taran
    - Voir avec eux surtout
- Terra Forma: Mission Grenoble
  - Train aller le <2024-05-28 Tue> : 07h40 Rennes - 13:39 Grenoble
  - Train retour le <2024-05-30 Thu> : 14h51 Grenoble - 20:30 Rennes
** Semaine du <2024-03-04 Mon> au <2024-03-07 Thu>
*** Planned
**** DONE SmartSense: Corriger le bug de récupération de la valeur `sector` dans une trame MQTT
CLOSED: [2024-03-06 Wed 10:43]
- Un test unitaire a mis en lumière qu'en absence d'un champ `sector` dans une trame MQTT, c'est la valeur du champ `sub_sector` qui est utilisé instead
- Corriger cela
**** DONE SmartSense: Explorer le remplacement de l'instance locale d'InfluxDB par le système de fichiers
CLOSED: [2024-03-07 Thu 14:09]
- Suis curieux de voir la complexité de remplacer InfluxDB par un fichier
- Et si on peut facilement importer les données par la suite dans l'instance InfluxDB d'un serveur
- Permettrait aussi de déterminer si la solution est viable d'un point de vue ressources, notamment en stockage
**** DONE SmartSense: Étudier la mise en place d'une interface WiFi
CLOSED: [2024-03-07 Thu 15:22]
- Voir si un hardware donné est conseillé pour la Raspberry CM3
- Et comment ça s'utiliserait d'un POV logiciel
**** DONE SmartSense: Traiter les problèmes relevés lors de l'uniformisation du code
CLOSED: [2024-03-07 Thu 16:40]
- J'ai noté plusieurs points problématiques au cours de mon passage sur l'ensemble des modules
  - Le module *sensor_board* est inutilisé
    - Code obsolète à supprimer ?
  - La librairie *telnetlib* est dépréciée
    - Passer à https://pypi.org/project/telnetlib3/ ?
  - Une conditionnelle étrange est effectuée dans le module *tlc59108_manager*
    - https://gitlab.inria.fr/smartsense/3douest/node-app/smartsense-node-app/-/blob/3feb5c56dc5e74dcea9eb2f2ba98bec40029acdf/TLC59108Manager.py#L370
    - Revient à `if data[0]`
    - `and` est-il l'opérateur désiré ?
    - Simplifier/corriger
  - Les appels à *OpenOCDManager.update()* dans *test_ocd* manquent d'un paramètre
    - J'ai l'impression que c'est le paramètre *board*
    - Ajouter les valeurs correctes ?
  - *USBBoardManager.read_data()* et *parse_to_mqtt()* sont particulièrement sujets aux retours de Pylint
    - Trop d'instructions
    - Trop de branchements
    - Trop de blocs imbriqués
    - Refactorer pour améliorer la lisibilité du code ?
    - Voir si on peut aussi factoriser du code à l'aide de fonctions
  - Corriger ces problèmes
    - Ou si pas de solution claire et rapide, créer une issue correspondante
**** IN-PROGRESS SmartSense: Intégrer la branche *refactor/main-app*
- MàJ par rapport à *refactor/mqtt-interface-remove-inheritance*
- Faire la MR
**** IN-PROGRESS SmartSense: Intégrer la branche *refactor/naming-conventions*
- Préparer la MR, en attendant l'intégration de *refactor/main-app*
**** IN-PROGRESS SmartSense: Intégrer la branche *refactor/usb-board-manager*
- Faire la MR
**** IN-PROGRESS SmartSense: Mettre en place une nouvelle version système du noeud
- Un retour effectué par plusieurs personnes est qu'influxdb fonctionne mieux sur un OS 64-bits
  - Plus précisément, InfluxDB a pris la décision à la version 1.8.x de ne supporter que ces systèmes
- Ça tombe bien, désormais il existe des versions de Raspbian 64-bits
- Permettrait au passage de mettre à jour l'OS
  - Utile pour les paquets/dépendances
  - Mais aussi les drivers pour périphériques WiFi
- Nécessite cependant d'établir le processus de setup d'un noeud
  - Quels logiciels sont requis ?
  - Quelles modifications de la configuration par défaut sont effectuées ?
- Peut être l'occasion de mettre ça au propre
- Et de paver la voie pour la mise en place d'un outil de déploiement et configuration des noeuds
  - Ansible ? Nix ?
*** Done
- SmartSense: Mettre en place une nouvelle version système du noeud
  - [ ] Créer une VM de Raspberry CM3 et setup Raspberry OS 64-bits
    - Revoir du côté de Qemu
  - [ ] Lister les dépendances, logiciels et configurations déployés sur le noeud SmartSense
  - [ ] Reproduire l'environnement sur la VM
- SmartSense: Lister les dépendances, logiciels et configurations déployés sur le noeud
  - Pour le moment, a identifié
    - collectd
      - Fichiers de configuration à /etc/collectd/collectd.conf et /etc/collectd/collectd.conf.d
    - dnsmasq v2.80
      - Fichier de configuration à /etc/dnsmasq.conf
    - InfluxDB v1.8.0 (initialement 1.6.4)
      - Données stockées dans /data/influxdb
      - Fichier de configuration à /etc/influxdb/influxdb.conf
    - mainApp
      - Application stockée dans /home/root/App
    - grafana v7.0.1
      - Fichier de configuration à /etc/grafana/grafana.ini
- SmartSense: Reproduire l'environnement sur la VM
  - Pose la question de si c'est une bonne idée de reproduire à l'identique l'environnement
    - Ensemble des dépendances/applications installées directement sur le système
  - Ou faudrait-il le faire évoluer ?
    - Mettre en place de la conteneurisation de ses composants
      - Reproductibilité, gestion explicite des versions des composants, esquive des conflits de dépendances
    - Quid de l'interaction de docker avec le hardware ?
      - Je pense à l'utilisation du GPIO et de l'interaction avec les composants Serial
      - Lien suivant indique que l'on peut lancer un conteneur en lui donnant accès aux parties du système, notamment hardware, pertinentes
        - https://stackoverflow.com/questions/30059784/docker-access-to-raspberry-pi-gpio-pins
      - Qui pointe vers la doc de :
        - https://docs.docker.com/engine/reference/run/#runtime-privilege-and-linux-capabilities
  - Si on commence à mettre en place des conteneurs, pose la question de l'outil à utiliser
    - Simplement Docker ?
      - On ne gère pas un cluster, mais un ensemble de noeuds indépendants
    - Ou k3s (et assimilés) ?
      - Est-ce que cet outil peut s'avérer pertinent même dans le cadre de déploiements de noeuds indépendants ?
      - Notamment grâce à un ensemble de fonctionnalités proposées ?
    - Volodia m'a expliqué qu'il n'y a pas particulièrement d'intérêt dans ce cas de figure
  - Et s'il y a un intérêt à coupler l'outil choisi, e.g. Docker, avec Ansible et consorts
  - Me paraît intéressant de creuser ces questions et de les aborder avec l'équipe
  - Après, pas nécessaire de faire évoluer cet aspect tout de suite
    - Mais à garder en tête
- SmartSense: Anomalie détectée
  - En monitorant les ressources du noeud SmartSence ce matin <2024-03-04 Mon>
  - J'ai constaté que la taille du swap avait diminué (1Go -> 558Mo)
  - Quelle est cette sorcellerie ?
  - Finalement, je pense avoir trouvé d'où vient ce phénomène
  - La partition sur lequel le swap est allouée était saturée au démarrage du système
  - Ainsi, le système ne pouvait que récupérer 500Mo d'espace pour le swap
  - J'ai re-déplacé les backups d'InfluxDB sur la partition /data
  - Voir si je retrouve 1Go de swap après redémarrage
- SmartSense: Étudier la mise en place d'une interface WiFi
  - J'ai pas encore clairement identifié/documenté les performances de Long Range WiFi
  - Temps de corriger cela
  - Modes de fonctionnement
    - Infrastructure
      - Les noeuds communiquent entre eux/avec l'extérieur par l'intermédiaire d'un Access Point
      - D'où besoin de se connecter à l'Access Point
    - Ad hoc
      - Noeuds communiquent entre eux directement
      - Noeuds doivent être configurés au préalable (SSID, channel, clé de sécurité)
    - Bridge
      - Noeud interagit avec le réseau via WiFi
      - Et retransmet/interopère avec son interface Ethernet
    - Range-extender
      - Noeud sert de relai pour le réseau WiFi
      - Diminue le débit du réseau
        - Puisque le noeud utilise la même interface pour recevoir/diffuser le signal, divise par 2
  - Impact de la fréquence
    - Plus la fréquence est faible, plus on a :
      - Meilleure distance
      - Meilleure pénétration du signal
    - Plus la fréquence est élevée, plus on a :
      - Meilleur débit
    - Quid de l'impact de la fréquence sur l'énergie ?
  - Long Range WiFi 2.4GHz
    - Distance
      - 802.11g : 140m
      - 802.11n : 250m
    - Débit
      - 100Mbps -> 11.92Mo/s
      - 802.11g : 6.5 - 54Mbps
      - 802.11n : 6.5 - 150Mbps
      - Dépend in fine de la distance et de la largeur de bande
    - Consommation énergétique
  - Long Range WiFi 5GHz
    - Distance
      - 802.11ac, qui utilise uniquement la fréquence 5GHz, est noté comme ayant une portée de 300m
        - Curieux des conditions pour atteindre cette portée
    - Débit
      - 1Gbps -> 119.2Mos/s
      - 802.11ac : 8.5Mbps à 3.4Gbps
      - Là aussi, à mettre en lien avec la distance, la largeur de bande, etc.
    - Consommation énergétique
  - WiFi HaLow
    - Norme IEEE 802.11ah
      - Récente : 2017
      - Y a l'air d'y avoir des modules supportant cette norme
      - Mais est-ce que l'antenne dont on dispose la supporte ?
    - Utilise fréquences < 1GHz
      - A pas l'air d'être une bande de fréquences pour laquelle un standard international est défini
      - A vu passer le commentaire que c'est très US-centric du coup comme norme
      - Sujet à creuser
    - Distance
      - D'après Wiki FR, 100m
      - D'après Wiki EN, ???
      - D'après article, spécification indique 1.5km
      - D'après article, expérience concluante dans 260-750m
    - Débit
      - En théorie, peut atteindre jusqu'à 43Mo/s (347Mbps)
      - D'après article, spécification indique 15 Mbps
      - D'après article, expérience indique ~0.5 à ~5Mbps
    - Consommation énergétique
  - Gain
    - Curieux de comment évolue en fonction du gain :
      - La portée
      - La consommation énergétique
  - Détails à prendre en compte
    - Finalement, s'agit d'un ensemble de tradeoffs
    - Capacité de pénétration du signal et distance vs débit
    - Largeur de bande et débit vs consommation énergie
    - Gain et distance vs consommation énergie
  - Besoin de définir nos requirements
    - Distance ?
    - LOS ?
    - Débit ?
    - Consommation énergie ?
    - Version/Norme supportée par l'antenne ?
  - Tableau récapitulatif
    - Trouvé sur Wikipedia
      - [[file:img/wifi-caracteristiques.png]]
- SmartSense: Monitorer le fonctionnement d'InfluxDB v1.8.10
  - Après 1j de fonctionnement, crash de la DB <2024-03-05 Tue> vers 08:20
    - [[file:logs/2024-03-05-crash-influxdb.log]]
  - De nouveau un OoM lors d'une compaction
  - Mais de manière surprenante, d'un snapshot ce coup-ci
  - Détecte un nouveau problème si on fait vivre la BD plus longtemps
  - À son redémarrage, InfluxDB ré-ouvre chaque shard et charge leur index
  - Étant donné notre retention policy, possède de multiples shards
    - Dans le cas présent, 30
  - La BD met ~8s à démarrer
  - Et vérifie pendant ce temps à de multiples reprises si elle a démarré
  - Voir si la BD interrompt son démarrage si le compteur dépasse un seuil
  - Après quelques minutes de fonctionnement, est retombé dans sa boucle de compaction d'un shard, crash, redémarrage
    - [[file:logs/2024-03-05-crash-influxdb-2.log]]
- SmartSense: Corriger le bug de récupération de la valeur `sector` dans une trame MQTT
  - Exemple de trame problématique
    - mode=production,location=external,sub_sector=2
  - Comment éviter un faux-positif ?
  - Peut vérifier si le caractère précédant la clé est un séparateur
  - Reste à gérer le cas limite où la clé est au début de la trame
  - Modification du code de `get_value_from_key` en conséquence
  - Ajout de tests unitaires dédiés à `get_value_from_key()`
  - Push de la branche : https://gitlab.inria.fr/smartsense/3douest/node-app/smartsense-node-app/-/tree/fix/get-value-from-key?ref_type=heads
- SmartSense: Traiter les problèmes relevés lors de l'uniformisation du code
  - Suppression du module *sensor_board*
    - Push de la branche : https://gitlab.inria.fr/smartsense/3douest/node-app/smartsense-node-app/-/tree/chore/remove-sensor-board-manager?ref_type=heads
  - Utilisation de l'opérateur bitwise AND
    - Push de la branche : https://gitlab.inria.fr/smartsense/3douest/node-app/smartsense-node-app/-/commits/fix/use-bitwise-and
  - Ajout de tâches spécifiques aux problèmes restants
- SmartSense: Explorer le remplacement de l'instance locale d'InfluxDB par le système de fichiers
  - L'utilisation naïve du système de fichiers à la place d'InfluxDB s'effectue facilement
    - Juste append dans un fichier
      #+BEGIN_PLAIN
with open("/data/test-storage.txt", "a") as writer:
    writer.write("\n".join(self.data))
      #+END_PLAIN
  - Problématiques
    - Taille du buffer
      - À quelle fréquence écrire sur le disque ?
      - Trop fréquent => usure de la SD, lenteurs dûes aux accès disques
      - Peu fréquent => perte de données en cas de panne/coupure du système, consommation mémoire
    - Taille du fichier
      - Est-ce qu'on utilise un seul et unique fichier ?
        - Simple d'utilisation
        - Mais quid de la consommation mémoire ?
          - Est-ce coûteux d'ouvrir en écriture un fichier volumineux ?
      - Ou on split les écritures dans plusieurs fichiers ?
        - Par measurements ? Par nombre d'entrées ?
  - A déployé rapidement cette version
  - Reste à voir comment import ensuite les données
  - Le CLI permet cela avec l'option import
  - Mais fichier doit respecter formalisme
  - Exemple fichier valide
    - [[file:files/influxdb-example-import.txt]]
  - Entête du fichier
    - Les lignes en commentaire SONT NÉCESSAIRES
  - Commande
    - influx -username admin -password <password> -import -path=test-storage.txt
  - MAIS affiche un message d'erreur
    - [[file:img/2024-03-07-influxdb-result-import-cmd.png]]
  - MAIS les records sont bels et bien dans l'état après coup
    - [[file:img/2024-03-07-influxdb-actual-state-after-import.png]]
  - En creusant le sujet, me suis rendu compte que c'était une entrée qui était invalide
    - N'était pas insérée finalement dans la DB
  - Et qui avait pour effet de polluer le message de retour de l'import
  - Entrée en question
    - meta_thermalImgHd,admin=p,mode=production,location=External,sector=1,subsector=1,node=b8:27:eb:8c:67:48,board=sensorBoard,mcu=thermal name="thermalImgHd",revision="3.00.00",period=1000,status="NOK",leptonVersion=unknow 1684526714284582016
  - Le problème provient du non-respect du formalisme du line protocol et de la valeur du field leptonVersion
  - En tant que tel, InfluxDB doit interpréter la valeur unknown comme une valeur particulière
  - En utilisant des "" pour préciser qu'il s'agit d'une chaîne de caractères, l'import fonctionne
  - Peut effectuer plusieurs imports sur la même base en utilisant la même entête
  - Peut donc
    - Importer au fur et à mesure les données de cette manière
    - Split un fichier unique de records en une multitude pour importer sans brusquer InfluxDB
      - C.f. https://docs.influxdata.com/influxdb/v1/tools/shell/#import-data-from-a-file-with--import
        #+BEGIN_QUOTE
If your data file has more than 5,000 points, it may be necessary to split that file into several files in order to write your data in batches to InfluxDB.
We recommend writing points in batches of 5,000 to 10,000 points.
Smaller batches, and more HTTP requests, will result in sub-optimal performance.
By default, the HTTP request times out after five seconds.
InfluxDB will still attempt to write the points after that time out but there will be no confirmation that they were successfully written.
        #+END_QUOTE
  - Métriques
    - Constaté une production de ~1,5Mo/min de données
    - A laissé tourner cette version durant 16h
    - A produit un fichier de 1.7Go de données
    - Serait intéressant de mettre en place des mécanismes de compression
  - J'ai regardé, la mise en place de la compression gzip se fait aisément en python
    - Exemple
      #+BEGIN_PLAIN
import gzip

with open("influxdb-example-import.txt", "rt") as reader:
    writer.write("\n".join(self.data))
      #+END_PLAIN
  - Déployé cette nouvelle version
    - À 17:56, heure du noeud
    - À ~11:45, heure réelle
  - Métriques
    - Au bout de 2:10, obtient un fichier de ~65Mo
    - D'où ~0.5Mo/min, 30Mo/h, 720Mo/j
    - Divise par 3 grosso modo la taille du fichier
    - C'est rentable
    - À titre de comparaison, les shards de 1h atteignaient une taille de 40Mo
** Semaine du <2024-02-26 Mon> au <2024-03-01 Fri>
*** Planned
**** DONE SmartSense: Résoudre le problème de monitoring du noeud
CLOSED: [2024-02-26 Mon 09:28]
- Rencontre une erreur inconnue au lancement du noeud ce matin (<2024-02-26 Mon>)
- L'application a l'air de fonctionner correctement
- Mais Grafana n'arrive pas à afficher les métriques systèmes collectées
- Obtient l'erreur "InfluxDB Error: not executed" lorsque j'affiche le dashboard *Hardware Monitoring*
- Ne rencontre pas de problèmes par contre avec le dashboard *Sensor Data*
- Identifier et résoudre le problème sous-jacent
**** DONE SmartSense: Reporter les observations sur le crash d'InfluxDB
CLOSED: [2024-02-26 Mon 14:50]
- Un détail frustrant lorsque je me suis penché sur le crash d'InfluxDB était le flou ambiant concernant le problème
  - Pas de présentation et de précisions concernant l'erreur rencontrée
  - Pas de log associé
  - Flou sur l'efficacité du workaround
- Serait bien de documenter cette fois-ci
  - Le problème
  - Les différentes approches envisagées pour le résoudre
  - Le résultat des approches mises en place
- Mettre à jour l'issue
  - https://gitlab.inria.fr/smartsense/3douest/node-app/smartsense-node-app/-/issues/1
**** DONE SmartSense: Suivre le comportement d'InfluxDB avec une shard duration de 2h
CLOSED: [2024-02-27 Tue 10:34]
- La config avec une shard duration de 4h a tout de même rencontré la OoM error
- Notamment car InfluxDB tardait à créer de nouveaux shards
- J'ai redéployé l'application en mettant 2h comme durée de fenêtre de travail des shards
- Monitorer l'application pour observer l'impact de cette diminution
**** DONE SmartSense: Suivre le comportement d'InfluxDB avec une shard duration de 1h
CLOSED: [2024-02-28 Wed 09:31]
- Recommencer l'expérience précédente avec cette fois-ci une durée de validité des shards de 1h
**** DONE SmartSense: Documenter les dernières observations concernant le crash d'InfluxDB
CLOSED: [2024-02-29 Thu 13:49]
- Mettre à jour l'issue avec le récapitulatif des configurations utilisées et les résultats obtenus
**** DONE SmartSense: Envoyer mail pour réunion point sur début du contrat
CLOSED: [2024-02-29 Thu 14:28]
- Éric est venu me voir pour m'expliquer que ça serait bien de faire une réunion
- Histoire de faire le point et de discuter de la suite
- Et qu'il faudrait s'y prendre tôt
  - Là avec les vacances
  - Et avec leur agenda qui se remplissent vite
- Envoyer mail
**** IN-PROGRESS SmartSense: Intégrer la branche *refactor/main-app*
- MàJ par rapport à *refactor/mqtt-interface-remove-inheritance*
- Faire la MR
**** IN-PROGRESS SmartSense: Intégrer la branche *refactor/naming-conventions*
- Préparer la MR, en attendant l'intégration de *refactor/main-app*
**** IN-PROGRESS SmartSense: Intégrer la branche *refactor/usb-board-manager*
- Faire la MR
**** IN-PROGRESS SmartSense: Traiter les problèmes relevés lors de l'uniformisation du code
- J'ai noté plusieurs points problématiques au cours de mon passage sur l'ensemble des modules
  - Le module *sensor_board* est inutilisé
    - Code obsolète à supprimer ?
  - La librairie *telnetlib* est dépréciée
    - Passer à https://pypi.org/project/telnetlib3/ ?
  - Une conditionnelle étrange est effectuée dans le module *tlc59108_manager*
    - https://gitlab.inria.fr/smartsense/3douest/node-app/smartsense-node-app/-/blob/3feb5c56dc5e74dcea9eb2f2ba98bec40029acdf/TLC59108Manager.py#L370
    - Revient à `if data[0]`
    - `and` est-il l'opérateur désiré ?
    - Simplifier/corriger
  - Les appels à *OpenOCDManager.update()* dans *test_ocd* manquent d'un paramètre
    - J'ai l'impression que c'est le paramètre *board*
    - Ajouter les valeurs correctes ?
  - *USBBoardManager.read_data()* et *parse_to_mqtt()* sont particulièrement sujets aux retours de Pylint
    - Trop d'instructions
    - Trop de branchements
    - Trop de blocs imbriqués
    - Refactorer pour améliorer la lisibilité du code ?
    - Voir si on peut aussi factoriser du code à l'aide de fonctions
  - Corriger ces problèmes
    - Ou si pas de solution claire et rapide, créer une issue correspondante
**** IN-PROGRESS SmartSense: Étudier la mise en place d'une interface WiFi
- Voir si un hardware donné est conseillé pour la Raspberry CM3
- Et comment ça s'utiliserait d'un POV logiciel
**** TODO SmartSense: Mettre en place une nouvelle version système du noeud
- Un retour effectué par plusieurs personnes est qu'influxdb fonctionne mieux sur un OS 64-bits
  - Plus précisément, InfluxDB a pris la décision à la version 1.8.x de ne supporter que ces systèmes
- Ça tombe bien, désormais il existe des versions de Raspbian 64-bits
- Permettrait au passage de mettre à jour l'OS
  - Utile pour les paquets/dépendances
  - Mais aussi les drivers pour périphériques WiFi
- Nécessite cependant d'établir le processus de setup d'un noeud
  - Quels logiciels sont requis ?
  - Quelles modifications de la configuration par défaut sont effectuées ?
- Peut être l'occasion de mettre ça au propre
- Et de paver la voie pour la mise en place d'un outil de déploiement et configuration des noeuds
  - Ansible ? Nix ?
*** Done
- SmartSense: Résoudre le problème de monitoring du noeud
  - Si j'interroge directement influxdb, observe les données de collectd être collectées et stockées
  - "Juste" une question d'affichage donc
  - En récupérant les requêtes soumises à la BDD, observe que celles du dashboard *Hardware Monitoring* mentionnent la retention policy par défaut
    #+BEGIN_PLAIN
SELECT mean(value) FROM collectd.autogen.df_value WHERE (host =~ /^raspberrypi$/ AND instance =~ /^data$/ AND type_instance = 'used') AND time >= now() - 1d GROUP BY time(2m) fill(previous)
    #+END_PLAIN
  - Tandis que celles du dashboard *Sensor Data* utilisent bien celle que j'ai défini
     #+BEGIN_PLAIN
SELECT mean(*) FROM SensorData.short_duration.event_airQuality WHERE (node =~ /^b8:27:eb:8c:67:48$/ AND mcu = 'esensor') AND time >= now() - 2d GROUP BY time(2m)
     #+END_PLAIN
  - "Corrige" le problème en supprimant/recréant "collectd" en utilisant le nom "autogen" pour sa rentention policy
  - Mais à garder en tête et à creuser
    - Pourquoi et comment le dashboard *Sensor Data* s'adapte au changement de nom de la retention policy ?
    - Et comment faire pour que le dashboard *Hardware Monitoring* adopte le même comportement ?
- SmartSense: Suivre le comportement d'InfluxDB avec une shard duration de 2h
  - A l'air de fonctionner correctement
  - J'ai rien dit
  - J'ai beau avoir des shards pour la fenêtre courante, aucune donnée dedans
    - Par ex, à 05:51 (heure du noeud), aucune donnée dans wal/ et data/ des shards correspondants (11 pour SensorData, 12 pour collectd)
    - Shards allant de 04:00 à 06:00
  - P-e tout simplement une histoire de décalage horaire
    - Est 05:51 (UTC+0200)
    - Si InfluxDB est en UTC+0, prendrait sens
    - Ce que je prenais pour un comportement aléatoire
    - Serait juste un fonctionnement décalé d'un shard
  - En arrivant <2024-02-27 Tue> matin, observe qu'InfluxDB a rencontré des crashs durant la nuit
  - Arrive pas à récupérer le log décrivant les events avant 1:19 (heure du noeud)
    - [[file:logs/2024-02-26-crash-influxdb.logs]]
  - À ce stade, avait déjà rencontré un problème
    - Présence d'un artefact de compression échouée dans le shard 23
  - Et à 2:10 (heure du noeud), compression du shard 25 provoque crash par OOM
  - Tiens, après ce crash, permutation de la situation entre les shards 23 et 25
    - Présence d'un artefact de compression dans le shard 25
    - InfluxDB crash maintenant en essayant de compresser le shard 23
      - Où est passé son artefact à lui ?
  - Bon, on va augmenter la taille du swap
    - Suivi le tuto : https://pimylifeup.com/raspberry-pi-swap-file/
    - Passé à 1Go
  - A rencontré le même problème après avoir relancé l'application
    - [[file:logs/2024-02-27-crash-influxdb.logs]]
  - InfluxDB semble ne pas vouloir utiliser/prendre en compte le swap
  - Cette configuration n'est donc pas valide pour notre système
- SmartSense: Reporter les observations sur le crash d'InfluxDB
  - Rédaction et ajout d'un commentaire sur l'issue décrivant
    - Les différentes étapes suivies
    - Le problème rencontré
    - Les solutions identifiées
    - L'approche actuellement en cours d'étude
- SmartSense: Étudier la mise en place d'une interface WiFi
  - J'ai discuté avec Govind rapidement
    - Celui-ci m'a donné quelques conseils à ce sujet
    - Utilisation d'un dongle USB
      - Plutôt que de passer par le serial
      - Serait plus simple/clé en main
    - Consommation énergie
      - Vu que le cas d'usage se positionne dans les situations avec contraintes énergétiques
        - Sur batterie, énergies renouvelables
      - Devrait viser un périphérique efficace sur cet aspect
  - Pose la question de si on a toujours un port USB de disponible
    - La documentation mentionne 2 ports d'extensions
    - Mais en regardant le noeud extérieur, ces ports sont pris
    - Pour quelles raisons ?
    - Documentation du noeud indique l'existence d'un troisième port, à l'arrière du noeud
    - Documentation du noeud extérieur indique qu'une interface d'extension est prise par la *ExternalSensorBoard*
    - Quid de la 2nde qui a l'air occupée dans le cas de mon noeud ?
      - Voir avec Mickaël s'il a une idée de ce dont il pourrait s'agir
- Discussion avec Matthieu
  - Workflow Git
    - Plutôt d'avis à squash les commits d'une MR
    - De retirer par contre les commits de merge
      - Possible en modifiant la config du repo
  - Protocole de messages
    - Liste quelques solutions
      - Protobuf, CaptainProto, FlatBuffer
    - Mais peur que ces solutions soient trop coûteuses pour l'environnement embarqué
    - Suggère plutôt
      - MsgPack
      - JSON Schema + JSONDumps/Loads
        - Probablement pas adapté vu qu'on repose sur le line protocol d'InfluxDB
  - InfluxDB
    - A dérivé du coup sur l'utilisation d'InfluxDB en embarqué
    - Pas trop convaincu
      - Un peu too much pour notre environnement et notre utilisation
    - Suggère plutôt l'utilisation du système de fichiers
  - Programmation Orientée Objet en Python
    - Certes, on voit les vestiges de l'historique de Python
    - Mais désormais adoptée par la communauté
    - Pas de raison d'hésiter/de se censurer
- Discussion avec Govind
  - On a aussi abordé la problématique liée à InfluxDB
  - Lui suggère à la place d'InfluxDB d'utiliser VictoriaMetrics
    - https://github.com/VictoriaMetrics/VictoriaMetrics
    - Une alternative performante
    - D'après leurs benchs, consommerait jusqu'à 10x moins de RAM qu'InfluxDB
    - Tout en étant jusqu'à 20x plus rapide
  - Sinon, me rappelle l'existence du swap
    - Si le problème, c'est qu'on manque de RAM
    - L'option évidente, c'est d'accroître la quantité maximale de swap
      - Actuellement 100Mo
      - On a 95Go de stockage non-utilisé
    - Serait p-e plus lent, certes
    - Mais permettrait de tanker l'exécution du mécanisme de compression
- SmartSense: Suivre le comportement d'InfluxDB avec une shard duration de 1h
  - Nettoyer le système
    - [X] Exporter et réinitialiser les données de la DB
    - [X] Création de l'utilisateur admin
    - [X] Création des BDs avec la bonne shard duration
  - Application lancée le <2024-02-27 Tue> à 11:00 (heure réelle)
    - 2023-05-16, 05:12 (heure noeud)
  - En arrivant le <2024-02-28 Wed>, 08:40 (heure réelle), constate crashs en boucle d'influxdb
    - Depuis 2023-05-17, 2:17 (heure noeud)
  - Malheureusement, ne dispose pas des logs complets
  - Le fichier de logs démarre à 2:17
    - [[file:logs/2024-02-28-crash-influxdb.logs]]
  - Et pas moyen de trouver/accéder à un fichier précédent
    #+BEGIN_PLAIN
> journalctl --list-boots
 0 13efb0f602b943d8b0ee823b1fa0df7d Wed 2023-05-17 02:17:16 CEST�<80><94>Wed 2023-05-17 03:18:48 CEST
> sudo ls -l /var/log/influxdb/
total 0
    #+END_PLAIN
  - Faudra modifier la config des logs influxdb pour conserver sur plusieurs jours les logs
    - Avoir une rota d'1 semaine ?
  - Anyway, cette configuration rencontre aussi le problème de OoM
  - Arrive au bout des options de configuration sur lesquelles jouer pour prévenir le problème
  - Conclusion
    - Pas de solution de ce côté là, au niveau de la configuration
    - Pistes restantes
      - MàJ d'InfluxDB en espérant une amélioration des perfs
        - Mais rien vu à ce sujet au niveau du CHANGELOG
      - Utilisation du "hack" pour désactiver le mécanisme de compression des shards
      - Remplacement d'InfluxDB
        - Telegraf ?
        - VictoriaMetrics ?
        - Append-Only Log ?
      - MàJ du hardware pour disposer de plus de RAM
- SmartSense: Vérifier la bonne utilisation du swap
  - Avant de rule out l'utilisation du swap, re-vérifier
  - J'ai juste augmenter la quantité de swap disponible
  - En supposant que si le noeud en a besoin, il l'utilisera
  - Ce qui n'est pas le comportement observé finalement
  - Des paramètres tels que la swappiness existent et permettent de pousser le noeud à swapper
  - Par défaut, valeur de 60 sur une échelle de 0 à 100 ou 200
    - Commande : cat /proc/sys/vm/swappiness
    - Initialement jusqu'à 100, maintenant autorise jusqu'à 200
      - Pas réussi à identifier si la version courante accepte les valeurs > 100
    - Plus c'est elevé, plus ça swap
      - 0 indique au système d'utiliser le swap qu'en ultime recours
        - i.e. ne désactive pas le swap
      - 100/200 indique au système de swapper de manière aggressive
        - i.e. de favoriser le swap à la RAM
        - mais ne désactive pas la RAM
  - Possibilité de modifier de manière volatile cette valeur
    - Commande : sudo sysctl vm.swappiness=<value>
    - Persiste jusqu'au prochain reboot
  - Ou de modifier la configuration de manière durable
    - Commande : sudo nano /etc/sysctl.conf
    - Ajout de la ligne vm.swappiness=<value>
  - A testé différentes valeurs avec la première méthode
  - Que ça soit avoisinant 100 ou > 100, rencontre toujours le problème OoM quand je démarre InfluxDB avec des shards volumineux
  - A testé la seconde méthode en utilisant la valeur 100
    - Pas d'utilisation du swap particulière
    - Quand j'approche de manquer de RAM, par exemple avec un prgogramme python à la volée, commence à utiliser le swap
  - J'ai réquisitionné la majorité de la RAM (848/923Mo)
    - 7:57 (heure du noeud)
  - On va voir si la prochaine compression s'exécute correctement
  - Oui, elle s'est bien passée
    - Bon, la quantité de RAM utilisée avait diminuée (600/923Mo)
    - Une bonne partie ayant été transférée au swap
  - Rencontre d'ailleurs une difficulté au moment de quitter le script Python
    - A l'air de recharger en mémoire les données avant de quitter le programme
    - Mais ne peut pas car mémoire saturée
    - Obligé de le tuer manuellement
  - RAM utilisée par InfluxDB augmente en continue ?
    - Pas déconnant si de plus en plus de données indexées
  - À 11:19 (heure noeud), ne consommait que
    - 191Mo de RAM
    - 133Mo de swap
  - Noeud a exécuté une compaction à ce moment
  - Est passé à
    - 411Mo de RAM
    - 104Mo de swap
    - L'augmentation provenant/concernant bien InfluxDB
  - En redémarrant InfluxDB, retombe à
    - 185Mo de RAM
    - 55Mo de swap
  - Est-ce que le mécanisme de compression a une fuite mémoire ?
    - Étrange que cela ne soit pas mentionné dans le CHANGELOG
  - N'explique pas pourquoi InfluxDB ne veut pas utiliser plus le swap
  - Nouveau crash détecté en arrivant <2024-02-29 Thu> matin
    - Crash à 00:10 (heure du noeud)
    - Logs
      - [[file:logs/2024-02-29-crash-influxdb.log]]
    - Monitoring
      - [[file:img/2024-02-29-00h10-grafana-hardware-monitoring.png]]
    - Toujours le même problème de OoM alors qu'on a du swap de dispo
  - Pose la question suivante
    - Est-ce que le mécanisme de compression peut utiliser le swap ?
    - Probablement que non
    - Utilisation du swap consiste à stocker sur le système de fichiers les données en mémoire non-utilisées
    - Si à un moment donné, doit faire un traitement qui nécessite de charger en RAM un volume plus important de données que ma quantité maximale de RAM, suis bloqué
- LivingFog: Lire *Leveraging fog computing and LoRaWAN technologies for smart marina management*
  - Lisais rapidement le papier pour des précisions sur LoRaWAN et le choix de cette technologie
  - Ce point n'est pas vraiment détaillé
  - Point intéressant est que la plateforme fog doit être utilisable par un ensemble de tenants
    - Idéal est que chacun-e puisse déployer son application
    - Pose des problèmes de sécurité
      - Consommation/Accaparation des ressources
      - Accès aux données des applications des autres tenants
    - À garder en tête
- SmartSense: Documenter les dernières observations concernant le crash d'InfluxDB
  - Depuis ma MàJ de l'issue, j'ai observé
    - Crashs avec durée de vie des shards de 2h puis 1h (minimum)
      - Donc la borne minimale au coût de mécanisme de compression des shards est déjà supérieure aux ressources de notre système
    - Augmentation de la taille du swap à 1Go, augmentation de la swappiness à 100
      - Observe qu'InfluxDB n'utilise pas le swap pour l'exécution du mécanisme de compression
- SmartSense: Envoyer mail pour réunion point sur début du contrat
  - De ce que je vois de leur agenda, la date du <2024-03-13 Wed> serait la 1ère à convenir
    - Y a bien des créneaux le <2024-03-11 Mon>
    - Mais Guillermo rentrera tout juste de congés
    - P-e pas le plus sympa pour lui
  - Faudrait que je prenne des nouvelles de Guillaume
    - Notamment comment ça va passer pour lui après son arrêt
      - Télétravail ? Arrêt prolongé ?
    - En profiter pour lui parler de la réunion et lui proposer d'y participer
- SmartSense: Lire l'issue *Compaction crash loops and data loss on Raspberry Pi 3 B+ under minimal load*
  - Issue disponible ici : https://github.com/influxdata/influxdb/issues/11339
  - Des gens rencontrent un problème similaire au notre depuis 2019
  - Lié à : https://github.com/influxdata/influxdb/pull/12362
  - Donc pas exactement similaire au notre
  - Eux saturent l'espace adressable d'un système 32-bits
  - Nous, InfluxDB plante bien avant
  - Mentionne bien que le mécanisme de compression nécessite probablement en mémoire le double de la taille du shard
  - Et qu'il peut échouer si le système manque de RAM à ce moment
  - Taille de nos shards
    - Étonnament, a l'air de pas mal varier
    - Premiers shards, compressés, ont une taille de ~25Mo
    - Shards suivants, toujours compressés, ont une taille de ~40Mo
    - Derniers shards, non compressés, ont une taille de ~40Mo
    - Donc le double reste d'un ordre de grandeur correct
  - Mais j'ai l'impression d'avoir observé l'appli crasher alors qu'on avait bien plus que ce montant de RAM disponible
  - Plusieurs retours d'expériences sur le fait que de passer à OS 64-bits permet de réduire la footprint en RAM d'InfluxDB
    - Sans raison particulière, i.e. sans avoir mis InfluxDB à jour
  - Mais programmes utiliseraient + de RAM de manière initiale
  - Globalement, en ressort qu'InfluxDB ne supporte pas les systèmes 32-bits
  - Passer à un système 64-bits peut, de manière surprenante, améliorer la stabilité du système
    - Même si la config ne le justifie pas (< 4Go de RAM)
    - À tester si ça aide dans notre cas
  - On est pas non plus à la dernière version d'InfluxDB compatible avec notre code
    - Actuellement à la 1.6.4
    - La 1.8.10 (dernière version avant la 2.0.0) est compatible d'après les tests
    - Faudrait aussi tester cette version, voir si elle améliore la situation
    - Me paraît une bonne idée dans tous les cas de se maintenir à jour
  - J'ai mis à jour InfluxDB sur le noeud
    - J'ai pu trouver le bon paquet ici : https://github.com/influxdata/influxdb/releases/tag/v1.8.10
    - Juste eu besoin de déinstaller influxdb-client sur le noeud
      - Entrait en conflit avec cette nouvelle version
      - Semblerait que le client est inclus dans ce paquet désormais
  - À voir si ça a un impact sur le comportement du système
** Semaine du <2024-02-19 Mon> au <2024-02-23 Fri>
*** Planned
**** DONE SmartSense: Revert la version de l'application à avant le workaround pour InfluxDB
CLOSED: [2024-02-19 Mon 09:18]
- Je n'ai pas constaté le problème de crash de InfluxDB avec la version fournie
- Peut supposer qu'il est effectivement contourné par le workaround implémenté
- Mais ce workaround n'est pas satisfaisant
  - Données difficiles à importer par la suite vu qu'éclatées en de nombreux fichiers
- Serait intéressant de voir si une autre solution peut être mise en place
- Revert à la version pré-workaround de l'application pour observer et documenter le problème
**** DONE SmartSense: Identifier les paramètres de configuration d'InfluxDB pertinents pour la compaction
CLOSED: [2024-02-20 Tue 15:26]
- L'origine des crashs d'InfluxDB a bien l'air d'être le mécanisme de compaction de la base de données
- Il serait intéressant de jeter un oeil aux différents paramètres de configuration
  - https://docs.influxdata.com/influxdb/v1/administration/config/
- Et d'identifier lesquels peuvent être pertinents dans notre cas
- Sachant que nous voulons soit :
  - Désactiver le mécanisme de compression, puisque nous ne faisons aucune lecture sur le noeud
  - Augmenter la fréquence du mécanisme de compression, de façon à rendre ses exécutions moins gourmandes
**** DONE SmartSense: Monitorer l'instance locale d'InfluxDB
CLOSED: [2024-02-21 Wed 11:20]
- Maintenant que j'ai revert le workaround
- Le problème rencontré autrefois devrait se reproduire
- Monitorer l'exécution de l'application pour détecter et identifier le problème rencontré
**** DONE SmartSense: Intégrer la branche *refactor/mqtt-interface-remove-inheritance*
CLOSED: [2024-02-22 Thu 11:17]
- Je voulais pas forcément intégrer cette modif
- Mais une bonne partie des commits de *refactor/main-app* modifient des lignes concernées par ces changements
- Je me retrouve avec des conflits fréquemment lorsque j'essaie de rebase en le retirant de l'historique
- Me parait plus safe, et préférable, d'intégrer ces changements
**** DONE SmartSense: Étudier la migration vers une nouvelle version d'InfluxDB
CLOSED: [2024-02-23 Fri 16:53]
- Le projet utilise pour le moment la version 1.6.4 d'InfluxDB
- On en est actuellement à la version 2.7.5
- Voir si y a un intérêt à migrer à cette nouvelle version
  - Features, performances
- Voir surtout si ça peut tourner sur la rpi qui équipe le noeud SmartSense
  - Tourne sur un Compute Module 3 : https://www.raspberrypi.com/products/compute-module-3/
**** DONE SmartSense: Tester les différentes configurations d'InfluxDB
CLOSED: [2024-02-23 Fri 16:54]
- Dans le but de résoudre le problème de crash d'InfluxDB
  - Provoqué par une compaction des shards trop couteuse semblerait
- A identifié des paramètres de la config qui pourraient avoir un impact si modifié
  - Set max-concurrent-compactions à 1 pour déterminer si cela suffit
  - Sinon
    - Modifier la retention policy autogen
      - ALTER RETENTION POLICY autogen ON SensorData SHARD DURATION 4h
      - ALTER RETENTION POLICY autogen ON collectd SHARD DURATION 4h
    - Et set compact-full-write-cold-duration à 4h
- Voir si cela permet de prévenir le crash
**** IN-PROGRESS SmartSense: Intégrer la branche *refactor/main-app*
- MàJ par rapport à *refactor/mqtt-interface-remove-inheritance*
- Faire la MR
**** IN-PROGRESS SmartSense: Intégrer la branche *refactor/naming-conventions*
- Préparer la MR, en attendant l'intégration de *refactor/main-app*
**** IN-PROGRESS SmartSense: Intégrer la branche *refactor/usb-board-manager*
- Faire la MR
**** IN-PROGRESS SmartSense: Traiter les problèmes relevés lors de l'uniformisation du code
- J'ai noté plusieurs points problématiques au cours de mon passage sur l'ensemble des modules
  - Le module *sensor_board* est inutilisé
    - Code obsolète à supprimer ?
  - La librairie *telnetlib* est dépréciée
    - Passer à https://pypi.org/project/telnetlib3/ ?
  - Une conditionnelle étrange est effectuée dans le module *tlc59108_manager*
    - https://gitlab.inria.fr/smartsense/3douest/node-app/smartsense-node-app/-/blob/3feb5c56dc5e74dcea9eb2f2ba98bec40029acdf/TLC59108Manager.py#L370
    - Revient à `if data[0]`
    - `and` est-il l'opérateur désiré ?
    - Simplifier/corriger
  - Les appels à *OpenOCDManager.update()* dans *test_ocd* manquent d'un paramètre
    - J'ai l'impression que c'est le paramètre *board*
    - Ajouter les valeurs correctes ?
  - *USBBoardManager.read_data()* et *parse_to_mqtt()* sont particulièrement sujets aux retours de Pylint
    - Trop d'instructions
    - Trop de branchements
    - Trop de blocs imbriqués
    - Refactorer pour améliorer la lisibilité du code ?
    - Voir si on peut aussi factoriser du code à l'aide de fonctions
  - Corriger ces problèmes
    - Ou si pas de solution claire et rapide, créer une issue correspondante
**** IN-PROGRESS SmartSense: Étudier la mise en place d'une interface WiFi
- Voir si un hardware donné est conseillé pour la Raspberry CM3
- Et comment ça s'utiliserait d'un POV logiciel
**** CANCELLED SmartSense: Ajouter *NodeManager*
- Idée est d'avoir un module qui se charge
  - Du provisioning
  - Des commandes ciblant le noeud entier
- Permet de démêler le code de *MainApp* du code du noeud
  - Et le routing des messages de leur traitement
*** Done
- SmartSense: Revert la version de l'application à avant le workaround pour InfluxDB
  - Création d'un backup de l'application dans /home/pi/backupApp
  - Stop de l'application
    - systemctl stop mainApp
  - Suppression des DBs
    - DROP DATABASE SensorData
  - Petit doute sur le comportement de l'application à propos de collectd
  - Qu'est-ce qui se passe si la BD n'existe pas au lancement de l'application
  - Dans le doute
    - DROP DATABASE collectd
  - À la réflexion, rien à voir avec l'application dans cette version
  - J'ai en effet pu constater que les writes échouaient
  - Recréation de la DB
    - CREATE DATABASE collectd
  - Récupération du code source à l'état désiré et transfert
    - git checkout 29a50461
    - scp DataToLocalInfluxDB.py  pi@192.168.1.2:/home/pi
    - sudo mv DataToLocalInfluxDB.py /home/root/App (sur rpi)
  - Redémarrage de l'application
    - systemctl start mainApp
  - Plus qu'à attendre l'erreur
- SmartSense: État du noeud au <2024-02-19 Mon>
  - Avec mes jours de télétravail, je me retrouve avec le noeud qui a tourné durant ~1 semaine
  - Semblerait qu'aucun bug particulier ne se soit produit pendant ce temps
    - [[file:img/2024-02-19-smartsense-node-uptime-1-week.png]]
  - A bien quelques erreurs loggées, mais concernent le parsing de trame MQTT
    - [[file:img/2024-02-19-smartsense-node-errors-parsing-data.png]]
  - Le workaround a l'air d'être efficace
- SmartSense: Intégrer la branche *refactor/naming-conventions*
  - MR préparée
- SmartSense: Traiter les problèmes relevés lors de l'uniformisation du code
  - Module `sensor_board` inutilisé
    - J'ai posé la question à Mickaël, à voir s'il sait quelque chose à ce sujet
  - `USBBoardManager.read_data()` et `parse_to_mqtt()`
    - Décomposition de `read_data()` en plusieurs méthodes
      - `handle_incoming_msgs()`, qui correspond au traitement des messages en entrée du module
        - Commande stop/start/timestamp et message MQTT
        - Quelle différence de nature entre les deux types de message?
  - La librairie *telnetlib* est dépréciée
    - La page annonçant la dépréciation de la librairie propose 2 alternatives
      - Lien de la page : https://peps.python.org/pep-0594/#deprecated-modules
    - *telnetlib3* et *Exscript*
      - https://github.com/jquast/telnetlib3
      - https://github.com/knipknap/exscript
    - *Exscript* semble 2.5x plus populaire
    - Mais bien plus ancien (2007 vs 2013)
    - Moyennement convaincu par l'API de *telnetlib3*
      - Asynchrone, donc besoin de faire remonter ce changement sur toute la chaîne d'appels
      - Absence de `read_until()` avec timeout
- SmartSense: Monitorer l'instance locale d'InfluxDB
  - Suis les logs des services de l'application et d'InfluxDB
    - journalctl -f -u mainApp/influxdb
  - Et les performances systèmes (htop)
  - A vu un pic de consommation mémoire
    - Passé de 300Mo de RAM à 850Mo
      - Sature presque les 923Mo du noeud
    - Swap quant à lui saturé (100Mo)
  - La consommation mémoire est retombée ensuite
    - 350Mo
  - Mais pas le swap
  - Pas de logs particuliers côté mainApp à ce moment
  - Par contre, niveau influxdb, on a :
    #+BEGIN_PLAIN
mai 13 21:24:54 raspberrypi influxd[435]: ts=2023-05-13T19:24:54.806599Z lvl=info msg="Cache snapshot (start)" log_id=0hdK~cBG000 engine=tsm1 trace_id=0hme0UaW000 op_name=tsm1_cache_snapshot op_event=start
mai 13 21:24:56 raspberrypi influxd[435]: ts=2023-05-13T19:24:56.780813Z lvl=info msg="Snapshot for path written" log_id=0hdK~cBG000 engine=tsm1 trace_id=0hme0UaW000 op_name=tsm1_cache_snapshot path=/data/influxdb/data/SensorData/autogen/706 duration=1974.300ms
mai 13 21:24:56 raspberrypi influxd[435]: ts=2023-05-13T19:24:56.781155Z lvl=info msg="Cache snapshot (end)" log_id=0hdK~cBG000 engine=tsm1 trace_id=0hme0UaW000 op_name=tsm1_cache_snapshot op_event=end op_elapsed=1974.582ms
mai 13 21:24:56 raspberrypi influxd[435]: ts=2023-05-13T19:24:56.807596Z lvl=info msg="TSM compaction (start)" log_id=0hdK~cBG000 engine=tsm1 tsm1_level=1 tsm1_strategy=level trace_id=0hme0bPl000 op_name=tsm1_compact_group op_event=start
mai 13 21:24:56 raspberrypi influxd[435]: ts=2023-05-13T19:24:56.807735Z lvl=info msg="Beginning compaction" log_id=0hdK~cBG000 engine=tsm1 tsm1_level=1 tsm1_strategy=level trace_id=0hme0bPl000 op_name=tsm1_compact_group tsm1_files_n=8
mai 13 21:24:56 raspberrypi influxd[435]: ts=2023-05-13T19:24:56.807794Z lvl=info msg="Compacting file" log_id=0hdK~cBG000 engine=tsm1 tsm1_level=1 tsm1_strategy=level trace_id=0hme0bPl000 op_name=tsm1_compact_group tsm1_index=0 tsm1_file=/data/influxdb/data/SensorData/autogen/706/000000009-000000001.tsm
mai 13 21:24:56 raspberrypi influxd[435]: ts=2023-05-13T19:24:56.807853Z lvl=info msg="Compacting file" log_id=0hdK~cBG000 engine=tsm1 tsm1_level=1 tsm1_strategy=level trace_id=0hme0bPl000 op_name=tsm1_compact_group tsm1_index=1 tsm1_file=/data/influxdb/data/SensorData/autogen/706/000000010-000000001.tsm
mai 13 21:24:56 raspberrypi influxd[435]: ts=2023-05-13T19:24:56.807914Z lvl=info msg="Compacting file" log_id=0hdK~cBG000 engine=tsm1 tsm1_level=1 tsm1_strategy=level trace_id=0hme0bPl000 op_name=tsm1_compact_group tsm1_index=2 tsm1_file=/data/influxdb/data/SensorData/autogen/706/000000011-000000001.tsm
mai 13 21:24:56 raspberrypi influxd[435]: ts=2023-05-13T19:24:56.807970Z lvl=info msg="Compacting file" log_id=0hdK~cBG000 engine=tsm1 tsm1_level=1 tsm1_strategy=level trace_id=0hme0bPl000 op_name=tsm1_compact_group tsm1_index=3 tsm1_file=/data/influxdb/data/SensorData/autogen/706/000000012-000000001.tsm
mai 13 21:24:56 raspberrypi influxd[435]: ts=2023-05-13T19:24:56.808027Z lvl=info msg="Compacting file" log_id=0hdK~cBG000 engine=tsm1 tsm1_level=1 tsm1_strategy=level trace_id=0hme0bPl000 op_name=tsm1_compact_group tsm1_index=4 tsm1_file=/data/influxdb/data/SensorData/autogen/706/000000013-000000001.tsm
mai 13 21:24:56 raspberrypi influxd[435]: ts=2023-05-13T19:24:56.808082Z lvl=info msg="Compacting file" log_id=0hdK~cBG000 engine=tsm1 tsm1_level=1 tsm1_strategy=level trace_id=0hme0bPl000 op_name=tsm1_compact_group tsm1_index=5 tsm1_file=/data/influxdb/data/SensorData/autogen/706/000000014-000000001.tsm
mai 13 21:24:56 raspberrypi influxd[435]: ts=2023-05-13T19:24:56.808140Z lvl=info msg="Compacting file" log_id=0hdK~cBG000 engine=tsm1 tsm1_level=1 tsm1_strategy=level trace_id=0hme0bPl000 op_name=tsm1_compact_group tsm1_index=6 tsm1_file=/data/influxdb/data/SensorData/autogen/706/000000015-000000001.tsm
mai 13 21:24:56 raspberrypi influxd[435]: ts=2023-05-13T19:24:56.808196Z lvl=info msg="Compacting file" log_id=0hdK~cBG000 engine=tsm1 tsm1_level=1 tsm1_strategy=level trace_id=0hme0bPl000 op_name=tsm1_compact_group tsm1_index=7 tsm1_file=/data/influxdb/data/SensorData/autogen/706/000000016-000000001.tsm
mai 13 21:24:58 raspberrypi influxd[435]: [httpd] ::1 - admin [13/May/2023:21:24:58 +0200] "POST /write?db=SensorData HTTP/1.1" 204 0 "-" "python-requests/2.21.0" d985282a-f1c3-11ed-8a7e-000000000000 33756
mai 13 21:25:05 raspberrypi influxd[435]: [httpd] ::1 - admin [13/May/2023:21:25:03 +0200] "POST /write?db=SensorData HTTP/1.1" 204 0 "-" "python-requests/2.21.0" dc24e767-f1c3-11ed-8a7f-000000000000 2189585
mai 13 21:25:07 raspberrypi influxd[435]: [httpd] ::1 - admin [13/May/2023:21:25:07 +0200] "POST /write?db=SensorData HTTP/1.1" 204 0 "-" "python-requests/2.21.0" de7238fe-f1c3-11ed-8a80-000000000000 58013
mai 13 21:25:11 raspberrypi influxd[435]: [httpd] ::1 - admin [13/May/2023:21:25:11 +0200] "POST /write?db=SensorData HTTP/1.1" 204 0 "-" "python-requests/2.21.0" e0eb72bc-f1c3-11ed-8a81-000000000000 417895
mai 13 21:25:16 raspberrypi influxd[435]: [httpd] ::1 - admin [13/May/2023:21:25:16 +0200] "POST /write?db=SensorData HTTP/1.1" 204 0 "-" "python-requests/2.21.0" e3fdf960-f1c3-11ed-8a82-000000000000 360349
mai 13 21:25:19 raspberrypi influxd[435]: [httpd] ::1 - admin [13/May/2023:21:25:19 +0200] "POST /write?db=SensorData HTTP/1.1" 204 0 "-" "python-requests/2.21.0" e5fceda3-f1c3-11ed-8a83-000000000000 138687
mai 13 21:25:24 raspberrypi influxd[435]: [httpd] ::1 - admin [13/May/2023:21:25:24 +0200] "POST /write?db=SensorData HTTP/1.1" 204 0 "-" "python-requests/2.21.0" e8815e7b-f1c3-11ed-8a84-000000000000 157898
mai 13 21:25:28 raspberrypi influxd[435]: ts=2023-05-13T19:25:28.295225Z lvl=info msg="Compacted file" log_id=0hdK~cBG000 engine=tsm1 tsm1_level=1 tsm1_strategy=level trace_id=0hme0bPl000 op_name=tsm1_compact_group tsm1_index=0 tsm1_file=/data/influxdb/data/SensorData/autogen/706/000000016-000000002.tsm.tmp
mai 13 21:25:28 raspberrypi influxd[435]: ts=2023-05-13T19:25:28.320221Z lvl=info msg="Finished compacting files" log_id=0hdK~cBG000 engine=tsm1 tsm1_level=1 tsm1_strategy=level trace_id=0hme0bPl000 op_name=tsm1_compact_group tsm1_files_n=1
mai 13 21:25:28 raspberrypi influxd[435]: ts=2023-05-13T19:25:28.320331Z lvl=info msg="TSM compaction (end)" log_id=0hdK~cBG000 engine=tsm1 tsm1_level=1 tsm1_strategy=level trace_id=0hme0bPl000 op_name=tsm1_compact_group op_event=end op_elapsed=31512.756ms
    #+END_PLAIN
  - A bien l'air de correspondre au mécanisme de compression d'InfluxDB
    - Voir s'il est désactivable
    - Voir s'il est nécessaire
  - Augmentation de la RAM de ~70Mo
    - Dûe à un mécanisme de cache d'InfluxDB
      #+BEGIN_PLAIN
mai 13 21:44:49 raspberrypi influxd[435]: ts=2023-05-13T19:44:49.815468Z lvl=info msg="Cache snapshot (start)" log_id=0hdK~cBG000 engine=tsm1 trace_id=0hmf9Qal000 op_name=tsm1_cache_snapshot op_event=start
mai 13 21:44:51 raspberrypi influxd[435]: ts=2023-05-13T19:44:51.828599Z lvl=info msg="Snapshot for path written" log_id=0hdK~cBG000 engine=tsm1 trace_id=0hmf9Qal000 op_name=tsm1_cache_snapshot path=/data/influxdb/data/SensorData/autogen/706 duration=2014.542ms
mai 13 21:44:51 raspberrypi influxd[435]: ts=2023-05-13T19:44:51.841164Z lvl=info msg="Cache snapshot (end)" log_id=0hdK~cBG000 engine=tsm1 trace_id=0hmf9Qal000 op_name=tsm1_cache_snapshot op_event=end op_elapsed=2025.725ms
      #+END_PLAIN
  - RAM revient à sa valeur d'origine au bout d'un moment
  - Ça a enfin explosé à 16h50
  - La base de données crash au cours de sa compaction
    #+BEGIN_PLAIN
mai 14 00:02:08 raspberrypi influxd[435]: [httpd] ::1 - admin [14/May/2023:00:02:08 +0200] "POST /write?db=SensorData HTTP/1.1" 204 0 "-" "python-requests/2.21.0" ce13ecf6-f1d9-11ed-9349-000000000000 47503
mai 14 00:02:12 raspberrypi influxd[435]: [httpd] ::1 - admin [14/May/2023:00:02:12 +0200] "POST /write?db=SensorData HTTP/1.1" 204 0 "-" "python-requests/2.21.0" d0877f75-f1d9-11ed-934a-000000000000 42272
mai 14 00:02:16 raspberrypi influxd[435]: [httpd] ::1 - admin [14/May/2023:00:02:16 +0200] "POST /write?db=SensorData HTTP/1.1" 204 0 "-" "python-requests/2.21.0" d30c972d-f1d9-11ed-934b-000000000000 29123
mai 14 00:02:21 raspberrypi influxd[435]: [httpd] ::1 - admin [14/May/2023:00:02:21 +0200] "POST /write?db=SensorData HTTP/1.1" 204 0 "-" "python-requests/2.21.0" d5ae5268-f1d9-11ed-934c-000000000000 53104
mai 14 00:02:21 raspberrypi influxd[435]: ts=2023-05-13T22:02:21.806703Z lvl=info msg="Cache snapshot (start)" log_id=0hdK~cBG000 engine=tsm1 trace_id=0hmn14wW000 op_name=tsm1_cache_snapshot op_event=start
mai 14 00:02:23 raspberrypi influxd[435]: ts=2023-05-13T22:02:23.767757Z lvl=info msg="Snapshot for path written" log_id=0hdK~cBG000 engine=tsm1 trace_id=0hmn14wW000 op_name=tsm1_cache_snapshot path=/data/influxdb/data/SensorData/autogen/706 duration=1961.160ms
mai 14 00:02:23 raspberrypi influxd[435]: ts=2023-05-13T22:02:23.768046Z lvl=info msg="Cache snapshot (end)" log_id=0hdK~cBG000 engine=tsm1 trace_id=0hmn14wW000 op_name=tsm1_cache_snapshot op_event=end op_elapsed=1961.412ms
mai 14 00:02:23 raspberrypi influxd[435]: ts=2023-05-13T22:02:23.807542Z lvl=info msg="TSM compaction (start)" log_id=0hdK~cBG000 engine=tsm1 tsm1_level=1 tsm1_strategy=level trace_id=0hmn1Ckl000 op_name=tsm1_compact_group op_event=start
mai 14 00:02:23 raspberrypi influxd[435]: ts=2023-05-13T22:02:23.807708Z lvl=info msg="Beginning compaction" log_id=0hdK~cBG000 engine=tsm1 tsm1_level=1 tsm1_strategy=level trace_id=0hmn1Ckl000 op_name=tsm1_compact_group tsm1_files_n=8
mai 14 00:02:23 raspberrypi influxd[435]: ts=2023-05-13T22:02:23.807755Z lvl=info msg="Compacting file" log_id=0hdK~cBG000 engine=tsm1 tsm1_level=1 tsm1_strategy=level trace_id=0hmn1Ckl000 op_name=tsm1_compact_group tsm1_index=0 tsm1_file=/data/influxdb/data/SensorData/autogen/706/000000017-000000001.tsm
mai 14 00:02:23 raspberrypi influxd[435]: ts=2023-05-13T22:02:23.807809Z lvl=info msg="Compacting file" log_id=0hdK~cBG000 engine=tsm1 tsm1_level=1 tsm1_strategy=level trace_id=0hmn1Ckl000 op_name=tsm1_compact_group tsm1_index=1 tsm1_file=/data/influxdb/data/SensorData/autogen/706/000000018-000000001.tsm
mai 14 00:02:23 raspberrypi influxd[435]: ts=2023-05-13T22:02:23.807861Z lvl=info msg="Compacting file" log_id=0hdK~cBG000 engine=tsm1 tsm1_level=1 tsm1_strategy=level trace_id=0hmn1Ckl000 op_name=tsm1_compact_group tsm1_index=2 tsm1_file=/data/influxdb/data/SensorData/autogen/706/000000019-000000001.tsm
mai 14 00:02:23 raspberrypi influxd[435]: ts=2023-05-13T22:02:23.807911Z lvl=info msg="Compacting file" log_id=0hdK~cBG000 engine=tsm1 tsm1_level=1 tsm1_strategy=level trace_id=0hmn1Ckl000 op_name=tsm1_compact_group tsm1_index=3 tsm1_file=/data/influxdb/data/SensorData/autogen/706/000000020-000000001.tsm
mai 14 00:02:23 raspberrypi influxd[435]: ts=2023-05-13T22:02:23.807960Z lvl=info msg="Compacting file" log_id=0hdK~cBG000 engine=tsm1 tsm1_level=1 tsm1_strategy=level trace_id=0hmn1Ckl000 op_name=tsm1_compact_group tsm1_index=4 tsm1_file=/data/influxdb/data/SensorData/autogen/706/000000021-000000001.tsm
mai 14 00:02:23 raspberrypi influxd[435]: ts=2023-05-13T22:02:23.808011Z lvl=info msg="Compacting file" log_id=0hdK~cBG000 engine=tsm1 tsm1_level=1 tsm1_strategy=level trace_id=0hmn1Ckl000 op_name=tsm1_compact_group tsm1_index=5 tsm1_file=/data/influxdb/data/SensorData/autogen/706/000000022-000000001.tsm
mai 14 00:02:23 raspberrypi influxd[435]: ts=2023-05-13T22:02:23.808061Z lvl=info msg="Compacting file" log_id=0hdK~cBG000 engine=tsm1 tsm1_level=1 tsm1_strategy=level trace_id=0hmn1Ckl000 op_name=tsm1_compact_group tsm1_index=6 tsm1_file=/data/influxdb/data/SensorData/autogen/706/000000023-000000001.tsm
mai 14 00:02:23 raspberrypi influxd[435]: ts=2023-05-13T22:02:23.808110Z lvl=info msg="Compacting file" log_id=0hdK~cBG000 engine=tsm1 tsm1_level=1 tsm1_strategy=level trace_id=0hmn1Ckl000 op_name=tsm1_compact_group tsm1_index=7 tsm1_file=/data/influxdb/data/SensorData/autogen/706/000000024-000000001.tsm
mai 14 00:02:25 raspberrypi influxd[435]: [httpd] ::1 - admin [14/May/2023:00:02:25 +0200] "POST /write?db=SensorData HTTP/1.1" 204 0 "-" "python-requests/2.21.0" d805ca3e-f1d9-11ed-934d-000000000000 33651
mai 14 00:02:34 raspberrypi systemd[1]: influxdb.service: Main process exited, code=killed, status=9/KILL
mai 14 00:02:34 raspberrypi systemd[1]: influxdb.service: Failed with result 'signal'.
    #+END_PLAIN
  - Pendant ce temps, mainApp essaie d'enregistrer ses données et rencontre une erreur
    #+BEGIN_PLAIN
mai 14 00:02:25 raspberrypi mainApp.py[2526]: [DataToLocalInfluxDB] : write_points 100
mai 14 00:02:38 raspberrypi mainApp.py[2526]: [DataToLocalInfluxDB] : data not saved, timeout ! HTTPConnectionPool(host='localhost', port=8086): Max retries exceeded with url: /write?db=SensorData (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x74bc8350>: Failed to establish a new connection: [Errno 111] Connection refused'))
mai 14 00:02:38 raspberrypi mainApp.py[2526]: [DataToLocalInfluxDB] : data not saved, timeout ! HTTPConnectionPool(host='localhost', port=8086): Max retries exceeded with url: /write?db=SensorData (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x74bc88f0>: Failed to establish a new connection: [Errno 111] Connection refused'))
mai 14 00:02:38 raspberrypi mainApp.py[2526]: [DataToLocalInfluxDB] : data not saved, timeout ! HTTPConnectionPool(host='localhost', port=8086): Max retries exceeded with url: /write?db=SensorData (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x74be50b0>: Failed to establish a new connection: [Errno 111] Connection refused'))
mai 14 00:02:38 raspberrypi mainApp.py[2526]: [DataToLocalInfluxDB] : data not saved, timeout ! HTTPConnectionPool(host='localhost', port=8086): Max retries exceeded with url: /write?db=SensorData (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x74bc8d10>: Failed to establish a new connection: [Errno 111] Connection refused'))
mai 14 00:02:38 raspberrypi mainApp.py[2526]: [DataToLocalInfluxDB] : data not saved, timeout ! HTTPConnectionPool(host='localhost', port=8086): Max retries exceeded with url: /write?db=SensorData (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x74bc84f0>: Failed to establish a new connection: [Errno 111] Connection refused'))
mai 14 00:02:38 raspberrypi mainApp.py[2526]: [DataToLocalInfluxDB] : data not saved, timeout ! HTTPConnectionPool(host='localhost', port=8086): Max retries exceeded with url: /write?db=SensorData (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x74bc83b0>: Failed to establish a new connection: [Errno 111] Connection refused'))
mai 14 00:02:38 raspberrypi mainApp.py[2526]: [DataToLocalInfluxDB] : data not saved, timeout ! HTTPConnectionPool(host='localhost', port=8086): Max retries exceeded with url: /write?db=SensorData (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x74be5510>: Failed to establish a new connection: [Errno 111] Connection refused'))
mai 14 00:02:38 raspberrypi mainApp.py[2526]: [DataToLocalInfluxDB] : data not saved, timeout ! HTTPConnectionPool(host='localhost', port=8086): Max retries exceeded with url: /write?db=SensorData (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x74be5b10>: Failed to establish a new connection: [Errno 111] Connection refused'))
mai 14 00:02:38 raspberrypi mainApp.py[2526]: [DataToLocalInfluxDB] : data not saved, timeout ! HTTPConnectionPool(host='localhost', port=8086): Max retries exceeded with url: /write?db=SensorData (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x74bc81f0>: Failed to establish a new connection: [Errno 111] Connection refused'))
mai 14 00:02:38 raspberrypi mainApp.py[2526]: [DataToLocalInfluxDB] : data not saved, timeout ! HTTPConnectionPool(host='localhost', port=8086): Max retries exceeded with url: /write?db=SensorData (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x74bc88b0>: Failed to establish a new connection: [Errno 111] Connection refused'))
mai 14 00:02:38 raspberrypi mainApp.py[2526]: [DataToLocalInfluxDB] : data not saved, timeout ! HTTPConnectionPool(host='localhost', port=8086): Max retries exceeded with url: /write?db=SensorData (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x74be5ab0>: Failed to establish a new connection: [Errno 111] Connection refused'))
mai 14 00:02:38 raspberrypi mainApp.py[2526]: [DataToLocalInfluxDB] : data not saved, timeout ! HTTPConnectionPool(host='localhost', port=8086): Max retries exceeded with url: /write?db=SensorData (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x74be5690>: Failed to establish a new connection: [Errno 111] Connection refused'))
mai 14 00:02:38 raspberrypi mainApp.py[2526]: [DataToLocalInfluxDB] : data not saved, timeout ! HTTPConnectionPool(host='localhost', port=8086): Max retries exceeded with url: /write?db=SensorData (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x74be53f0>: Failed to establish a new connection: [Errno 111] Connection refused'))
mai 14 00:02:38 raspberrypi mainApp.py[2526]: [DataToLocalInfluxDB] : data not saved, timeout ! HTTPConnectionPool(host='localhost', port=8086): Max retries exceeded with url: /write?db=SensorData (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x74bc8f50>: Failed to establish a new connection: [Errno 111] Connection refused'))
mai 14 00:02:38 raspberrypi mainApp.py[2526]: [DataToLocalInfluxDB] : data not saved, timeout ! HTTPConnectionPool(host='localhost', port=8086): Max retries exceeded with url: /write?db=SensorData (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x74bc8730>: Failed to establish a new connection: [Errno 111] Connection refused'))
    #+END_PLAIN
  - Au bout d'un moment, la BDD redémarre automatiquement et réaccepte les requêtes
    - Voir [[file:logs/2024-02-19-crash-influxdb.log]], lignes 24-112
  - MainApp arrive alors à enregistrer ses données
    - [[file:logs/2024-02-19-crash-main-app.log]], lignes 445-449
  - J'ai récup les graphiques de Grafana au moment du crash
    - Sensor Data
      - [[file:img/2024-02-19-crash-grafana-sensor-data-part-1.png]]
      - [[file:img/2024-02-19-crash-grafana-sensor-data-part-2.png]]
    - Hardware Monitoring
      - [[file:img/2024-02-19-crash-grafana-hardware-monitoring-part-1.png]]
      - [[file:img/2024-02-19-crash-grafana-hardware-monitoring-part-2.png]]
  - Observe tout de même une perte de données à cette période
    - Pour quelle(s) raison(s) ?
  - À la réflexion, les courbes concernant les métriques systèmes sont étranges
    - Le pic de consommation est montré à 00:03:00, i.e. après le redémarrage d'influxdb et le lancement d'une nouvelle compaction
    - Rien de particulier n'apparaît à 00:02:20, lors de la compaction initiale, et à 00:02:30, lors du kill d'influxdb
  - P-e que le fait que grafana se base sur les données stockées par influxdb a un lien
    - Si influxdb était en train de planter à ce moment là
    - N'a pas pu enregistrer les données collectées par collectd
      - Pas sûr que le support de collectd par influxdb intègre un mécanisme de bufferisation des écritures
    - Les courbes montrées se basent-elles donc sur des données ou font-elles de l'interpolation ?
      - L'interpolation expliquerait l'absence de pic
  - Mickaël me faisait part de sa surprise que l'application redémarre normalement
  - Mais à la réflexion, j'ai p-e eu de la chance sur ce coup-ci
  - Influxdb redémarre et retente d'effectuer une compaction
    - Ici c'est passé
  - Mais possible que cette nouvelle compaction fasse de nouveau planter influxdb
  - Tomberait alors dans une boucle de crashs
  - Voir pour poursuivre/reproduire l'expérience si on peut observer cette boucle de crashs
  - J'ai pu observé cette boucle de crash
  - À 02:55 (heure node), la BD a de nouveau planté
  - Sauf que cette fois-ci, la nouvelle compaction enclenché à son redémarrage provoque un nouveau crash
  - A bouclé ensuite jusqu'à ce que je l'interrompe à 03:15
  - Observant pendant ce temps les métriques système, le problème semble venir d'une saturation de la RAM
  - Les logs confirment cette hypothèse
    #+BEGIN_PLAIN
mai 14 03:15:45 raspberrypi influxd[9666]: runtime: out of memory: cannot allocate 74612736-byte block (887980032 in use)
mai 14 03:15:45 raspberrypi influxd[9666]: fatal error: out of memory
mai 14 03:15:45 raspberrypi influxd[9666]: runtime stack:
mai 14 03:15:45 raspberrypi influxd[9666]: runtime.throw(0x8ca717, 0xd)
mai 14 03:15:45 raspberrypi influxd[9666]:         /usr/lib/go-1.11/src/runtime/panic.go:608 +0x5c
mai 14 03:15:45 raspberrypi influxd[9666]: runtime.largeAlloc(0x47265ce, 0x101, 0x1918c40)
mai 14 03:15:45 raspberrypi influxd[9666]:         /usr/lib/go-1.11/src/runtime/malloc.go:1021 +0x120
mai 14 03:15:45 raspberrypi influxd[9666]: runtime.mallocgc.func1()
mai 14 03:15:45 raspberrypi influxd[9666]:         /usr/lib/go-1.11/src/runtime/malloc.go:914 +0x38
mai 14 03:15:45 raspberrypi influxd[9666]: runtime.systemstack(0x152)
mai 14 03:15:45 raspberrypi influxd[9666]:         /usr/lib/go-1.11/src/runtime/asm_arm.s:354 +0x84
mai 14 03:15:45 raspberrypi influxd[9666]: runtime.mstart()
mai 14 03:15:45 raspberrypi influxd[9666]:         /usr/lib/go-1.11/src/runtime/proc.go:1229
mai 14 03:15:45 raspberrypi influxd[9666]: goroutine 63 [running]:
mai 14 03:15:45 raspberrypi influxd[9666]: runtime.systemstack_switch()
mai 14 03:15:45 raspberrypi influxd[9666]:         /usr/lib/go-1.11/src/runtime/asm_arm.s:298 +0x4 fp=0x5cd14f4 sp=0x5cd14f0 pc=0x6aa68
mai 14 03:15:45 raspberrypi influxd[9666]: runtime.mallocgc(0x47265ce, 0x7e4568, 0x1501, 0x3ff00000)
mai 14 03:15:45 raspberrypi influxd[9666]:         /usr/lib/go-1.11/src/runtime/malloc.go:913 +0x898 fp=0x5cd1558 sp=0x5cd14f4 pc=0x1cdf0
mai 14 03:15:45 raspberrypi influxd[9666]: runtime.makeslice(0x7e4568, 0x47265ce, 0x47265ce, 0x0, 0xa2a0cf, 0x0)
mai 14 03:15:45 raspberrypi influxd[9666]:         /usr/lib/go-1.11/src/runtime/slice.go:70 +0x68 fp=0x5cd156c sp=0x5cd1558 pc=0x55558
mai 14 03:15:45 raspberrypi influxd[9666]: github.com/golang/snappy.Encode(0x0, 0x0, 0x0, 0x29122000, 0x3cfc4df, 0x40ae000, 0x3e8, 0x397e000, 0x15c9)
mai 14 03:15:45 raspberrypi influxd[9666]:         github.com/golang/snappy/encode.go:22 +0x1f4 fp=0x5cd1594 sp=0x5cd156c pc=0x468700
mai 14 03:15:45 raspberrypi influxd[9666]: github.com/influxdata/influxdb/tsdb/engine/tsm1.(*StringEncoder).Bytes(0x5cb5668, 0x397e000, 0x15c9, 0x1f41, 0x0, 0x0)
mai 14 03:15:45 raspberrypi influxd[9666]:         github.com/influxdata/influxdb/tsdb/engine/tsm1/string.go:56 +0x44 fp=0x5cd15d0 sp=0x5cd1594 pc=0x63f06c
mai 14 03:15:45 raspberrypi influxd[9666]: github.com/influxdata/influxdb/tsdb/engine/tsm1.encodeStringValuesBlock.func1(0x6448000, 0x3e8, 0x600, 0x9f01c8, 0x1926a80, 0x5cb5668, 0x0, 0x0, 0x0, 0x5cd1674, ...)
mai 14 03:15:45 raspberrypi influxd[9666]:         github.com/influxdata/influxdb/tsdb/engine/tsm1/encoding.gen.go:1252 +0xec fp=0x5cd1624 sp=0x5cd15d0 pc=0x6517e4
mai 14 03:15:45 raspberrypi influxd[9666]: github.com/influxdata/influxdb/tsdb/engine/tsm1.encodeStringValuesBlock(0x0, 0x0, 0x0, 0x6448000, 0x3e8, 0x600, 0x64498a0, 0x1b78800, 0x36c0, 0x1890, ...)
mai 14 03:15:45 raspberrypi influxd[9666]:         github.com/influxdata/influxdb/tsdb/engine/tsm1/encoding.gen.go:1262 +0xdc fp=0x5cd1680 sp=0x5cd1624 pc=0x601134
mai 14 03:15:46 raspberrypi influxd[9666]: github.com/influxdata/influxdb/tsdb/engine/tsm1.StringValues.Encode(0x6448000, 0x3e8, 0x600, 0x0, 0x0, 0x0, 0x64498a0, 0x36c, 0x476, 0x1b78800, ...)
mai 14 03:15:46 raspberrypi influxd[9666]:         github.com/influxdata/influxdb/tsdb/engine/tsm1/encoding.gen.go:1227 +0x44 fp=0x5cd16b0 sp=0x5cd1680 pc=0x601020
mai 14 03:15:46 raspberrypi influxd[9666]: github.com/influxdata/influxdb/tsdb/engine/tsm1.(*tsmKeyIterator).chunkString(0x4030000, 0x0, 0x0, 0x0, 0x36c, 0x36c, 0x6448000)
mai 14 03:15:46 raspberrypi influxd[9666]:         github.com/influxdata/influxdb/tsdb/engine/tsm1/compact.gen.go:772 +0x60 fp=0x5cd1714 sp=0x5cd16b0 pc=0x5ed0b0
mai 14 03:15:46 raspberrypi influxd[9666]: github.com/influxdata/influxdb/tsdb/engine/tsm1.(*tsmKeyIterator).combineString(0x4030000, 0x27c7101, 0x9f1088, 0x27c7190, 0x4)
mai 14 03:15:46 raspberrypi influxd[9666]:         github.com/influxdata/influxdb/tsdb/engine/tsm1/compact.gen.go:697 +0x86c fp=0x5cd179c sp=0x5cd1714 pc=0x5ec6cc
mai 14 03:15:46 raspberrypi influxd[9666]: github.com/influxdata/influxdb/tsdb/engine/tsm1.(*tsmKeyIterator).mergeString(0x4030000)
mai 14 03:15:46 raspberrypi influxd[9666]:         github.com/influxdata/influxdb/tsdb/engine/tsm1/compact.gen.go:632 +0x24c fp=0x5cd17b4 sp=0x5cd179c pc=0x5ebd20
mai 14 03:15:46 raspberrypi influxd[9666]: github.com/influxdata/influxdb/tsdb/engine/tsm1.(*tsmKeyIterator).merge(0x4030000)
mai 14 03:15:46 raspberrypi influxd[9666]:         github.com/influxdata/influxdb/tsdb/engine/tsm1/compact.go:1501 +0xf4 fp=0x5cd17dc sp=0x5cd17b4 pc=0x5f5ee0
mai 14 03:15:46 raspberrypi influxd[9666]: github.com/influxdata/influxdb/tsdb/engine/tsm1.(*tsmKeyIterator).Next(0x4030000, 0x9ad97f9)
mai 14 03:15:46 raspberrypi influxd[9666]:         github.com/influxdata/influxdb/tsdb/engine/tsm1/compact.go:1356 +0xdb8 fp=0x5cd18f0 sp=0x5cd17dc pc=0x5f5c7c
mai 14 03:15:46 raspberrypi influxd[9666]: github.com/influxdata/influxdb/tsdb/engine/tsm1.(*Compactor).write(0x18a8540, 0x4032000, 0x46, 0x9f2cd8, 0x4030000, 0x402e101, 0x0, 0x0)
mai 14 03:15:46 raspberrypi influxd[9666]:         github.com/influxdata/influxdb/tsdb/engine/tsm1/compact.go:1109 +0x144 fp=0x5cd1968 sp=0x5cd18f0 pc=0x5f42a0
mai 14 03:15:46 raspberrypi influxd[9666]: github.com/influxdata/influxdb/tsdb/engine/tsm1.(*Compactor).writeNewFiles(0x18a8540, 0x22, 0x3, 0x3162240, 0x4, 0x4, 0x9f2cd8, 0x4030000, 0x1, 0x8fa554, ...)
mai 14 03:15:46 raspberrypi influxd[9666]:         github.com/influxdata/influxdb/tsdb/engine/tsm1/compact.go:1013 +0x128 fp=0x5cd19c0 sp=0x5cd1968 pc=0x5f3db8
mai 14 03:15:46 raspberrypi influxd[9666]: github.com/influxdata/influxdb/tsdb/engine/tsm1.(*Compactor).compact(0x18a8540, 0x8fa400, 0x3162240, 0x4, 0x4, 0x0, 0x0, 0x0, 0x0, 0x0)
mai 14 03:15:46 raspberrypi influxd[9666]:         github.com/influxdata/influxdb/tsdb/engine/tsm1/compact.go:921 +0x4a8 fp=0x5cd1a50 sp=0x5cd19c0 pc=0x5f3560
mai 14 03:15:46 raspberrypi influxd[9666]: github.com/influxdata/influxdb/tsdb/engine/tsm1.(*Compactor).CompactFull(0x18a8540, 0x3162240, 0x4, 0x4, 0x0, 0x0, 0x0, 0x0, 0x0)
mai 14 03:15:46 raspberrypi influxd[9666]:         github.com/influxdata/influxdb/tsdb/engine/tsm1/compact.go:939 +0x108 fp=0x5cd1aa0 sp=0x5cd1a50 pc=0x5f36f8
mai 14 03:15:46 raspberrypi influxd[9666]: github.com/influxdata/influxdb/tsdb/engine/tsm1.(*compactionStrategy).compactGroup(0x192b080)
mai 14 03:15:46 raspberrypi influxd[9666]:         github.com/influxdata/influxdb/tsdb/engine/tsm1/engine.go:2134 +0xaac fp=0x5cd1f88 sp=0x5cd1aa0 pc=0x613244
mai 14 03:15:46 raspberrypi influxd[9666]: github.com/influxdata/influxdb/tsdb/engine/tsm1.(*compactionStrategy).Apply(0x192b080)
mai 14 03:15:46 raspberrypi influxd[9666]:         github.com/influxdata/influxdb/tsdb/engine/tsm1/engine.go:2111 +0x2c fp=0x5cd1fbc sp=0x5cd1f88 pc=0x612758
mai 14 03:15:46 raspberrypi influxd[9666]: github.com/influxdata/influxdb/tsdb/engine/tsm1.(*Engine).compactHiPriorityLevel.func1(0x5576d00, 0x19760b0, 0x2, 0x192b080)
mai 14 03:15:46 raspberrypi influxd[9666]:         github.com/influxdata/influxdb/tsdb/engine/tsm1/engine.go:2028 +0xc0 fp=0x5cd1fdc sp=0x5cd1fbc pc=0x655220
mai 14 03:15:46 raspberrypi influxd[9666]: runtime.goexit()
mai 14 03:15:46 raspberrypi influxd[9666]:         /usr/lib/go-1.11/src/runtime/asm_arm.s:867 +0x4 fp=0x5cd1fdc sp=0x5cd1fdc pc=0x6c8d4
mai 14 03:15:46 raspberrypi influxd[9666]: created by github.com/influxdata/influxdb/tsdb/engine/tsm1.(*Engine).compactHiPriorityLevel
mai 14 03:15:46 raspberrypi influxd[9666]:         github.com/influxdata/influxdb/tsdb/engine/tsm1/engine.go:2023 +0xf4
mai 14 03:15:46 raspberrypi influxd[9666]: goroutine 1 [chan receive]:
mai 14 03:15:46 raspberrypi influxd[9666]: main.(*Main).Run(0x1989fac, 0x1886068, 0x2, 0x3, 0xfbcf70, 0x0)
mai 14 03:15:46 raspberrypi influxd[9666]:         github.com/influxdata/influxdb/cmd/influxd/main.go:90 +0x234
mai 14 03:15:47 raspberrypi influxd[9666]: main.main()
mai 14 03:15:47 raspberrypi influxd[9666]:         github.com/influxdata/influxdb/cmd/influxd/main.go:45 +0x130
mai 14 03:15:49 raspberrypi influxd[9666]: goroutine 19 [syscall, 1 minutes]:
mai 14 03:15:49 raspberrypi influxd[9666]: os/signal.signal_recv(0x0)
mai 14 03:15:49 raspberrypi influxd[9666]:         /usr/lib/go-1.11/src/runtime/sigqueue.go:139 +0x130
mai 14 03:15:49 raspberrypi influxd[9666]: os/signal.loop()
mai 14 03:15:49 raspberrypi influxd[9666]:         /usr/lib/go-1.11/src/os/signal/signal_unix.go:23 +0x14
mai 14 03:15:49 raspberrypi influxd[9666]: created by os/signal.init.0
mai 14 03:15:49 raspberrypi influxd[9666]:         /usr/lib/go-1.11/src/os/signal/signal_unix.go:29 +0x30
mai 14 03:15:49 raspberrypi influxd[9666]: goroutine 22 [IO wait, 1 minutes]:
mai 14 03:15:49 raspberrypi influxd[9666]: internal/poll.runtime_pollWait(0x647c2fc0, 0x72, 0x0)
mai 14 03:15:49 raspberrypi influxd[9666]:         /usr/lib/go-1.11/src/runtime/netpoll.go:173 +0x44
mai 14 03:15:49 raspberrypi influxd[9666]: internal/poll.(*pollDesc).wait(0x18a5374, 0x72, 0x1824000, 0x0, 0x0)
mai 14 03:15:49 raspberrypi influxd[9666]:         /usr/lib/go-1.11/src/internal/poll/fd_poll_runtime.go:85 +0x7c
mai 14 03:15:49 raspberrypi influxd[9666]: internal/poll.(*pollDesc).waitRead(0x18a5374, 0xffffff00, 0x0, 0x0)
mai 14 03:15:49 raspberrypi influxd[9666]:         /usr/lib/go-1.11/src/internal/poll/fd_poll_runtime.go:90 +0x2c
mai 14 03:15:49 raspberrypi influxd[9666]: internal/poll.(*FD).Accept(0x18a5360, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0)
mai 14 03:15:49 raspberrypi influxd[9666]:         /usr/lib/go-1.11/src/internal/poll/fd_unix.go:384 +0x17c
mai 14 03:15:49 raspberrypi influxd[9666]: net.(*netFD).accept(0x18a5360, 0xffffffff, 0x0, 0x1)
mai 14 03:15:49 raspberrypi influxd[9666]:         /usr/lib/go-1.11/src/net/fd_unix.go:238 +0x20
mai 14 03:15:49 raspberrypi influxd[9666]: net.(*TCPListener).accept(0x19ea738, 0x19d5200, 0x40000000, 0x0)
mai 14 03:15:49 raspberrypi influxd[9666]:         /usr/lib/go-1.11/src/net/tcpsock_posix.go:139 +0x20
mai 14 03:15:49 raspberrypi influxd[9666]: net.(*TCPListener).Accept(0x19ea738, 0x1b0d8, 0x1885cf0, 0x0, 0x1)
mai 14 03:15:49 raspberrypi influxd[9666]:         /usr/lib/go-1.11/src/net/tcpsock.go:260 +0x3c
mai 14 03:15:49 raspberrypi influxd[9666]: github.com/influxdata/influxdb/tcp.(*Mux).Serve(0x19d5200, 0x9f0968, 0x19ea738, 0x19ea738, 0x0)
mai 14 03:15:49 raspberrypi influxd[9666]:         github.com/influxdata/influxdb/tcp/mux.go:75 +0x60
mai 14 03:15:49 raspberrypi influxd[9666]: created by github.com/influxdata/influxdb/cmd/influxd/run.(*Server).Open
mai 14 03:15:49 raspberrypi influxd[9666]:         github.com/influxdata/influxdb/cmd/influxd/run/server.go:398 +0x1ec
mai 14 03:15:49 raspberrypi influxd[9666]: goroutine 117 [select]:
mai 14 03:15:49 raspberrypi influxd[9666]: github.com/influxdata/influxdb/tsdb.(*PointBatcher).Start.func2(0x2a52840, 0x5a9fce0, 0x5a9fcd0, 0x36972c0)
mai 14 03:15:49 raspberrypi influxd[9666]:         github.com/influxdata/influxdb/tsdb/batcher.go:87 +0xcc
mai 14 03:15:49 raspberrypi influxd[9666]: created by github.com/influxdata/influxdb/tsdb.(*PointBatcher).Start
mai 14 03:15:49 raspberrypi influxd[9666]:         github.com/influxdata/influxdb/tsdb/batcher.go:84 +0x108
mai 14 03:15:49 raspberrypi influxd[9666]: goroutine 118 [runnable]:
mai 14 03:15:49 raspberrypi influxd[9666]: internal/poll.runtime_pollWait(0x647c2f40, 0x72, 0x0)
mai 14 03:15:49 raspberrypi influxd[9666]:         /usr/lib/go-1.11/src/runtime/netpoll.go:173 +0x44
mai 14 03:15:49 raspberrypi influxd[9666]: internal/poll.(*pollDesc).wait(0x193c604, 0x72, 0x0, 0x0, 0x0)
mai 14 03:15:49 raspberrypi influxd[9666]:         /usr/lib/go-1.11/src/internal/poll/fd_poll_runtime.go:85 +0x7c
mai 14 03:15:50 raspberrypi influxd[9666]: internal/poll.(*pollDesc).waitRead(0x193c604, 0x402c000, 0x5ac, 0x5ac)
mai 14 03:15:50 raspberrypi influxd[9666]:         /usr/lib/go-1.11/src/internal/poll/fd_poll_runtime.go:90 +0x2c
mai 14 03:15:50 raspberrypi influxd[9666]: internal/poll.(*FD).ReadFrom(0x193c5f0, 0x402c000, 0x5ac, 0x5ac, 0x0, 0x0, 0x0, 0x0, 0x0)
mai 14 03:15:50 raspberrypi influxd[9666]:         /usr/lib/go-1.11/src/internal/poll/fd_unix.go:219 +0x138
mai 14 03:15:50 raspberrypi influxd[9666]: net.(*netFD).readFrom(0x193c5f0, 0x402c000, 0x5ac, 0x5ac, 0x525, 0x9eac98, 0x16fc0, 0x172d0, 0x34632d0)
mai 14 03:15:50 raspberrypi influxd[9666]:         /usr/lib/go-1.11/src/net/fd_unix.go:208 +0x38
mai 14 03:15:50 raspberrypi influxd[9666]: net.(*UDPConn).readFrom(0x4db4cf0, 0x402c000, 0x5ac, 0x5ac, 0x12768, 0x129b4, 0xfcf650, 0x578a2c)
mai 14 03:15:50 raspberrypi influxd[9666]:         /usr/lib/go-1.11/src/net/udpsock_posix.go:47 +0x38
mai 14 03:15:50 raspberrypi influxd[9666]: net.(*UDPConn).ReadFromUDP(0x4db4cf0, 0x402c000, 0x5ac, 0x5ac, 0x0, 0x34cc5e0, 0x0, 0x0)
mai 14 03:15:50 raspberrypi influxd[9666]:         /usr/lib/go-1.11/src/net/udpsock.go:109 +0x58
mai 14 03:15:50 raspberrypi influxd[9666]: github.com/influxdata/influxdb/services/collectd.(*Service).serve(0x19f8150)
mai 14 03:15:50 raspberrypi influxd[9666]:         github.com/influxdata/influxdb/services/collectd/service.go:348 +0x90
mai 14 03:15:50 raspberrypi influxd[9666]: github.com/influxdata/influxdb/services/collectd.(*Service).Open.func2(0x19f8150)
mai 14 03:15:50 raspberrypi influxd[9666]:         github.com/influxdata/influxdb/services/collectd/service.go:208 +0x48
mai 14 03:15:50 raspberrypi influxd[9666]: created by github.com/influxdata/influxdb/services/collectd.(*Service).Open
mai 14 03:15:50 raspberrypi influxd[9666]:         github.com/influxdata/influxdb/services/collectd/service.go:208 +0x528
mai 14 03:15:50 raspberrypi influxd[9666]: goroutine 95 [select]:
mai 14 03:15:50 raspberrypi influxd[9666]: github.com/influxdata/influxdb/tsdb/engine/tsm1.(*Engine).compact(0x314e000, 0x5576d20)
mai 14 03:15:50 raspberrypi influxd[9666]:         github.com/influxdata/influxdb/tsdb/engine/tsm1/engine.go:1949 +0x1ac
mai 14 03:15:50 raspberrypi influxd[9666]: github.com/influxdata/influxdb/tsdb/engine/tsm1.(*Engine).enableLevelCompactions.func1(0x5576d20, 0x314e000)
mai 14 03:15:50 raspberrypi influxd[9666]:         github.com/influxdata/influxdb/tsdb/engine/tsm1/engine.go:407 +0x48
mai 14 03:15:50 raspberrypi influxd[9666]: created by github.com/influxdata/influxdb/tsdb/engine/tsm1.(*Engine).enableLevelCompactions
mai 14 03:15:50 raspberrypi influxd[9666]:         github.com/influxdata/influxdb/tsdb/engine/tsm1/engine.go:407 +0x114
mai 14 03:15:50 raspberrypi influxd[9666]: goroutine 94 [select]:
mai 14 03:15:50 raspberrypi influxd[9666]: github.com/influxdata/influxdb/tsdb/engine/tsm1.(*Engine).compactCache(0x314e000)
mai 14 03:15:50 raspberrypi influxd[9666]:         github.com/influxdata/influxdb/tsdb/engine/tsm1/engine.go:1902 +0xd8
mai 14 03:15:50 raspberrypi influxd[9666]: github.com/influxdata/influxdb/tsdb/engine/tsm1.(*Engine).enableSnapshotCompactions.func1(0x5576d10, 0x314e000)
mai 14 03:15:50 raspberrypi influxd[9666]:         github.com/influxdata/influxdb/tsdb/engine/tsm1/engine.go:486 +0x40
mai 14 03:15:50 raspberrypi influxd[9666]: created by github.com/influxdata/influxdb/tsdb/engine/tsm1.(*Engine).enableSnapshotCompactions
mai 14 03:15:50 raspberrypi influxd[9666]:         github.com/influxdata/influxdb/tsdb/engine/tsm1/engine.go:486 +0xfc
mai 14 03:15:50 raspberrypi influxd[9666]: goroutine 97 [select]:
mai 14 03:15:50 raspberrypi influxd[9666]: github.com/influxdata/influxdb/tsdb/engine/tsm1.(*Engine).compact(0x200e000, 0x5576d40)
mai 14 03:15:50 raspberrypi influxd[9666]:         github.com/influxdata/influxdb/tsdb/engine/tsm1/engine.go:1949 +0x1ac
mai 14 03:15:50 raspberrypi influxd[9666]: github.com/influxdata/influxdb/tsdb/engine/tsm1.(*Engine).enableLevelCompactions.func1(0x5576d40, 0x200e000)
mai 14 03:15:50 raspberrypi influxd[9666]:         github.com/influxdata/influxdb/tsdb/engine/tsm1/engine.go:407 +0x48
mai 14 03:15:50 raspberrypi influxd[9666]: created by github.com/influxdata/influxdb/tsdb/engine/tsm1.(*Engine).enableLevelCompactions
mai 14 03:15:50 raspberrypi influxd[9666]:         github.com/influxdata/influxdb/tsdb/engine/tsm1/engine.go:407 +0x114
mai 14 03:15:50 raspberrypi influxd[9666]: goroutine 96 [select]:
mai 14 03:15:50 raspberrypi influxd[9666]: github.com/influxdata/influxdb/tsdb/engine/tsm1.(*Engine).compactCache(0x200e000)
mai 14 03:15:50 raspberrypi influxd[9666]:         github.com/influxdata/influxdb/tsdb/engine/tsm1/engine.go:1902 +0xd8
mai 14 03:15:50 raspberrypi influxd[9666]: github.com/influxdata/influxdb/tsdb/engine/tsm1.(*Engine).enableSnapshotCompactions.func1(0x5576d30, 0x200e000)
mai 14 03:15:50 raspberrypi influxd[9666]:         github.com/influxdata/influxdb/tsdb/engine/tsm1/engine.go:486 +0x40
mai 14 03:15:50 raspberrypi influxd[9666]: created by github.com/influxdata/influxdb/tsdb/engine/tsm1.(*Engine).enableSnapshotCompactions
mai 14 03:15:50 raspberrypi influxd[9666]:         github.com/influxdata/influxdb/tsdb/engine/tsm1/engine.go:486 +0xfc
mai 14 03:15:50 raspberrypi influxd[9666]: goroutine 120 [select]:
mai 14 03:15:50 raspberrypi influxd[9666]: github.com/influxdata/influxdb/cmd/influxd/run.(*Command).monitorServerErrors(0x18a8cc0)
mai 14 03:15:50 raspberrypi influxd[9666]:         github.com/influxdata/influxdb/cmd/influxd/run/command.go:174 +0x134
mai 14 03:15:50 raspberrypi influxd[9666]: created by github.com/influxdata/influxdb/cmd/influxd/run.(*Command).Run
mai 14 03:15:50 raspberrypi influxd[9666]:         github.com/influxdata/influxdb/cmd/influxd/run/command.go:155 +0x920
mai 14 03:15:50 raspberrypi influxd[9666]: goroutine 119 [select]:
mai 14 03:15:50 raspberrypi influxd[9666]: github.com/influxdata/influxdb/coordinator.(*PointsWriter).WritePointsPrivileged(0x18a5310, 0x188ce80, 0x8, 0x19ed1e0, 0x7, 0x0, 0xe698000, 0x97, 0x1388, 0x0, ...)
mai 14 03:15:50 raspberrypi influxd[9666]:         github.com/influxdata/influxdb/coordinator/points_writer.go:350 +0x530
mai 14 03:15:50 raspberrypi influxd[9666]: github.com/influxdata/influxdb/services/collectd.(*Service).writePoints(0x19f8150)
mai 14 03:15:50 raspberrypi influxd[9666]:         github.com/influxdata/influxdb/services/collectd/service.go:403 +0x2e0
mai 14 03:15:50 raspberrypi influxd[9666]: github.com/influxdata/influxdb/services/collectd.(*Service).Open.func3(0x19f8150)
mai 14 03:15:50 raspberrypi influxd[9666]:         github.com/influxdata/influxdb/services/collectd/service.go:209 +0x48
mai 14 03:15:50 raspberrypi influxd[9666]: created by github.com/influxdata/influxdb/services/collectd.(*Service).Open
mai 14 03:15:50 raspberrypi influxd[9666]:         github.com/influxdata/influxdb/services/collectd/service.go:209 +0x544
mai 14 03:15:50 raspberrypi influxd[9666]: goroutine 116 [select]:
mai 14 03:15:50 raspberrypi influxd[9666]: github.com/influxdata/influxdb/services/retention.(*Service).run(0x18975f0)
mai 14 03:15:50 raspberrypi influxd[9666]:         github.com/influxdata/influxdb/services/retention/service.go:78 +0x8c0
mai 14 03:15:50 raspberrypi influxd[9666]: github.com/influxdata/influxdb/services/retention.(*Service).Open.func1(0x18975f0)
mai 14 03:15:50 raspberrypi influxd[9666]:         github.com/influxdata/influxdb/services/retention/service.go:51 +0x48
mai 14 03:15:50 raspberrypi influxd[9666]: created by github.com/influxdata/influxdb/services/retention.(*Service).Open
mai 14 03:15:50 raspberrypi influxd[9666]:         github.com/influxdata/influxdb/services/retention/service.go:51 +0x130
mai 14 03:15:50 raspberrypi influxd[9666]: goroutine 115 [IO wait, 1 minutes]:
mai 14 03:15:50 raspberrypi influxd[9666]: internal/poll.runtime_pollWait(0x647c2e40, 0x72, 0x0)
mai 14 03:15:50 raspberrypi influxd[9666]:         /usr/lib/go-1.11/src/runtime/netpoll.go:173 +0x44
mai 14 03:15:50 raspberrypi influxd[9666]: internal/poll.(*pollDesc).wait(0x193c014, 0x72, 0x3162100, 0x0, 0x0)
mai 14 03:15:50 raspberrypi influxd[9666]:         /usr/lib/go-1.11/src/internal/poll/fd_poll_runtime.go:85 +0x7c
mai 14 03:15:50 raspberrypi influxd[9666]: internal/poll.(*pollDesc).waitRead(0x193c014, 0xffffff00, 0x0, 0x0)
mai 14 03:15:50 raspberrypi influxd[9666]:         /usr/lib/go-1.11/src/internal/poll/fd_poll_runtime.go:90 +0x2c
mai 14 03:15:50 raspberrypi influxd[9666]: internal/poll.(*FD).Accept(0x193c000, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0)
mai 14 03:15:50 raspberrypi influxd[9666]:         /usr/lib/go-1.11/src/internal/poll/fd_unix.go:384 +0x17c
mai 14 03:15:50 raspberrypi influxd[9666]: net.(*netFD).accept(0x193c000, 0x0, 0x1c028c0, 0x192c000)
mai 14 03:15:50 raspberrypi influxd[9666]:         /usr/lib/go-1.11/src/net/fd_unix.go:238 +0x20
mai 14 03:15:50 raspberrypi influxd[9666]: net.(*TCPListener).accept(0x664f890, 0x2e245e0, 0x1a0338, 0x20)
mai 14 03:15:50 raspberrypi influxd[9666]:         /usr/lib/go-1.11/src/net/tcpsock_posix.go:139 +0x20
mai 14 03:15:50 raspberrypi influxd[9666]: net.(*TCPListener).Accept(0x664f890, 0x870820, 0x31621c0, 0x7fcc40, 0xfb26f8)
mai 14 03:15:50 raspberrypi influxd[9666]:         /usr/lib/go-1.11/src/net/tcpsock.go:260 +0x3c
mai 14 03:15:50 raspberrypi influxd[9666]: net/http.(*Server).Serve(0x2e24580, 0x9f0968, 0x664f890, 0x0, 0x0)
mai 14 03:15:50 raspberrypi influxd[9666]:         /usr/lib/go-1.11/src/net/http/server.go:2826 +0x1e0
mai 14 03:15:50 raspberrypi influxd[9666]: net/http.Serve(0x9f0968, 0x664f890, 0x9ea260, 0x19f80e0, 0x70797272, 0x6e692c69)
mai 14 03:15:50 raspberrypi influxd[9666]:         /usr/lib/go-1.11/src/net/http/server.go:2423 +0x58
mai 14 03:15:50 raspberrypi influxd[9666]: github.com/influxdata/influxdb/services/httpd.(*Service).serve(0x18a90e0, 0x9f0968, 0x664f890)
mai 14 03:15:50 raspberrypi influxd[9666]:         github.com/influxdata/influxdb/services/httpd/service.go:249 +0x38
mai 14 03:15:50 raspberrypi influxd[9666]: github.com/influxdata/influxdb/services/httpd.(*Service).serveTCP(0x18a90e0)
mai 14 03:15:50 raspberrypi influxd[9666]:         github.com/influxdata/influxdb/services/httpd/service.go:237 +0x2c
mai 14 03:15:50 raspberrypi influxd[9666]: created by github.com/influxdata/influxdb/services/httpd.(*Service).Open
mai 14 03:15:50 raspberrypi influxd[9666]:         github.com/influxdata/influxdb/services/httpd/service.go:185 +0x474
mai 14 03:15:50 raspberrypi influxd[9666]: goroutine 93 [select]:
mai 14 03:15:50 raspberrypi influxd[9666]: github.com/influxdata/influxdb/tsdb/engine/tsm1.(*Engine).compact(0x19760b0, 0x5576d00)
mai 14 03:15:50 raspberrypi influxd[9666]:         github.com/influxdata/influxdb/tsdb/engine/tsm1/engine.go:1949 +0x1ac
mai 14 03:15:50 raspberrypi influxd[9666]: github.com/influxdata/influxdb/tsdb/engine/tsm1.(*Engine).enableLevelCompactions.func1(0x5576d00, 0x19760b0)
mai 14 03:15:51 raspberrypi influxd[9666]:         github.com/influxdata/influxdb/tsdb/engine/tsm1/engine.go:407 +0x48
mai 14 03:15:51 raspberrypi influxd[9666]: created by github.com/influxdata/influxdb/tsdb/engine/tsm1.(*Engine).enableLevelCompactions
mai 14 03:15:51 raspberrypi influxd[9666]:         github.com/influxdata/influxdb/tsdb/engine/tsm1/engine.go:407 +0x114
mai 14 03:15:51 raspberrypi influxd[9666]: goroutine 92 [select]:
mai 14 03:15:51 raspberrypi influxd[9666]: github.com/influxdata/influxdb/tsdb/engine/tsm1.(*Engine).compactCache(0x19760b0)
mai 14 03:15:51 raspberrypi influxd[9666]:         github.com/influxdata/influxdb/tsdb/engine/tsm1/engine.go:1902 +0xd8
mai 14 03:15:51 raspberrypi influxd[9666]: github.com/influxdata/influxdb/tsdb/engine/tsm1.(*Engine).enableSnapshotCompactions.func1(0x5576cf0, 0x19760b0)
mai 14 03:15:51 raspberrypi influxd[9666]:         github.com/influxdata/influxdb/tsdb/engine/tsm1/engine.go:486 +0x40
mai 14 03:15:51 raspberrypi influxd[9666]: created by github.com/influxdata/influxdb/tsdb/engine/tsm1.(*Engine).enableSnapshotCompactions
mai 14 03:15:51 raspberrypi influxd[9666]:         github.com/influxdata/influxdb/tsdb/engine/tsm1/engine.go:486 +0xfc
mai 14 03:15:51 raspberrypi influxd[9666]: goroutine 130 [select]:
mai 14 03:15:51 raspberrypi influxd[9666]: github.com/influxdata/influxdb/tsdb/engine/tsm1.(*Engine).compactCache(0x26d6000)
mai 14 03:15:51 raspberrypi influxd[9666]:         github.com/influxdata/influxdb/tsdb/engine/tsm1/engine.go:1902 +0xd8
mai 14 03:15:51 raspberrypi influxd[9666]: github.com/influxdata/influxdb/tsdb/engine/tsm1.(*Engine).enableSnapshotCompactions.func1(0x5576d50, 0x26d6000)
mai 14 03:15:51 raspberrypi influxd[9666]:         github.com/influxdata/influxdb/tsdb/engine/tsm1/engine.go:486 +0x40
mai 14 03:15:51 raspberrypi influxd[9666]: created by github.com/influxdata/influxdb/tsdb/engine/tsm1.(*Engine).enableSnapshotCompactions
mai 14 03:15:51 raspberrypi influxd[9666]:         github.com/influxdata/influxdb/tsdb/engine/tsm1/engine.go:486 +0xfc
mai 14 03:15:51 raspberrypi influxd[9666]: goroutine 131 [select]:
mai 14 03:15:51 raspberrypi influxd[9666]: github.com/influxdata/influxdb/tsdb/engine/tsm1.(*Engine).compact(0x26d6000, 0x5576d60)
mai 14 03:15:51 raspberrypi influxd[9666]:         github.com/influxdata/influxdb/tsdb/engine/tsm1/engine.go:1949 +0x1ac
mai 14 03:15:51 raspberrypi influxd[9666]: github.com/influxdata/influxdb/tsdb/engine/tsm1.(*Engine).enableLevelCompactions.func1(0x5576d60, 0x26d6000)
mai 14 03:15:51 raspberrypi influxd[9666]:         github.com/influxdata/influxdb/tsdb/engine/tsm1/engine.go:407 +0x48
mai 14 03:15:51 raspberrypi influxd[9666]: created by github.com/influxdata/influxdb/tsdb/engine/tsm1.(*Engine).enableLevelCompactions
mai 14 03:15:51 raspberrypi influxd[9666]:         github.com/influxdata/influxdb/tsdb/engine/tsm1/engine.go:407 +0x114
mai 14 03:15:51 raspberrypi influxd[9666]: goroutine 132 [select]:
mai 14 03:15:51 raspberrypi influxd[9666]: github.com/influxdata/influxdb/tsdb.(*Store).monitorShards(0x18a7200)
mai 14 03:15:51 raspberrypi influxd[9666]:         github.com/influxdata/influxdb/tsdb/store.go:1776 +0x12c
mai 14 03:15:51 raspberrypi influxd[9666]: created by github.com/influxdata/influxdb/tsdb.(*Store).Open
mai 14 03:15:51 raspberrypi influxd[9666]:         github.com/influxdata/influxdb/tsdb/store.go:197 +0x20c
mai 14 03:15:51 raspberrypi influxd[9666]: goroutine 133 [select]:
mai 14 03:15:51 raspberrypi influxd[9666]: github.com/influxdata/influxdb/services/subscriber.(*Service).run(0x18c6900)
mai 14 03:15:51 raspberrypi influxd[9666]:         github.com/influxdata/influxdb/services/subscriber/service.go:239 +0x1d8
mai 14 03:15:51 raspberrypi influxd[9666]: github.com/influxdata/influxdb/services/subscriber.(*Service).Open.func1(0x18c6900)
mai 14 03:15:51 raspberrypi influxd[9666]:         github.com/influxdata/influxdb/services/subscriber/service.go:98 +0x48
mai 14 03:15:51 raspberrypi influxd[9666]: created by github.com/influxdata/influxdb/services/subscriber.(*Service).Open
mai 14 03:15:51 raspberrypi influxd[9666]:         github.com/influxdata/influxdb/services/subscriber/service.go:96 +0x13c
mai 14 03:15:51 raspberrypi influxd[9666]: goroutine 134 [select, 1 minutes]:
mai 14 03:15:51 raspberrypi influxd[9666]: github.com/influxdata/influxdb/services/subscriber.(*Service).waitForMetaUpdates(0x18c6900)
mai 14 03:15:51 raspberrypi influxd[9666]:         github.com/influxdata/influxdb/services/subscriber/service.go:165 +0x8c
mai 14 03:15:51 raspberrypi influxd[9666]: github.com/influxdata/influxdb/services/subscriber.(*Service).Open.func2(0x18c6900)
mai 14 03:15:51 raspberrypi influxd[9666]:         github.com/influxdata/influxdb/services/subscriber/service.go:102 +0x48
mai 14 03:15:51 raspberrypi influxd[9666]: created by github.com/influxdata/influxdb/services/subscriber.(*Service).Open
mai 14 03:15:51 raspberrypi influxd[9666]:         github.com/influxdata/influxdb/services/subscriber/service.go:100 +0x158
mai 14 03:15:51 raspberrypi influxd[9666]: goroutine 135 [runnable]:
mai 14 03:15:51 raspberrypi influxd[9666]: github.com/influxdata/influxdb/pkg/estimator/hll.(*Plus).Clone(0x3fc4b40, 0x8faf20, 0x2e24200)
mai 14 03:15:51 raspberrypi influxd[9666]:         github.com/influxdata/influxdb/pkg/estimator/hll/hll.go:141 +0xd8
mai 14 03:15:51 raspberrypi influxd[9666]: github.com/influxdata/influxdb/tsdb/index/tsi1.(*Index).MeasurementsSketches(0x2e24200, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0)
mai 14 03:15:51 raspberrypi influxd[9666]:         github.com/influxdata/influxdb/tsdb/index/tsi1/index.go:751 +0x7c
mai 14 03:15:51 raspberrypi influxd[9666]: github.com/influxdata/influxdb/tsdb/engine/tsm1.(*Engine).MeasurementsSketches(0x427ad10, 0x9f7b78, 0x427ad10, 0x0, 0x0, 0x8294c, 0x18a7210)
mai 14 03:15:51 raspberrypi influxd[9666]:         github.com/influxdata/influxdb/tsdb/engine/tsm1/engine.go:592 +0x28
mai 14 03:15:51 raspberrypi influxd[9666]: github.com/influxdata/influxdb/tsdb.(*Shard).MeasurementsSketches(0x18e88c0, 0x1, 0x0, 0x4cfaa0, 0x18a7210, 0xffffffff, 0x0)
mai 14 03:15:51 raspberrypi influxd[9666]:         github.com/influxdata/influxdb/tsdb/shard.go:751 +0x64
mai 14 03:15:51 raspberrypi influxd[9666]: github.com/influxdata/influxdb/tsdb.(*Store).MeasurementsCardinality.func1(0x18e88c0, 0x1932800, 0x3ca16e0, 0x8, 0x8, 0x4beed0, 0x0)
mai 14 03:15:51 raspberrypi influxd[9666]:         github.com/influxdata/influxdb/tsdb/store.go:1076 +0x88
mai 14 03:15:51 raspberrypi influxd[9666]: github.com/influxdata/influxdb/tsdb.(*Store).sketchesForDatabase(0x18a7200, 0x188ad34, 0x9, 0x8fa474, 0x1932330, 0x0, 0x0, 0x8, 0x8, 0x1827cc0)
mai 14 03:15:51 raspberrypi influxd[9666]:         github.com/influxdata/influxdb/tsdb/store.go:1003 +0xd0
mai 14 03:15:51 raspberrypi influxd[9666]: github.com/influxdata/influxdb/tsdb.(*Store).MeasurementsCardinality(0x18a7200, 0x188ad34, 0x9, 0xaf0, 0x0, 0x0, 0x0)
mai 14 03:15:51 raspberrypi influxd[9666]:         github.com/influxdata/influxdb/tsdb/store.go:1072 +0x34
mai 14 03:15:51 raspberrypi influxd[9666]: github.com/influxdata/influxdb/tsdb.(*Store).Statistics(0x18a7200, 0x19ccae0, 0x1, 0x1, 0x64a5b90)
mai 14 03:15:51 raspberrypi influxd[9666]:         github.com/influxdata/influxdb/tsdb/store.go:116 +0x2fc
mai 14 03:15:51 raspberrypi influxd[9666]: github.com/influxdata/influxdb/cmd/influxd/run.(*Server).Statistics(0x1889680, 0x19ccae0, 0x188973c, 0x8c95b6, 0xd6516d61)
mai 14 03:15:51 raspberrypi influxd[9666]:         github.com/influxdata/influxdb/cmd/influxd/run/server.go:237 +0x94
mai 14 03:15:51 raspberrypi influxd[9666]: github.com/influxdata/influxdb/monitor.(*Monitor).gatherStatistics(0x1889710, 0x3e620c8, 0x1, 0x2, 0x19ccae0, 0x0, 0x0, 0x0)
mai 14 03:15:51 raspberrypi influxd[9666]:         github.com/influxdata/influxdb/monitor/service.go:352 +0x88
mai 14 03:15:51 raspberrypi influxd[9666]: github.com/influxdata/influxdb/monitor.(*Monitor).Statistics(0x1889710, 0x19ccae0, 0xdbf22d3c, 0xe, 0xfbcf70, 0x540be400, 0x2)
mai 14 03:15:51 raspberrypi influxd[9666]:         github.com/influxdata/influxdb/monitor/service.go:343 +0xb54
mai 14 03:15:51 raspberrypi influxd[9666]: github.com/influxdata/influxdb/monitor.(*Monitor).storeStatistics(0x1889710)
mai 14 03:15:51 raspberrypi influxd[9666]:         github.com/influxdata/influxdb/monitor/service.go:441 +0x61c
mai 14 03:15:51 raspberrypi influxd[9666]: created by github.com/influxdata/influxdb/monitor.(*Monitor).Open
mai 14 03:15:51 raspberrypi influxd[9666]:         github.com/influxdata/influxdb/monitor/service.go:126 +0x2b8
mai 14 03:15:51 raspberrypi influxd[9666]: goroutine 136 [select, 1 minutes]:
mai 14 03:15:51 raspberrypi influxd[9666]: github.com/influxdata/influxdb/services/precreator.(*Service).runPrecreation(0x1897260)
mai 14 03:15:51 raspberrypi influxd[9666]:         github.com/influxdata/influxdb/services/precreator/service.go:76 +0xc8
mai 14 03:15:51 raspberrypi influxd[9666]: created by github.com/influxdata/influxdb/services/precreator.(*Service).Open
mai 14 03:15:51 raspberrypi influxd[9666]:         github.com/influxdata/influxdb/services/precreator/service.go:54 +0x164
mai 14 03:15:51 raspberrypi influxd[9666]: goroutine 137 [select, 1 minutes]:
mai 14 03:15:51 raspberrypi influxd[9666]: github.com/influxdata/influxdb/tcp.(*listener).Accept(0x1897650, 0x0, 0x0, 0x0, 0x0)
mai 14 03:15:51 raspberrypi influxd[9666]:         github.com/influxdata/influxdb/tcp/mux.go:236 +0xd8
mai 14 03:15:51 raspberrypi influxd[9666]: github.com/influxdata/influxdb/services/snapshotter.(*Service).serve(0x18972c0)
mai 14 03:15:51 raspberrypi influxd[9666]:         github.com/influxdata/influxdb/services/snapshotter/service.go:94 +0x54
mai 14 03:15:51 raspberrypi influxd[9666]: created by github.com/influxdata/influxdb/services/snapshotter.(*Service).Open
mai 14 03:15:51 raspberrypi influxd[9666]:         github.com/influxdata/influxdb/services/snapshotter/service.go:68 +0x74
mai 14 03:15:51 raspberrypi influxd[9666]: goroutine 138 [select]:
mai 14 03:15:51 raspberrypi influxd[9666]: github.com/influxdata/influxdb/services/continuous_querier.(*Service).backgroundLoop(0x18a9080)
mai 14 03:15:51 raspberrypi influxd[9666]:         github.com/influxdata/influxdb/services/continuous_querier/service.go:215 +0x114
mai 14 03:15:51 raspberrypi influxd[9666]: created by github.com/influxdata/influxdb/services/continuous_querier.(*Service).Open
mai 14 03:15:51 raspberrypi influxd[9666]:         github.com/influxdata/influxdb/services/continuous_querier/service.go:133 +0x154
mai 14 03:15:51 raspberrypi influxd[9666]: goroutine 543 [runnable]:
mai 14 03:15:51 raspberrypi influxd[9666]: github.com/influxdata/influxdb/tsdb/engine/tsm1.(*ring).write(0x4198980, 0x3453d10, 0x41, 0x50, 0x5a339c0, 0x1, 0x1, 0x1c344, 0x12c64, 0x24114)
mai 14 03:15:51 raspberrypi influxd[9666]:         github.com/influxdata/influxdb/tsdb/engine/tsm1/ring.go:99 +0x94
mai 14 03:15:51 raspberrypi influxd[9666]: github.com/influxdata/influxdb/tsdb/engine/tsm1.(*Cache).WriteMulti(0x1ad0580, 0x22e7e40, 0x26d6000, 0x4f)
mai 14 03:15:51 raspberrypi influxd[9666]:         github.com/influxdata/influxdb/tsdb/engine/tsm1/cache.go:343 +0x2b0
mai 14 03:15:51 raspberrypi influxd[9666]: github.com/influxdata/influxdb/tsdb/engine/tsm1.(*Engine).WritePoints(0x26d6000, 0x18f0500, 0x97, 0x97, 0x0, 0x0)
mai 14 03:15:51 raspberrypi influxd[9666]:         github.com/influxdata/influxdb/tsdb/engine/tsm1/engine.go:1336 +0xbc0
mai 14 03:15:51 raspberrypi influxd[9666]: github.com/influxdata/influxdb/tsdb.(*Shard).WritePoints(0x1ad2280, 0x18f0500, 0x97, 0x97, 0x0, 0x0)
mai 14 03:15:51 raspberrypi influxd[9666]:         github.com/influxdata/influxdb/tsdb/shard.go:511 +0x1c0
mai 14 03:15:51 raspberrypi influxd[9666]: github.com/influxdata/influxdb/tsdb.(*Store).WriteToShard(0x18a7200, 0x2c1, 0x0, 0x18f0500, 0x97, 0x97, 0x4, 0x0)
mai 14 03:15:51 raspberrypi influxd[9666]:         github.com/influxdata/influxdb/tsdb/store.go:1297 +0xc8
mai 14 03:15:51 raspberrypi influxd[9666]: github.com/influxdata/influxdb/coordinator.(*PointsWriter).writeToShard(0x18a5310, 0x2cbf3a0, 0x188ce80, 0x8, 0x19ed1e0, 0x7, 0x18f0500, 0x97, 0x97, 0x5d3801, ...)
mai 14 03:15:51 raspberrypi influxd[9666]:         github.com/influxdata/influxdb/coordinator/points_writer.go:370 +0x84
mai 14 03:15:51 raspberrypi influxd[9666]: github.com/influxdata/influxdb/coordinator.(*PointsWriter).WritePointsPrivileged.func1(0x18a5310, 0x1827280, 0x2cbf3a0, 0x188ce80, 0x8, 0x19ed1e0, 0x7, 0x18f0500, 0x97, 0x97)
mai 14 03:15:51 raspberrypi influxd[9666]:         github.com/influxdata/influxdb/coordinator/points_writer.go:312 +0x5c
mai 14 03:15:51 raspberrypi influxd[9666]: created by github.com/influxdata/influxdb/coordinator.(*PointsWriter).WritePointsPrivileged
mai 14 03:15:51 raspberrypi influxd[9666]:         github.com/influxdata/influxdb/coordinator/points_writer.go:311 +0x208
mai 14 03:15:52 raspberrypi systemd[1]: influxdb.service: Main process exited, code=exited, status=2/INVALIDARGUMENT
mai 14 03:15:52 raspberrypi systemd[1]: influxdb.service: Failed with result 'exit-code'.
    #+END_PLAIN
  - Logs complets disponibles
    - InfluxDB : [[file:logs/2024-02-21-crash-influxdb.log]]
    - MainApp : [[file:logs/2024-02-21-crash-main-app.log]]
  - Voir si modifier la taille des shards permet de résoudre le problème
- SmartSense: Identifier les paramètres de configuration d'InfluxDB pertinents pour la compaction
  - max-concurrent-compactions
    - https://docs.influxdata.com/influxdb/v1/administration/config/#max-concurrent-compactions--0
    - Par défaut, utilise jusqu'à 50% du CPU lors d'une compaction
    - Limiter à 1 ?
  - compact-throughput
    - https://docs.influxdata.com/influxdb/v1/administration/config/#compact-throughput--48m
    - Quantité de données écrites sur le disk par seconde lors d'une compaction
    - Par défaut, 48m = 48Mo/s
    - Me paraît raisonnable
  - compact-throughput-burst
    - https://docs.influxdata.com/influxdb/v1/administration/config/#compact-throughput-burst--48m
    - Pas sûr de comprendre la diff avec l'option précédente
    - Par défaut, 48m = 48Mo/s
  - max-index-log-file-size
    - https://docs.influxdata.com/influxdb/v1/administration/config/#max-index-log-file-size--1m
    - A l'air de correspondre à la taille limite du WAL afin qu'il soit clos, compacté et intégré à l'index
    - Indique que diminuer cette taille limite entrainera des compactions moins coûteuses mais plus fréquentes, et impactant le débit en écriture
    - Mais par défaut, 1m = 1Mo
    - Me paraît pas bien grand
    - Je suppose que c'est pas cette partie là du mécanisme de compression qui impacte et fait crasher influxdb
  - Aparté
    - Alors pas pertinent par rapport au problème actuel
    - Mais serait intéressant de vérifier les paramètres vis-à-vis de la rétention de données
    - retention.enabled
      - https://docs.influxdata.com/influxdb/v1/administration/config/#enabled--true
      - Par défaut activé
    - meta.retention_autocreate
      - https://docs.influxdata.com/influxdb/v1/administration/config/#retention-autocreate--true
      - Par défaut activé
      - Créé une *retention policy* autogen qui
        - Conserve indéfiniment les données
        - Replication factor de 1
        - Et une *shard group duration* de 7j
      - avec *shard*
        - Fichier TSM qui contient les données d'un ensemble donné de *serie* sur un interval de temps donné
        - avec *serie*, ensemble de *points* avec le même *measurement*, les mêmes *tags*, et les mêmes *field keys*
      - *shard group*
        - Ensemble de *shards* sur un interval de temps donné
      - et donc *shard group duration*
        - Interval de temps représenté/géré par un *shard group*
        - Tous les points de cette période seront donc stockés au sein de ce *shard group*
  - Donc finalement, la *retention policy* est p-e un paramètre sur lequel on peut jouer
    - En diminuant la *shard group duration*, par défaut 1 semaine, à une granularité plus fine, e.g. 4h
      - Sachant que la valeur minimale est 1h
    - Forcerait la création de nouveaux *shards* régulièrement
    - Donc *shards* plus petits
    - Et permettrait p-e de lisser les compactions
    - Notamment en combinaison de data.compact-full-write-cold-duration
      - https://docs.influxdata.com/influxdb/v1/administration/config/#compact-full-write-cold-duration--4h
      - Qui déclenche une compaction sur un shard non-modifié depuis 4h
      - On pourrait donc "forcer" la création et la compaction régulière de nouveaux shards
    - Reste à voir l'impact de la multiplication des shards
    - Tombe sur ces recommandations : https://docs.influxdata.com/influxdb/v1/concepts/schema_and_data_layout/#shard-group-duration-recommendations
      - 100k points par shard group
      - 1000 points par série
    - Dans notre cas, on a une 30aine de séries régulières
    - Nous suffit donc "que" de 3k points par série pour atteindre ces quotas
    - Voir la fréquence d'échantillonage, mais je suppose qu'on fait plusieurs mesures par seconde
  - Suis tombé sinon sur https://docs.influxdata.com/influxdb/v1/concepts/tsi-details/#important-compaction-configuration-settings
    - Ne met rien de nouveau en lumière
    - Le commentaire suivant n'est pas rassurant
      #+BEGIN_QUOTE
These configuration settings are especially beneficial for systems with irregular loads,
limiting compactions during periods of high usage, and letting compactions catch up during periods of lower load.
In systems with stable loads, if compactions interfere with other operations, typically,
the system is undersized for its load, and configuration changes won’t help much.
      #+END_QUOTE
  - Conclusion
    - Set max-concurrent-compactions à 1 pour déterminer si cela suffit
    - Sinon
      - Modifier la retention policy autogen
        - ALTER RETENTION POLICY "autogen" ON "Sensor Data" SHARD DURATION 4h
        - ALTER RETENTION POLICY "autogen" ON "collectd" SHARD DURATION 4h
      - Et set compact-full-write-cold-duration à 4h
- SmartSense: Étudier la migration vers une nouvelle version d'InfluxDB
  - Sans monter jusqu'à la v2, serait p-e intéressant de voir les modifications apportées par les versions suivantes
  - CHANGELOG récupéré
    - Via le lien : https://github.com/influxdata/influxdb/blob/v1.10.0/CHANGELOG.md
  - A trouvé une page dédiée aux release notes
    - https://docs.influxdata.com/influxdb/v1/about_the_project/release-notes/
  - 1.8.5
    - Allow disable compaction per shard
      - https://github.com/influxdata/influxdb/commit/86118d2
      - Nécessite la création d'un fichier /do_not_compact/ dans le dossier de chaque shard
      - Peut retrouver l'issue et la PR correspondante ici : https://github.com/influxdata/influxdb/issues/19761
  - 1.7.10
    - Fix compaction logic on infrequent cache snapshots which resulted in frequent full compactions rather than level compactions.
    - During compactions, skip TSM files with block read errors from previous compactions.
  - 1.7.9
    - Guard against compaction burst throughput limit.
  - 1.7.5 À ÉVITER
    - Plante inopinément si utilise le default in-memory index
  - 1.7.3 À ÉVITER
    - Data loss lors de la compaction
    - Introduit limit force-full and cold compaction size : https://github.com/influxdata/influxdb/pull/10536
    - Mais cette modification semble revert un peu plus tard
  - 1.7.0
    - The `ifql` section in the configuration has been changed to `flux` and it is enabled by default on port `:8082`.
    - Compaction performance improvements for Time Series Index (TSI).
  - Conclusion
    - Concernant les versions 1.x, la 1.7.10 a l'air intéressante
    - Mais voit pas de raisons de ne pas upgrade à la plus récente, la 1.8.10
    - Du mal par contre à voir comment on pourrait utiliser la fonctionnalité de désactivation de la compaction des shards
    - Nécessite la création d'un fichier /do_not_compact/ dans le dossier de chaque shard
- SmartSense: Tester les différentes configurations d'InfluxDB
  - Me retrouve avec une instance d'InfluxDB "empoisonnée"
    - Crash au démarrage
  - Doit déjà remettre ça au propre
    - [X] Exporter l'état de la DB
    - [X] Recréer l'utilisateur admin
      - CREATE USER admin WITH PASSWORD '<password>' WITH ALL PRIVILEGES
    - [X] Modifier la retention policy autogen
      - ALTER RETENTION POLICY autogen ON collectd SHARD DURATION 4h
      - Mais SensorData n'ayant pas encore été créée, elle n'a pas de RETENTION POLICY
    - [X] Vérifier la configuration de compact-full-write-code-duration
      - Bien à 4h
  - J'ai relancé avec cette nouvelle config
  - En vérifiant, la RETENTION POLICY définie automatiquement pour SensorData, me suis aperçu que c'était celle par DEFAULT
    - ALTER RETENTION POLICY autogen ON SensorData SHARD DURATION 4h
  - Afin de préparer le terrain pour la solution où on désactive totalement le mécanisme de compression des shards
  - Serait bien d'identifier où sont stocker les shards exactement
  - Suppose que c'est dans /data/influxdb/SensorData/autogen
    - Où chaque dossier correspond à un shard IMO
    - Vu la présence de fichiers .tsm
    - Pas trop sûr de ce qui est stocké dans /data/influxdb/SensorData/_series
    - Vu l'arborescence dans la v2
      - Expliquée ici : https://docs.influxdata.com/influxdb/v2/reference/internals/file-system-layout/#tsm-directories-and-files-layout
    - Pas sûr de comprendre où sont stockées les données
      - Je ne vois que des index
  - Ah ben finalement, j'ai remarqué que les shards ne sont pas modifiés à la volée lors de l'altération de la /retention policy/
    - À l'aide de SHOW SHARDS
  - Besoin de supprimer et recréer la DB du coup
    - DROP DATABASE SensorData; CREATE DATABASE SensorData WITH SHARD DURATION 4h NAME short_lived
  - Cette fois-ci, c'est bon
  - On va faire le test avec cette config
    - Donc sans avoir modifié collectd
    - À voir comment ça se comporte
  - A bien un nouveau shard qui s'est créé pour la table SensorData
    #+BEGIN_PLAIN
> SHOW SHARDS
name: _internal
id database  retention_policy shard_group start_time           end_time             expiry_time          owners
-- --------  ---------------- ----------- ----------           --------             -----------          ------
1  _internal monitor          1           2023-05-14T00:00:00Z 2023-05-15T00:00:00Z 2023-05-22T00:00:00Z

name: collectd
id database retention_policy shard_group start_time           end_time             expiry_time          owners
-- -------- ---------------- ----------- ----------           --------             -----------          ------
2  collectd autogen          2           2023-05-08T00:00:00Z 2023-05-15T00:00:00Z 2023-05-15T00:00:00Z

name: SensorData
id database   retention_policy shard_group start_time           end_time             expiry_time          owners
-- --------   ---------------- ----------- ----------           --------             -----------          ------
4  SensorData short_lived      4           2023-05-14T04:00:00Z 2023-05-14T08:00:00Z 2023-05-14T08:00:00Z
5  SensorData short_lived      5           2023-05-14T08:00:00Z 2023-05-14T12:00:00Z 2023-05-14T12:00:00Z
    #+END_PLAIN
  - Reste à voir si cela a un impact
  - Une première compaction a eu lieu à 09:37 (heure du noeud)
    #+BEGIN_PLAIN
mai 14 09:37:58 raspberrypi influxd[438]: ts=2023-05-14T07:37:58.486512Z lvl=info msg="Cache snapshot (start)" log_id=0hnIEsZG000 engine=tsm1 trace_id=0hnIy1qW000 op_name=tsm1_cache_snapshot op_event=start
mai 14 09:38:00 raspberrypi influxd[438]: ts=2023-05-14T07:38:00.785781Z lvl=info msg="Snapshot for path written" log_id=0hnIEsZG000 engine=tsm1 trace_id=0hnIy1qW000 op_name=tsm1_cache_snapshot path=/data/influxdb/data/SensorData/short_lived/4 duration=2299.397ms
mai 14 09:38:00 raspberrypi influxd[438]: ts=2023-05-14T07:38:00.786026Z lvl=info msg="Cache snapshot (end)" log_id=0hnIEsZG000 engine=tsm1 trace_id=0hnIy1qW000 op_name=tsm1_cache_snapshot op_event=end op_elapsed=2299.539ms
mai 14 09:38:01 raspberrypi influxd[438]: ts=2023-05-14T07:38:01.487226Z lvl=info msg="TSM compaction (start)" log_id=0hnIEsZG000 engine=tsm1 tsm1_level=1 tsm1_strategy=level trace_id=0hnIyDZl000 op_name=tsm1_compact_group op_event=start
mai 14 09:38:01 raspberrypi influxd[438]: ts=2023-05-14T07:38:01.487357Z lvl=info msg="Beginning compaction" log_id=0hnIEsZG000 engine=tsm1 tsm1_level=1 tsm1_strategy=level trace_id=0hnIyDZl000 op_name=tsm1_compact_group tsm1_files_n=8
mai 14 09:38:01 raspberrypi influxd[438]: ts=2023-05-14T07:38:01.487412Z lvl=info msg="Compacting file" log_id=0hnIEsZG000 engine=tsm1 tsm1_level=1 tsm1_strategy=level trace_id=0hnIyDZl000 op_name=tsm1_compact_group tsm1_index=0 tsm1_file=/data/influxdb/data/SensorData/short_lived/4/000000001-000000001.tsm
mai 14 09:38:01 raspberrypi influxd[438]: ts=2023-05-14T07:38:01.487468Z lvl=info msg="Compacting file" log_id=0hnIEsZG000 engine=tsm1 tsm1_level=1 tsm1_strategy=level trace_id=0hnIyDZl000 op_name=tsm1_compact_group tsm1_index=1 tsm1_file=/data/influxdb/data/SensorData/short_lived/4/000000002-000000001.tsm
mai 14 09:38:01 raspberrypi influxd[438]: ts=2023-05-14T07:38:01.487504Z lvl=info msg="Compacting file" log_id=0hnIEsZG000 engine=tsm1 tsm1_level=1 tsm1_strategy=level trace_id=0hnIyDZl000 op_name=tsm1_compact_group tsm1_index=2 tsm1_file=/data/influxdb/data/SensorData/short_lived/4/000000003-000000001.tsm
mai 14 09:38:01 raspberrypi influxd[438]: ts=2023-05-14T07:38:01.487559Z lvl=info msg="Compacting file" log_id=0hnIEsZG000 engine=tsm1 tsm1_level=1 tsm1_strategy=level trace_id=0hnIyDZl000 op_name=tsm1_compact_group tsm1_index=3 tsm1_file=/data/influxdb/data/SensorData/short_lived/4/000000004-000000001.tsm
mai 14 09:38:01 raspberrypi influxd[438]: ts=2023-05-14T07:38:01.487594Z lvl=info msg="Compacting file" log_id=0hnIEsZG000 engine=tsm1 tsm1_level=1 tsm1_strategy=level trace_id=0hnIyDZl000 op_name=tsm1_compact_group tsm1_index=4 tsm1_file=/data/influxdb/data/SensorData/short_lived/4/000000005-000000001.tsm
mai 14 09:38:01 raspberrypi influxd[438]: ts=2023-05-14T07:38:01.487659Z lvl=info msg="Compacting file" log_id=0hnIEsZG000 engine=tsm1 tsm1_level=1 tsm1_strategy=level trace_id=0hnIyDZl000 op_name=tsm1_compact_group tsm1_index=5 tsm1_file=/data/influxdb/data/SensorData/short_lived/4/000000006-000000001.tsm
mai 14 09:38:01 raspberrypi influxd[438]: ts=2023-05-14T07:38:01.487693Z lvl=info msg="Compacting file" log_id=0hnIEsZG000 engine=tsm1 tsm1_level=1 tsm1_strategy=level trace_id=0hnIyDZl000 op_name=tsm1_compact_group tsm1_index=6 tsm1_file=/data/influxdb/data/SensorData/short_lived/4/000000007-000000001.tsm
mai 14 09:38:01 raspberrypi influxd[438]: ts=2023-05-14T07:38:01.487750Z lvl=info msg="Compacting file" log_id=0hnIEsZG000 engine=tsm1 tsm1_level=1 tsm1_strategy=level trace_id=0hnIyDZl000 op_name=tsm1_compact_group tsm1_index=7 tsm1_file=/data/influxdb/data/SensorData/short_lived/4/000000009-000000001.tsm
mai 14 09:38:01 raspberrypi influxd[438]: [httpd] ::1 - admin [14/May/2023:09:38:01 +0200] "POST /write?db=SensorData HTTP/1.1" 204 0 "-" "python-requests/2.21.0" 41460f35-f22a-11ed-80ad-000000000000 39453
mai 14 09:38:05 raspberrypi influxd[438]: [httpd] ::1 - admin [14/May/2023:09:38:05 +0200] "POST /write?db=SensorData HTTP/1.1" 204 0 "-" "python-requests/2.21.0" 43b8a241-f22a-11ed-80ae-000000000000 46620
mai 14 09:38:10 raspberrypi influxd[438]: [httpd] ::1 - admin [14/May/2023:09:38:10 +0200] "POST /write?db=SensorData HTTP/1.1" 204 0 "-" "python-requests/2.21.0" 466891a3-f22a-11ed-80af-000000000000 34139
mai 14 09:38:14 raspberrypi influxd[438]: [httpd] ::1 - admin [14/May/2023:09:38:14 +0200] "POST /write?db=SensorData HTTP/1.1" 204 0 "-" "python-requests/2.21.0" 48ea1b32-f22a-11ed-80b0-000000000000 50190
mai 14 09:38:18 raspberrypi influxd[438]: [httpd] ::1 - admin [14/May/2023:09:38:18 +0200] "POST /write?db=SensorData HTTP/1.1" 204 0 "-" "python-requests/2.21.0" 4b4e3cfa-f22a-11ed-80b1-000000000000 148018
mai 14 09:38:22 raspberrypi influxd[438]: [httpd] ::1 - admin [14/May/2023:09:38:22 +0200] "POST /write?db=SensorData HTTP/1.1" 204 0 "-" "python-requests/2.21.0" 4dc15fba-f22a-11ed-80b2-000000000000 71808
mai 14 09:38:26 raspberrypi influxd[438]: [httpd] ::1 - admin [14/May/2023:09:38:26 +0200] "POST /write?db=SensorData HTTP/1.1" 204 0 "-" "python-requests/2.21.0" 50471862-f22a-11ed-80b3-000000000000 31373
mai 14 09:38:28 raspberrypi influxd[438]: ts=2023-05-14T07:38:28.382788Z lvl=info msg="Compacted file" log_id=0hnIEsZG000 engine=tsm1 tsm1_level=1 tsm1_strategy=level trace_id=0hnIyDZl000 op_name=tsm1_compact_group tsm1_index=0 tsm1_file=/data/influxdb/data/SensorData/short_lived/4/000000009-000000002.tsm.tmp
mai 14 09:38:28 raspberrypi influxd[438]: ts=2023-05-14T07:38:28.387826Z lvl=info msg="Finished compacting files" log_id=0hnIEsZG000 engine=tsm1 tsm1_level=1 tsm1_strategy=level trace_id=0hnIyDZl000 op_name=tsm1_compact_group tsm1_files_n=1
mai 14 09:38:28 raspberrypi influxd[438]: ts=2023-05-14T07:38:28.387872Z lvl=info msg="TSM compaction (end)" log_id=0hnIEsZG000 engine=tsm1 tsm1_level=1 tsm1_strategy=level trace_id=0hnIyDZl000 op_name=tsm1_compact_group op_event=end op_elapsed=26900.686ms
    #+END_PLAIN
  - A concerné le shard 4, i.e. le 1er shard de SensorData
    - On est passé d'un shard contenant 7 fichiers tsm de niveau 1
      #+BEGIN_PLAIN
> sudo ./test.sh (ls -l /data/influxdb/data/SensorData/4)
total 107420
-rw-r--r--  1 influxdb influxdb 15832867 mai   14 07:21 000000001-000000001.tsm
-rw-r--r--  1 influxdb influxdb 15405657 mai   14 07:42 000000002-000000001.tsm
-rw-r--r--  1 influxdb influxdb 15948310 mai   14 08:01 000000003-000000001.tsm
-rw-r--r--  1 influxdb influxdb 15935606 mai   14 08:19 000000004-000000001.tsm
-rw-r--r--  1 influxdb influxdb 15514571 mai   14 08:38 000000005-000000001.tsm
-rw-r--r--  1 influxdb influxdb 15647234 mai   14 08:57 000000006-000000001.tsm
-rw-r--r--  1 influxdb influxdb 15689207 mai   14 09:18 000000007-000000001.tsm
-rw-r--r--  1 influxdb influxdb     2798 mai   14 07:07 fields.idx
drwxr-xr-x 10 influxdb influxdb     4096 mai   14 07:02 index
      #+END_PLAIN
    - À un fichier tsm unique de niveau 2
      #+BEGIN_PLAIN
> sudo ./test.sh
total 122592
-rw-r--r--  1 influxdb influxdb 125525378 mai   14 09:38 000000009-000000002.tsm
-rw-r--r--  1 influxdb influxdb      2798 mai   14 07:07 fields.idx
drwxr-xr-x 10 influxdb influxdb      4096 mai   14 07:02 index
      #+END_PLAIN
  - On remarque que cette première étape de compression a déjà été couteuse
    - 27s d'exécution
    - 60% de la RAM utilisée
      - [[file:img/2024-02-22-09h37-tsm-compaction-grafana-hardware-monitoring.png]]
  - Et qu'on obtient un fichier volumineux en résultat
    - 125Mo
  - Quid d'une compaction de niveau 2 traitant plusieurs de ces fichiers ?
  - Par contre je ne pige pas pour collectd, où sont les données ?
    - Pas de fichiers tsm stockés au niveau du shard
      #+BEGIN_PLAIN
> sudo ls -l /data/influxdb/data/collectd/autogen/2
total 8
-rw-r--r--  1 influxdb influxdb  486 mai   14 03:55 fields.idx
drwxr-xr-x 10 influxdb influxdb 4096 mai   14 03:55 index
      #+END_PLAIN
  - Nouvelle compaction a eu lieu à 12:10 (heure du noeud)
    - Logs
      #+BEGIN_PLAIN
mai 14 12:10:19 raspberrypi influxd[438]: [httpd] ::1 - admin [14/May/2023:12:10:19 +0200] "POST /write?db=SensorData HTTP/1.1" 204 0 "-" "python-requests/2.21.0" 87bcab70-f23f-11ed-897b-000000000000 95581
mai 14 12:10:20 raspberrypi influxd[438]: ts=2023-05-14T10:10:20.073216Z lvl=info msg="Cache snapshot (start)" log_id=0hnIEsZG000 engine=tsm1 trace_id=0hnRf~AG000 op_name=tsm1_cache_snapshot op_event=start
mai 14 12:10:22 raspberrypi influxd[438]: ts=2023-05-14T10:10:22.761640Z lvl=info msg="Snapshot for path written" log_id=0hnIEsZG000 engine=tsm1 trace_id=0hnRf~AG000 op_name=tsm1_cache_snapshot path=/data/influxdb/data/SensorData/short_lived/5 duration=2688.487ms
mai 14 12:10:22 raspberrypi influxd[438]: ts=2023-05-14T10:10:22.761926Z lvl=info msg="Cache snapshot (end)" log_id=0hnIEsZG000 engine=tsm1 trace_id=0hnRf~AG000 op_name=tsm1_cache_snapshot op_event=end op_elapsed=2688.765ms
mai 14 12:10:23 raspberrypi influxd[438]: ts=2023-05-14T10:10:23.074276Z lvl=info msg="TSM compaction (start)" log_id=0hnIEsZG000 engine=tsm1 tsm1_level=1 tsm1_strategy=level trace_id=0hnRgAtW000 op_name=tsm1_compact_group op_event=start
mai 14 12:10:23 raspberrypi influxd[438]: ts=2023-05-14T10:10:23.074467Z lvl=info msg="Beginning compaction" log_id=0hnIEsZG000 engine=tsm1 tsm1_level=1 tsm1_strategy=level trace_id=0hnRgAtW000 op_name=tsm1_compact_group tsm1_files_n=8
mai 14 12:10:23 raspberrypi influxd[438]: ts=2023-05-14T10:10:23.074521Z lvl=info msg="Compacting file" log_id=0hnIEsZG000 engine=tsm1 tsm1_level=1 tsm1_strategy=level trace_id=0hnRgAtW000 op_name=tsm1_compact_group tsm1_index=0 tsm1_file=/data/influxdb/data/SensorData/short_lived/5/000000001-000000001.tsm
mai 14 12:10:23 raspberrypi influxd[438]: ts=2023-05-14T10:10:23.074605Z lvl=info msg="Compacting file" log_id=0hnIEsZG000 engine=tsm1 tsm1_level=1 tsm1_strategy=level trace_id=0hnRgAtW000 op_name=tsm1_compact_group tsm1_index=1 tsm1_file=/data/influxdb/data/SensorData/short_lived/5/000000003-000000001.tsm
mai 14 12:10:23 raspberrypi influxd[438]: ts=2023-05-14T10:10:23.074663Z lvl=info msg="Compacting file" log_id=0hnIEsZG000 engine=tsm1 tsm1_level=1 tsm1_strategy=level trace_id=0hnRgAtW000 op_name=tsm1_compact_group tsm1_index=2 tsm1_file=/data/influxdb/data/SensorData/short_lived/5/000000004-000000001.tsm
mai 14 12:10:23 raspberrypi influxd[438]: ts=2023-05-14T10:10:23.074758Z lvl=info msg="Compacting file" log_id=0hnIEsZG000 engine=tsm1 tsm1_level=1 tsm1_strategy=level trace_id=0hnRgAtW000 op_name=tsm1_compact_group tsm1_index=3 tsm1_file=/data/influxdb/data/SensorData/short_lived/5/000000005-000000001.tsm
mai 14 12:10:23 raspberrypi influxd[438]: ts=2023-05-14T10:10:23.074815Z lvl=info msg="Compacting file" log_id=0hnIEsZG000 engine=tsm1 tsm1_level=1 tsm1_strategy=level trace_id=0hnRgAtW000 op_name=tsm1_compact_group tsm1_index=4 tsm1_file=/data/influxdb/data/SensorData/short_lived/5/000000006-000000001.tsm
mai 14 12:10:23 raspberrypi influxd[438]: ts=2023-05-14T10:10:23.074923Z lvl=info msg="Compacting file" log_id=0hnIEsZG000 engine=tsm1 tsm1_level=1 tsm1_strategy=level trace_id=0hnRgAtW000 op_name=tsm1_compact_group tsm1_index=5 tsm1_file=/data/influxdb/data/SensorData/short_lived/5/000000007-000000001.tsm
mai 14 12:10:23 raspberrypi influxd[438]: ts=2023-05-14T10:10:23.075023Z lvl=info msg="Compacting file" log_id=0hnIEsZG000 engine=tsm1 tsm1_level=1 tsm1_strategy=level trace_id=0hnRgAtW000 op_name=tsm1_compact_group tsm1_index=6 tsm1_file=/data/influxdb/data/SensorData/short_lived/5/000000008-000000001.tsm
mai 14 12:10:23 raspberrypi influxd[438]: ts=2023-05-14T10:10:23.075175Z lvl=info msg="Compacting file" log_id=0hnIEsZG000 engine=tsm1 tsm1_level=1 tsm1_strategy=level trace_id=0hnRgAtW000 op_name=tsm1_compact_group tsm1_index=7 tsm1_file=/data/influxdb/data/SensorData/short_lived/5/000000009-000000001.tsm
mai 14 12:10:23 raspberrypi influxd[438]: [httpd] ::1 - admin [14/May/2023:12:10:23 +0200] "POST /write?db=SensorData HTTP/1.1" 204 0 "-" "python-requests/2.21.0" 8a381246-f23f-11ed-897c-000000000000 28174
mai 14 12:10:27 raspberrypi influxd[438]: [httpd] ::1 - admin [14/May/2023:12:10:27 +0200] "POST /write?db=SensorData HTTP/1.1" 204 0 "-" "python-requests/2.21.0" 8cc62371-f23f-11ed-897d-000000000000 29153
mai 14 12:10:34 raspberrypi influxd[438]: [httpd] ::1 - admin [14/May/2023:12:10:31 +0200] "POST /write?db=SensorData HTTP/1.1" 204 0 "-" "python-requests/2.21.0" 8f44fe59-f23f-11ed-897e-000000000000 2250719
mai 14 12:10:36 raspberrypi influxd[438]: [httpd] ::1 - admin [14/May/2023:12:10:35 +0200] "POST /write?db=SensorData HTTP/1.1" 204 0 "-" "python-requests/2.21.0" 91b68b5e-f23f-11ed-897f-000000000000 56507
mai 14 12:10:40 raspberrypi influxd[438]: [httpd] ::1 - admin [14/May/2023:12:10:40 +0200] "POST /write?db=SensorData HTTP/1.1" 204 0 "-" "python-requests/2.21.0" 942b5608-f23f-11ed-8980-000000000000 86398
mai 14 12:10:44 raspberrypi influxd[438]: [httpd] ::1 - admin [14/May/2023:12:10:44 +0200] "POST /write?db=SensorData HTTP/1.1" 204 0 "-" "python-requests/2.21.0" 96954054-f23f-11ed-8981-000000000000 547207
mai 14 12:10:45 raspberrypi influxd[438]: ts=2023-05-14T10:10:45.148315Z lvl=info msg="Compacted file" log_id=0hnIEsZG000 engine=tsm1 tsm1_level=1 tsm1_strategy=level trace_id=0hnRgAtW000 op_name=tsm1_compact_group tsm1_index=0 tsm1_file=/data/influxdb/data/SensorData/short_lived/5/000000009-000000002.tsm.tmp
mai 14 12:10:45 raspberrypi influxd[438]: ts=2023-05-14T10:10:45.148498Z lvl=info msg="Finished compacting files" log_id=0hnIEsZG000 engine=tsm1 tsm1_level=1 tsm1_strategy=level trace_id=0hnRgAtW000 op_name=tsm1_compact_group tsm1_files_n=1
mai 14 12:10:45 raspberrypi influxd[438]: ts=2023-05-14T10:10:45.148537Z lvl=info msg="TSM compaction (end)" log_id=0hnIEsZG000 engine=tsm1 tsm1_level=1 tsm1_strategy=level trace_id=0hnRgAtW000 op_name=tsm1_compact_group op_event=end op_elapsed=22074.286ms
      #+END_PLAIN
    - Performances
      - [[file:img/2024-02-22-12h10-tsm-compaction-grafana-hardware-monitoring.png]]
    - A impacté le shard 5
      #+BEGIN_PLAIN
> sudo ./test.sh (ls -l /data/influxdb/data/SensorData/short_lived/5)
-rw-r--r--  1 influxdb influxdb 109136415 mai   14 12:10 000000009-000000002.tsm
-rw-r--r--  1 influxdb influxdb  15752157 mai   14 12:28 000000010-000000001.tsm
-rw-r--r--  1 influxdb influxdb  15483866 mai   14 12:47 000000011-000000001.tsm
-rw-r--r--  1 influxdb influxdb  15175954 mai   14 13:05 000000012-000000001.tsm
-rw-r--r--  1 influxdb influxdb  14752678 mai   14 13:24 000000013-000000001.tsm
-rw-r--r--  1 influxdb influxdb  15400436 mai   14 13:42 000000014-000000001.tsm
-rw-r--r--  1 influxdb influxdb  14456614 mai   14 14:10 000000015-000000001.tsm
-rw-r--r--  1 influxdb influxdb       960 mai   14 11:45 fields.idx
drwxr-xr-x 10 influxdb influxdb      4096 mai   14 09:02 index
      #+END_PLAIN
    - Mais remarque depuis l'ajout de nouvelles données
    - Qui entraineront probablement une compaction supplémentaire
    - Mais d'où sortent ces nouvelles données ?
    - L'heure de fin du shard était fixée à 12:00
  - Nouvelle compaction a eu lieu à 14:10 (heure du noeud)
    - Logs
      #+BEGIN_PLAIN
mai 14 14:10:05 raspberrypi influxd[438]: ts=2023-05-14T12:10:05.487024Z lvl=info msg="TSM compaction (start)" log_id=0hnIEsZG000 engine=tsm1 tsm1_strategy=full tsm1_optimize=false trace_id=0hnYXZBW000 op_name=tsm1_compact_group op_event=start
mai 14 14:10:05 raspberrypi influxd[438]: ts=2023-05-14T12:10:05.487144Z lvl=info msg="Beginning compaction" log_id=0hnIEsZG000 engine=tsm1 tsm1_strategy=full tsm1_optimize=false trace_id=0hnYXZBW000 op_name=tsm1_compact_group tsm1_files_n=3
mai 14 14:10:05 raspberrypi influxd[438]: ts=2023-05-14T12:10:05.487189Z lvl=info msg="Compacting file" log_id=0hnIEsZG000 engine=tsm1 tsm1_strategy=full tsm1_optimize=false trace_id=0hnYXZBW000 op_name=tsm1_compact_group tsm1_index=0 tsm1_file=/data/influxdb/data/SensorData/short_lived/4/000000009-000000002.tsm
mai 14 14:10:05 raspberrypi influxd[438]: ts=2023-05-14T12:10:05.487271Z lvl=info msg="Compacting file" log_id=0hnIEsZG000 engine=tsm1 tsm1_strategy=full tsm1_optimize=false trace_id=0hnYXZBW000 op_name=tsm1_compact_group tsm1_index=1 tsm1_file=/data/influxdb/data/SensorData/short_lived/4/000000010-000000001.tsm
mai 14 14:10:05 raspberrypi influxd[438]: ts=2023-05-14T12:10:05.487317Z lvl=info msg="Compacting file" log_id=0hnIEsZG000 engine=tsm1 tsm1_strategy=full tsm1_optimize=false trace_id=0hnYXZBW000 op_name=tsm1_compact_group tsm1_index=2 tsm1_file=/data/influxdb/data/SensorData/short_lived/4/000000011-000000001.tsm
mai 14 14:10:08 raspberrypi influxd[438]: [httpd] ::1 - admin [14/May/2023:14:10:08 +0200] "POST /write?db=SensorData HTTP/1.1" 204 0 "-" "python-requests/2.21.0" 44b407e9-f250-11ed-903e-000000000000 106170
mai 14 14:10:13 raspberrypi influxd[438]: [httpd] ::1 - admin [14/May/2023:14:10:12 +0200] "POST /write?db=SensorData HTTP/1.1" 204 0 "-" "python-requests/2.21.0" 47523785-f250-11ed-903f-000000000000 1219454
mai 14 14:10:16 raspberrypi influxd[438]: [httpd] ::1 - admin [14/May/2023:14:10:16 +0200] "POST /write?db=SensorData HTTP/1.1" 204 0 "-" "python-requests/2.21.0" 49c01987-f250-11ed-9040-000000000000 275720
mai 14 14:10:20 raspberrypi influxd[438]: [httpd] ::1 - admin [14/May/2023:14:10:20 +0200] "POST /write?db=SensorData HTTP/1.1" 204 0 "-" "python-requests/2.21.0" 4c0932da-f250-11ed-9041-000000000000 248464
mai 14 14:10:24 raspberrypi influxd[438]: ts=2023-05-14T12:10:24.956136Z lvl=info msg="Compacted file" log_id=0hnIEsZG000 engine=tsm1 tsm1_strategy=full tsm1_optimize=false trace_id=0hnYXZBW000 op_name=tsm1_compact_group tsm1_index=0 tsm1_file=/data/influxdb/data/SensorData/short_lived/4/000000011-000000002.tsm.tmp
mai 14 14:10:24 raspberrypi influxd[438]: ts=2023-05-14T12:10:24.956414Z lvl=info msg="Finished compacting files" log_id=0hnIEsZG000 engine=tsm1 tsm1_strategy=full tsm1_optimize=false trace_id=0hnYXZBW000 op_name=tsm1_compact_group tsm1_files_n=1
mai 14 14:10:24 raspberrypi influxd[438]: ts=2023-05-14T12:10:24.956567Z lvl=info msg="TSM compaction (end)" log_id=0hnIEsZG000 engine=tsm1 tsm1_strategy=full tsm1_optimize=false trace_id=0hnYXZBW000 op_name=tsm1_compact_group op_event=end op_elapsed=19469.573ms
      #+END_PLAIN
    - Performances
      - [[file:img/2024-02-22-14h10-tsm-compaction-grafana-hardware-monitoring.png]]
    - A impacté cette fois-ci le shard 4
      #+BEGIN_PLAIN
> sudo ./test.sh
-rw-r--r--  1 influxdb influxdb 143834863 mai   14 14:10 000000011-000000002.tsm
-rw-r--r--  1 influxdb influxdb      2798 mai   14 07:07 fields.idx
drwxr-xr-x 10 influxdb influxdb      4096 mai   14 07:02 index
      #+END_PLAIN
    - De manière similaire, pourquoi ces nouvelles données au niveau du shard 4 ?
      - Censé être inactif depuis 08:00
  - Comportement étrange du shard 6
    - Alors qu'il est censé couvrir les données que de 12:00 à 16:00
    - À 17:03, il est toujours en train de récupérer des données
    - Et son successeur n'est toujours pas créé
    - Que fait influxdb ?
    - Et où sont stockées les données ?
      - À 17:20, aucun fichier .tsm ne semble avoir été mis à jour
      - i.e. aucune MàJ dans _series depuis 07:02 et 10:04, dans les shards depuis 17:03
      - Probablement dans le WAL du coup
    - Ah non c'est bon, à 17:22, MàJ des fichiers du shard 6
  - Mais toujours pas de shard 7 à 17:24, soit 1:24 après la fin du shard 6
  - C'est bien ça, les données que je ne vois pas sont en fait stockées dans le WAL
    #+BEGIN_PLAIN
> sudo ls -l /data/influxdb/wal/collectd/autogen/2
total 14320
-rw-r--r-- 1 influxdb influxdb 14657777 mai   14 17:32 _00001.wal
> sudo ls -l /data/influxdb/wal/SensorData/short_lived/6
total 8888
-rw-r--r-- 1 influxdb influxdb 9099577 mai   14 17:33 _00023.wal
    #+END_PLAIN
  - Pourquoi le WAL se comporte de manière si différente entre les BDs ?
    - Écrit régulièrement pour SensorData
    - Et pour le moment, ne constate pas la moindre écriture pour collectd
    - A une taille limite de cache, 25Mo
    - Mais cette taille semble jamais atteinte dans le cas de de SensorData
      - Pas vu de fichier tsm de niveau 1 dépassant les 20Mo
    - La discussion suivante apporte une réponse :
      - https://community.influxdata.com/t/what-determines-when-these-influxdb-wal-directory-contents-are-moved-to-data/14785
      - Correspond à la taille en mémoire de la structure WAL
      - Forcément, sa taille est réduite lors du passage en TSM, cette structure étant plus efficace d'un point de vue taille
  - Ne comprends pas la différence entre les options cache-snapshot-memory-size et max-index-log-file-size
    - cache-snapshot-memory-size : la taille du TSM en mémoire avant qu'il soit écrit sur le disque
    - max-index-log-file-size : la taille de l'index du WAL avant qu'il soit écrit sur le disque
    - WAL
      - Structure de données optimisée pour l'écriture des données et non pas la lecture
      - Sert de buffer avant de reporter les écritures au TSM
      - Données dans le WAL sont tout de même comprises dans les requêtes
      - Durable
    - Cache
      - Représentation en mémoire du WAL
      - Lorsqu'il atteint la taille limite, données sont écrites dans un TSM
      - Cache et fichiers WAL correspondants sont réinitialisé/supprimés
    - TSM
      - Structure de données optimisée pour la lecture
    - La discussion suivante aide bien à comprendre : https://community.influxdata.com/t/how-influxdb-work-with-files-tsm-db-wal/787/9
  - En vrai, nous on se passerait bien des TSM
    - Mais il n'y a pas l'air d'y avoir de mode de fonctionnement WAL-only
  - De nouveau un crash à 22:00 (heure du noeud)
    - Toujours un problème out of RAM
  - Sauf que cette fois-ic, rencontre un problème au redémarrage d'InfluxDB
    - Logs
      #+BEGIN_PLAIN
mai 14 22:00:20 raspberrypi influxd[6485]: ts=2023-05-14T20:00:20.383999Z lvl=info msg="TSM compaction (start)" log_id=0hnyQydG000 engine=tsm1 tsm1_strategy=full tsm1_optimize=false trace_id=0hnyRecl000 op_name=tsm1_compact_group op_event=start
mai 14 22:00:20 raspberrypi influxd[6485]: ts=2023-05-14T20:00:20.384182Z lvl=info msg="Beginning compaction" log_id=0hnyQydG000 engine=tsm1 tsm1_strategy=full tsm1_optimize=false trace_id=0hnyRecl000 op_name=tsm1_compact_group tsm1_files_n=6
mai 14 22:00:20 raspberrypi influxd[6485]: ts=2023-05-14T20:00:20.384263Z lvl=info msg="Compacting file" log_id=0hnyQydG000 engine=tsm1 tsm1_strategy=full tsm1_optimize=false trace_id=0hnyRecl000 op_name=tsm1_compact_group tsm1_index=0 tsm1_file=/data/influxdb/data/SensorData/short_lived/6/000000008-000000002.tsm
mai 14 22:00:20 raspberrypi influxd[6485]: ts=2023-05-14T20:00:20.384317Z lvl=info msg="Compacting file" log_id=0hnyQydG000 engine=tsm1 tsm1_strategy=full tsm1_optimize=false trace_id=0hnyRecl000 op_name=tsm1_compact_group tsm1_index=1 tsm1_file=/data/influxdb/data/SensorData/short_lived/6/000000009-000000001.tsm
mai 14 22:00:20 raspberrypi influxd[6485]: ts=2023-05-14T20:00:20.384399Z lvl=info msg="Compacting file" log_id=0hnyQydG000 engine=tsm1 tsm1_strategy=full tsm1_optimize=false trace_id=0hnyRecl000 op_name=tsm1_compact_group tsm1_index=2 tsm1_file=/data/influxdb/data/SensorData/short_lived/6/000000010-000000001.tsm
mai 14 22:00:20 raspberrypi influxd[6485]: ts=2023-05-14T20:00:20.384480Z lvl=info msg="Compacting file" log_id=0hnyQydG000 engine=tsm1 tsm1_strategy=full tsm1_optimize=false trace_id=0hnyRecl000 op_name=tsm1_compact_group tsm1_index=3 tsm1_file=/data/influxdb/data/SensorData/short_lived/6/000000011-000000001.tsm
mai 14 22:00:20 raspberrypi influxd[6485]: ts=2023-05-14T20:00:20.384562Z lvl=info msg="Compacting file" log_id=0hnyQydG000 engine=tsm1 tsm1_strategy=full tsm1_optimize=false trace_id=0hnyRecl000 op_name=tsm1_compact_group tsm1_index=4 tsm1_file=/data/influxdb/data/SensorData/short_lived/6/000000012-000000001.tsm
mai 14 22:00:20 raspberrypi influxd[6485]: ts=2023-05-14T20:00:20.384643Z lvl=info msg="Compacting file" log_id=0hnyQydG000 engine=tsm1 tsm1_strategy=full tsm1_optimize=false trace_id=0hnyRecl000 op_name=tsm1_compact_group tsm1_index=5 tsm1_file=/data/influxdb/data/SensorData/short_lived/6/000000013-000000001.tsm
mai 14 22:00:21 raspberrypi influxd[6485]: [httpd] ::1 - admin [14/May/2023:22:00:21 +0200] "POST /write?db=SensorData HTTP/1.1" 204 0 "-" "python-requests/2.21.0" f4d8655f-f291-11ed-8002-000000000000 37448
mai 14 22:00:25 raspberrypi influxd[6485]: [httpd] ::1 - admin [14/May/2023:22:00:25 +0200] "POST /write?db=SensorData HTTP/1.1" 204 0 "-" "python-requests/2.21.0" f769a241-f291-11ed-8003-000000000000 45923
mai 14 22:00:29 raspberrypi influxd[6485]: [httpd] ::1 - admin [14/May/2023:22:00:29 +0200] "POST /write?db=SensorData HTTP/1.1" 204 0 "-" "python-requests/2.21.0" f9ccecd3-f291-11ed-8004-000000000000 62994
mai 14 22:00:34 raspberrypi influxd[6485]: [httpd] ::1 - admin [14/May/2023:22:00:33 +0200] "POST /write?db=SensorData HTTP/1.1" 204 0 "-" "python-requests/2.21.0" fc7080e1-f291-11ed-8005-000000000000 510367
mai 14 22:00:38 raspberrypi influxd[6485]: [httpd] ::1 - admin [14/May/2023:22:00:38 +0200] "POST /write?db=SensorData HTTP/1.1" 204 0 "-" "python-requests/2.21.0" fefc8a02-f291-11ed-8006-000000000000 198751
mai 14 22:00:42 raspberrypi influxd[6485]: [httpd] ::1 - admin [14/May/2023:22:00:42 +0200] "POST /write?db=SensorData HTTP/1.1" 204 0 "-" "python-requests/2.21.0" 0168ff58-f292-11ed-8007-000000000000 356423
mai 14 22:00:46 raspberrypi influxd[6485]: [httpd] ::1 - admin [14/May/2023:22:00:46 +0200] "POST /write?db=SensorData HTTP/1.1" 204 0 "-" "python-requests/2.21.0" 03ed81f9-f292-11ed-8008-000000000000 67127
mai 14 22:00:50 raspberrypi influxd[6485]: [httpd] ::1 - admin [14/May/2023:22:00:50 +0200] "POST /write?db=SensorData HTTP/1.1" 204 0 "-" "python-requests/2.21.0" 0685bed8-f292-11ed-8009-000000000000 89830
mai 14 22:00:55 raspberrypi influxd[6485]: ts=2023-05-14T20:00:55.222738Z lvl=info msg="Error replacing new TSM files" log_id=0hnyQydG000 engine=tsm1 tsm1_strategy=full tsm1_optimize=false trace_id=0hnyRecl000 op_name=tsm1_compact_group error="cannot allocate memory"
mai 14 22:00:55 raspberrypi influxd[6485]: [httpd] ::1 - admin [14/May/2023:22:00:54 +0200] "POST /write?db=SensorData HTTP/1.1" 204 0 "-" "python-requests/2.21.0" 08e1a644-f292-11ed-800a-000000000000 601332
mai 14 22:00:56 raspberrypi influxd[6485]: ts=2023-05-14T20:00:56.230288Z lvl=info msg="TSM compaction (end)" log_id=0hnyQydG000 engine=tsm1 tsm1_strategy=full tsm1_optimize=false trace_id=0hnyRecl000 op_name=tsm1_compact_group op_event=end op_elapsed=35846.263ms
mai 14 22:00:56 raspberrypi influxd[6485]: ts=2023-05-14T20:00:56.384020Z lvl=info msg="TSM compaction (start)" log_id=0hnyQydG000 engine=tsm1 tsm1_strategy=full tsm1_optimize=false trace_id=0hnyTrFl000 op_name=tsm1_compact_group op_event=start
mai 14 22:00:56 raspberrypi influxd[6485]: ts=2023-05-14T20:00:56.384158Z lvl=info msg="Beginning compaction" log_id=0hnyQydG000 engine=tsm1 tsm1_strategy=full tsm1_optimize=false trace_id=0hnyTrFl000 op_name=tsm1_compact_group tsm1_files_n=6
mai 14 22:00:56 raspberrypi influxd[6485]: ts=2023-05-14T20:00:56.384212Z lvl=info msg="Compacting file" log_id=0hnyQydG000 engine=tsm1 tsm1_strategy=full tsm1_optimize=false trace_id=0hnyTrFl000 op_name=tsm1_compact_group tsm1_index=0 tsm1_file=/data/influxdb/data/SensorData/short_lived/6/000000008-000000002.tsm
mai 14 22:00:56 raspberrypi influxd[6485]: ts=2023-05-14T20:00:56.384272Z lvl=info msg="Compacting file" log_id=0hnyQydG000 engine=tsm1 tsm1_strategy=full tsm1_optimize=false trace_id=0hnyTrFl000 op_name=tsm1_compact_group tsm1_index=1 tsm1_file=/data/influxdb/data/SensorData/short_lived/6/000000009-000000001.tsm
mai 14 22:00:56 raspberrypi influxd[6485]: ts=2023-05-14T20:00:56.384331Z lvl=info msg="Compacting file" log_id=0hnyQydG000 engine=tsm1 tsm1_strategy=full tsm1_optimize=false trace_id=0hnyTrFl000 op_name=tsm1_compact_group tsm1_index=2 tsm1_file=/data/influxdb/data/SensorData/short_lived/6/000000010-000000001.tsm
mai 14 22:00:56 raspberrypi influxd[6485]: ts=2023-05-14T20:00:56.384387Z lvl=info msg="Compacting file" log_id=0hnyQydG000 engine=tsm1 tsm1_strategy=full tsm1_optimize=false trace_id=0hnyTrFl000 op_name=tsm1_compact_group tsm1_index=3 tsm1_file=/data/influxdb/data/SensorData/short_lived/6/000000011-000000001.tsm
mai 14 22:00:56 raspberrypi influxd[6485]: ts=2023-05-14T20:00:56.384445Z lvl=info msg="Compacting file" log_id=0hnyQydG000 engine=tsm1 tsm1_strategy=full tsm1_optimize=false trace_id=0hnyTrFl000 op_name=tsm1_compact_group tsm1_index=4 tsm1_file=/data/influxdb/data/SensorData/short_lived/6/000000012-000000001.tsm
mai 14 22:00:56 raspberrypi influxd[6485]: ts=2023-05-14T20:00:56.384501Z lvl=info msg="Compacting file" log_id=0hnyQydG000 engine=tsm1 tsm1_strategy=full tsm1_optimize=false trace_id=0hnyTrFl000 op_name=tsm1_compact_group tsm1_index=5 tsm1_file=/data/influxdb/data/SensorData/short_lived/6/000000013-000000001.tsm
mai 14 22:00:56 raspberrypi influxd[6485]: ts=2023-05-14T20:00:56.384893Z lvl=info msg="Aborted compaction" log_id=0hnyQydG000 engine=tsm1 tsm1_strategy=full tsm1_optimize=false trace_id=0hnyTrFl000 op_name=tsm1_compact_group error="compaction in progress: open /data/influxdb/data/SensorData/short_lived/6/000000013-000000002.tsm.tmp: file exists"
mai 14 22:00:57 raspberrypi influxd[6485]: ts=2023-05-14T20:00:57.385245Z lvl=info msg="TSM compaction (end)" log_id=0hnyQydG000 engine=tsm1 tsm1_strategy=full tsm1_optimize=false trace_id=0hnyTrFl000 op_name=tsm1_compact_group op_event=end op_elapsed=1001.241ms
mai 14 22:00:58 raspberrypi influxd[6485]: ts=2023-05-14T20:00:58.384198Z lvl=info msg="TSM compaction (start)" log_id=0hnyQydG000 engine=tsm1 tsm1_strategy=full tsm1_optimize=false trace_id=0hnyTz40000 op_name=tsm1_compact_group op_event=start
mai 14 22:00:58 raspberrypi influxd[6485]: ts=2023-05-14T20:00:58.384359Z lvl=info msg="Beginning compaction" log_id=0hnyQydG000 engine=tsm1 tsm1_strategy=full tsm1_optimize=false trace_id=0hnyTz40000 op_name=tsm1_compact_group tsm1_files_n=6
mai 14 22:00:58 raspberrypi influxd[6485]: ts=2023-05-14T20:00:58.384414Z lvl=info msg="Compacting file" log_id=0hnyQydG000 engine=tsm1 tsm1_strategy=full tsm1_optimize=false trace_id=0hnyTz40000 op_name=tsm1_compact_group tsm1_index=0 tsm1_file=/data/influxdb/data/SensorData/short_lived/6/000000008-000000002.tsm
mai 14 22:00:58 raspberrypi influxd[6485]: ts=2023-05-14T20:00:58.384493Z lvl=info msg="Compacting file" log_id=0hnyQydG000 engine=tsm1 tsm1_strategy=full tsm1_optimize=false trace_id=0hnyTz40000 op_name=tsm1_compact_group tsm1_index=1 tsm1_file=/data/influxdb/data/SensorData/short_lived/6/000000009-000000001.tsm
mai 14 22:00:58 raspberrypi influxd[6485]: ts=2023-05-14T20:00:58.384564Z lvl=info msg="Compacting file" log_id=0hnyQydG000 engine=tsm1 tsm1_strategy=full tsm1_optimize=false trace_id=0hnyTz40000 op_name=tsm1_compact_group tsm1_index=2 tsm1_file=/data/influxdb/data/SensorData/short_lived/6/000000010-000000001.tsm
mai 14 22:00:58 raspberrypi influxd[6485]: ts=2023-05-14T20:00:58.384623Z lvl=info msg="Compacting file" log_id=0hnyQydG000 engine=tsm1 tsm1_strategy=full tsm1_optimize=false trace_id=0hnyTz40000 op_name=tsm1_compact_group tsm1_index=3 tsm1_file=/data/influxdb/data/SensorData/short_lived/6/000000011-000000001.tsm
mai 14 22:00:58 raspberrypi influxd[6485]: ts=2023-05-14T20:00:58.384706Z lvl=info msg="Compacting file" log_id=0hnyQydG000 engine=tsm1 tsm1_strategy=full tsm1_optimize=false trace_id=0hnyTz40000 op_name=tsm1_compact_group tsm1_index=4 tsm1_file=/data/influxdb/data/SensorData/short_lived/6/000000012-000000001.tsm
mai 14 22:00:58 raspberrypi influxd[6485]: ts=2023-05-14T20:00:58.384768Z lvl=info msg="Compacting file" log_id=0hnyQydG000 engine=tsm1 tsm1_strategy=full tsm1_optimize=false trace_id=0hnyTz40000 op_name=tsm1_compact_group tsm1_index=5 tsm1_file=/data/influxdb/data/SensorData/short_lived/6/000000013-000000001.tsm
mai 14 22:00:58 raspberrypi influxd[6485]: ts=2023-05-14T20:00:58.385214Z lvl=info msg="Aborted compaction" log_id=0hnyQydG000 engine=tsm1 tsm1_strategy=full tsm1_optimize=false trace_id=0hnyTz40000 op_name=tsm1_compact_group error="compaction in progress: open /data/influxdb/data/SensorData/short_lived/6/000000013-000000002.tsm.tmp: file exists"
mai 14 22:00:58 raspberrypi influxd[6485]: [httpd] ::1 - admin [14/May/2023:22:00:58 +0200] "POST /write?db=SensorData HTTP/1.1" 204 0 "-" "python-requests/2.21.0" 0b6daeed-f292-11ed-800b-000000000000 38946
mai 14 22:00:59 raspberrypi influxd[6485]: ts=2023-05-14T20:00:59.385566Z lvl=info msg="TSM compaction (end)" log_id=0hnyQydG000 engine=tsm1 tsm1_strategy=full tsm1_optimize=false trace_id=0hnyTz40000 op_name=tsm1_compact_group op_event=end op_elapsed=1001.396ms
      #+END_PLAIN
    - De ce que je comprends, au redémarrage, InfluxDB retente la compaction
    - Cette fois-ci, il abort car il tombe de nouveau out-of-memory
    - Mais au lieu de s'arrêter là, il retente la compaction
    - Détecte un fichier temp de la tentative de compaction précédente et abort de nouveau
    - Et boucle de cette manière en continue
  - Donc instance toujours fonctionnelle, mais néanmoins détériorée
  - Comment empêcher cela de se reproduire ?
    - Diminuer la durée de la fenêtre de temps d'un shard à 2h
    - En espérant que cette fois-ci, cela aboutisse bien à des shards plus petits
    - En profite pour modifier la configuration InfluxDB pour limiter les compactions en parallèle
      - La présentation de cette option utilise l'angle du CPU
      - Mais plusieurs compactions en concurrence devrait aussi impacter la RAM
        - e.g. de plusieurs shards
      - Pas sûr qu'on est déjà eu ce cas de figure
      - Mais dans le doute, ça coûte rien
  - Remise en état de la DB
    - Export des données
      - sudo cp -r /data/influxdb/ 2024-02-23-backupDataInfluxDB
    - Recréation du dossier de données
      - cd /data
      - sudo mkdir influxdb
      - sudo chown influxdb:influxdb influxdb/
    - MàJ de la config
      - sudo cp working-influxdb.conf /etc/influxdb/influxdb.conf
    - Démarrage d'InfluxDB
    - Création de l'utilisateur admin
    - Quitter/Relancer l'invit de commande
    - Créer les measurements avec les bonnes retention policies
      - DROP DATABASE collectd; CREATE DATABASE collectd WITH SHARD DURATION 2h NAME autogen
      - CREATE DATABASE SensorData WITH SHARD DURATION 2h NAME short_duration
  - Le fait que la BD démarre avec des shards dont la fenêtre est déjà censée être passée est peu encourageant
    - On va voir si ça se corrige par la suite
- SmartSense: Intégrer la branche *refactor/main-app*
  - MàJ par rapport à la branche *main* maintenant que *refactor/mqtt-interface-remove-inheritance* a été intégrée
- SmartSense: Intégrer la branche *refactor/usb-board-manager*
  - MR créée
- SmartSense: Étudier la mise en place d'une interface WiFi
  - Parcours plusieurs pages à la recherche d'infos
    - CM3+ and WiFi/BT CYW43455 : https://forums.raspberrypi.com/viewtopic.php?t=183618
      - Un peu HS, mais a l'air d'avoir rassemblé beaucoup de discussions
      - Des commentaires seront p-e pertinents pour la mise en place d'une interface WiFi pour CM3
    - How to connect Wi-Fi with ESP32 on CM3 ? : https://forums.raspberrypi.com/viewtopic.php?t=268087
      - Court
      - Mais donne quelques pistes intéressantes
        - Utilisation du device comme un serial device, configuration spécifique via des commandes AT
        - Utilisation du device via un port USB, utilisation de drivers standards
  - Existe donc un forum dédié aux CMs
    - https://forums.raspberrypi.com/viewforum.php?f=98&sid=417b80a60019b2755077dade3b1165b0
  - Description du fonctionnement du Compute Module
    - https://www.raspberrypi.com/documentation/computers/compute-module.html
    - Link des documents techniques sur l'architecture de la CM3 et de la Compute Module IO board (CMIO)
    - Présente s'initialise la CM3 à partir de la description hardware décrite dans des Device Trees.
    - Indique comment surcharger cette configuration par le biais d'overlays pour intégrer du hardware supplémentaire
      - Au hasard, carte WiFi
- SmartSense: Récapitulatif concernant bug InfluxDB
  - Pistes
    - Réduire davantage la fenêtre des shards
      - 4h00 semblait correcte comme durée
      - Mais le fait qu'InfluxDB ne spawn pas forcément de nouveaux shards à ce rythme casse cette stratégie
      - Voir si réduire davantage permet de mitiger ce problème
    - MàJ InfluxDB et vérifier si cela a un impact
      - Vérifier si le mécanisme de compression est plus performant
      - Vérifier si le mécanisme d'allocation des shards est plus réactif
    - MàJ InfluxDB et mettre en place un job de désactivation du mécanisme de compression des shards
      - Explorer les sous-dossiers de /data/influxdb/data/
      - Pour chaque feuille, i.e. dossier d'un shard, créer le fichier /do_not_compact/
    - Remplacer l'utilisation d'InfluxDB locale par un export brut
      - Possible d'insérer des données stockées dans un fichier au format Line
      - https://docs.influxdata.com/influxdb/v1/guides/write_data/#writing-points-from-a-file
      - Indique cependant qu'il faut limiter à 5k entrées par fichier
      - Y aurait besoin d'un utilitaire dans ce cas
        - Ce qu'on cherche à éviter
    - Upgrade du matériel
      - Dans ce cas, devrait probablement redesign le noeud SmartSense
** Semaine du <2024-02-12 Mon> au <2024-02-16 Fri>
*** Planned
**** DONE SmartSense: Self provision *InaManager* et *TlcManager*
CLOSED: [2024-02-13 Tue 17:00]
- À l'heure actuelle, on a un comportement étrange
- Le noeud sait, qu'en fonction de la configuration, il doit se self provision ou non
- Mais pas *InaManager* et *TlcManager*
- C'est lorsque leurs messages remontent à son niveau que le noeud les interceptent et y répond à la place du serveur
- Me paraît plus clair et simple d'avoir les différents modules qui se chargent eux-mêmes de leur self provisioning
**** DONE SmartSense: Mettre en place des *Factory* pour les *Manager*
CLOSED: [2024-02-14 Wed 16:59]
- Les objets *Manager* sont des objets un minimum complexe
- Potentiellement avec plusieurs configurations, en fonction du comportement voulu
  - Je pense à toi *MqttInterface*
- L'utilisation de *Factory* permettrait de préparer le terrain pour de l'injection de dépendances
- D'expliciter les paramètres requis pour chaque *Manager*
  - Au lieu de les avoir éparpillés dans le constructeur et dans la méthode executée lors du lancement du process
- Et permettrait aussi de déléguer l'instanciation des *Queue*
**** DONE SmartSense: Ajouter des tests pour *MainApp.handleProvisioningMsg()*
CLOSED: [2024-02-14 Wed 16:59]
- Vérifier le bon fonctionnement de cette méthode
- Me paraît un bon point d'entrée pour mettre en place des tests pour *MainApp*
**** DONE SmartSense: Rework *MainApp* pour permettre la mise en place de tests
CLOSED: [2024-02-15 Thu 14:08]
- En essayant de mettre en place des tests, j'ai rencontré le problème suivant
- *MainApp* instancie tous les composants de l'application, notamment ceux qui interagissent avec le hardware
- Empêche donc l'exécution de tests, même si c'est pour une méthode n'interagissant pas avec cette partie de l'application
  - Au hasard, *MainApp.handleProvisioningMsg()*
- Plusieurs pistes pour corriger ce problème
  - Mise en place d'injections de dépendances
    - Un framework à utiliser ?
  - Décomposition de *MainApp* en modules
    - Mais comment ?
  - Combinaison des deux
**** DONE SmartSense: Intégrer la branche *refactor/split-mqtt-interface*
CLOSED: [2024-02-15 Thu 14:08]
- Maintenant que la branche *feat/autonomous-node-and-add-tools* a été intégrée
- Peut commencer à intégrer les modifications dans mon backlog
- MàJ la branche *refactor/split-mqtt-interface* par rapport à *main*
- Faire la MR
**** DONE SmartSense: Mettre en place les conventions de nommage
CLOSED: [2024-02-16 Fri 16:44]
- Le projet n'a pas adopté initialement les conventions de nommage de Python
- Les avertissements de Pylint sur les morceaux de code complexes et conseils sur comment les retravailler se retrouvent donc noyés
- Afin d'éviter cela, j'ai adopté les conventions Python pour les nouveaux modules que j'ai défini
- On se retrouve donc avec un micmac de conventions dans le projet
- Modifier la base existante pour adopter les conventions Python
**** IN-PROGRESS SmartSense: Intégrer la branche *refactor/mqtt-interface-remove-inheritance*
- Je voulais pas forcément intégrer cette modif
- Mais une bonne partie des commits de *refactor/main-app* modifient des lignes concernées par ces changements
- Je me retrouve avec des conflits fréquemment lorsque j'essaie de rebase en le retirant de l'historique
- Me parait plus safe, et préférable, d'intégrer ces changements
**** IN-PROGRESS SmartSense: Intégrer la branche *refactor/main-app*
- MàJ par rapport à *refactor/mqtt-interface-remove-inheritance*
- Faire la MR
**** IN-PROGRESS SmartSense: Ajouter *NodeManager*
- Idée est d'avoir un module qui se charge
  - Du provisioning
  - Des commandes ciblant le noeud entier
- Permet de démêler le code de *MainApp* du code du noeud
  - Et le routing des messages de leur traitement
*** Done
- SmartSense: Rework *MainApp* pour permettre la mise en place de tests
  - Plusieurs tentatives de décomposition de *MainApp* en modules
  - *MessageManager*
    - À partir des queues de message des différents process
      - In et Out
    - Route les messages
      - Récupère de *MainApp* les méthodes suivantes
        - *route_incoming_message()*
        - *read_msg()*, l'équivalent du main
        - *publish_msg()*
    - Difficultés
      - Quid de *handle_cmd_msg()* ?
        - Pour certaines commandes, on délègue au module correspondant, i.e. on forward le message au bon module
        - Mais pour d'autres, on effectue directement des actions, e.g. la commande "restart"
        - Cela nécessite d'interagir (appels de fonctions) directement avec le module concerné
        - Comment gérer cela ?
      - Quid du provisioning ?
        - Est-ce qu'on déplace la logique du provisioning dans ce module ?
        - D'un côté, me paraît pas trop déconnant vu qu'on utilise le locationPath pour router les messages notamment sortant
        - Mais de l'autre, que ça soit le ce module qui soit à l'initiative du message de provisioning et qui intègre une logique d'attente me paraît étrange
  - *ProcessManager*
  - Idées
    - Ajout d'un module *NodeManager*
      - Pour le moment, le comportement du noeud et de *Main* sont confondus
      - D'où le fait que le *MessageManager* doive effectuer des actions lorsqu'il s'aperçoit qu'un message, e.g. une commande, lui est destiné
      - Et qu'il communique à la fois par le biais de messages et d'appels de fonction avec ces objets
      - Ajouter un module dédié permettrait de découpler cette logique
      - *NodeManager* pourrait être en charge de
        - Se provisionner
        - Exécuter les commandes concernant le noeud, ou une board mais nécessitant un niveau d'indirection supplémentaire
        - La synchronisation des modules
      - Permettrait de découpler instanciation des différents modules et fonctionnement du noeud itself
    - Self-provisioning de chacun des modules
      - Pourquoi seul le noeud sait qu'il doit se self-provision ?
      - Et qu'on se retrouve à devoir gérer le provisioning des modules Ina et Tlc au niveau du routing des messages ?
      - Pourquoi ne pas indiquer aux modules concernés qu'ils doivent se self-provision ?
      - Directement passer cette information
- SmartSense: Ajouter *NodeManager*
  - J'ai commencé à faire cela
  - Mais je viens de me rendre compte d'un potentiel problème
  - À l'heure actuelle, les commandes sont exécutées par le process principal
  - L'ajout de *NodeManager* déplacerait leur exécution dans un nouveau process
  - Donc pendant que la commande s'exécute, d'autres commandes peuvent être récupérées par le process principal, envoyées à un module et traitées
  - Est-ce qu'on ne risque pas de rencontrer des problèmes de concurrence du coup ?
    - Genre un /updateFirmwareCmd/ en concurrence d'un /set meta/
  - Après, est-ce qu'on a pas déjà ces problèmes ?
  - À vérifier
- SmartSense: Ajouter des tests pour *MainApp.handleProvisioningMsg()*
  - Maintenant que j'ai mis en place des *Factory*, peut voir pour mock leur comportement dans les tests
  - Peut les mocker dans le cadre d'un test à l'aide de *unittest.mock.patch()*
  - *patch()* peut aussi mocker une classe carrément
  - En mockant *MqttInterface* et *INA219Manager*, arrive à instancier *MainApp*
  - Pas besoin des *Factory* du coup
  - Quoique, rencontre l'erreur habituelle quand essaie d'intégrer les tests sur l'état pré-factory
    - *patch()* n'a pas l'air d'avoir d'effet
    - Le constructeur de *INA219Manager* est bel et bien appelé
  - Peut reproduire le bug sur la branche avec factory en repassant sur un appel au constructeur plutôt qu'à la factory
  - Why?
  - Tout simplement parce que j'utilise mal *patch()*
    - https://docs.python.org/3/library/unittest.mock.html#where-to-patch
  - Doit pas patcher d'où vient l'objet, mais où est son utilisation
    - Donc patcher *MainApp.INA219Manager* et non pas *INA219Manager.INA219Manager*
    - Cela fonctionnait avec la factory car *INA219Manager.INA219Manager* correspondait bien au module où l'objet était utilisé
  - Correction des patchs
  - Ajout de tests en guise d'exemple
    - Directement dans la branche *refactor/main-app*
- SmartSense: Intégrer la branche *refactor/split-mqtt-interface*
  - Ajout des modifs demandées par Mickaël
  - Intégration de la branche
- SmartSense: Intégrer la branche *refactor/mqtt-interface-remove-inheritance*
  - Rebase sur la branche *main*
  - Intégration du commit qui ajoute *QualityOfService*
  - Création de la MR : https://gitlab.inria.fr/smartsense/3douest/node-app/smartsense-node-app/-/merge_requests/4
- SmartSense: Intégrer la branche *refactor/main-app*
  - Rebase sur la branche *refactor/mqtt-interface-remove-inheritance*
  - Modification au passage du type annoté pour *sector* et *sub_sector*
  - Préparation du draft de la MR
    - En attendant que *refactor/mqtt-interface-remove-inheritance* soit validée et intégrée
  - MR : https://gitlab.inria.fr/smartsense/3douest/node-app/smartsense-node-app/-/merge_requests/5
** Semaine du <2024-02-05 Mon> au <2024-02-09 Fri>
*** Planned
**** DONE SmartSense: Ajouter des tests pour *DataToLocalInfluxDB.parseMqtt()*
CLOSED: [2024-02-05 Mon 13:58]
- J'ai atteint la limite du refactoring que je peux faire dans le projet en ayant à peu près confiance
- Et n'ayant pas de précisions sur les évolutions à apporter
- Serait préférable de consolider les tests pour le moment
- Pour le moment, vérifie le bon fonctionnement de *parseMqtt()* avec un payload correct
- Vérifier pour d'autres payloads corrects
- Vérifier pour payload invalide
**** DONE SmartSense: Ajouter des tests pour l'instanciation de *DataToLocalInfluxDB*
CLOSED: [2024-02-05 Mon 14:35]
- Comment se comporte l'objet si la base de données est inaccessible ?
**** DONE SmartSense: Ajouter des tests pour l'instanciation de *MqttInterface*
CLOSED: [2024-02-05 Mon 15:13]
- Comment se comporte l'objet si le message broker est inaccessible ?
**** DONE SmartSense: Prendre en main le noeud SmartSense
CLOSED: [2024-02-07 Wed 08:21]
- J'ai récupéré via Guillermo le noeud autonome que m'a setup Mickaël
- Le brancher et se connecter dessus pour mieux comprendre son fonctionnement
**** DONE SmartSense: Intégrer les modifications effectuées à la branche *main*
CLOSED: [2024-02-07 Wed 16:52]
- J'ai fini de mettre en place une première série de tests
- Avant d'aller plus loin et d'effectuer de nouvelles modifs
- Serait bien de me poser avec Mickaël pour reviewer mes branches et discuter de ce que j'ai fait
**** DONE SmartSense: Intégrer la branche *autonomous-node* à la branche *main*
CLOSED: [2024-02-07 Wed 16:53]
- Cette branche a l'air d'apporter des workarounds pour les problèmes constatés lors de l'utilisation du noeud SmartSense en mode autonome
- Notamment, exporte et reset la DB périodiquement pour éviter le mécanisme de compression des shards
  - Qui a l'air d'être trop couteux pour tourner sur le noeud
- Améliore aussi la lisibilité du code
**** DONE SmartSense: Préparer la branche *refactor/main-app* pour intégration
CLOSED: [2024-02-08 Thu 11:03]
- MàJ la branche par rapport à *refactor/split-mqtt-interface*
- Review les différents commits
  - Notamment l'instanciation de l'instance *MqttInterface* pour publier les données me semble incorrecte à la réflexion
    - Effectuée avant de set *mainApp.sectorBrokerPort*
**** DONE SmartSense: Explorer les données collectées par le noeud
CLOSED: [2024-02-08 Thu 15:15]
- Maintenant que j'ai un noeud avec l'application en route dessus
- Peut voir concrètement des samples de données collectées
- Peut m'aider à créer mon jeu d'essai pour les tests unitaires
**** IN-PROGRESS SmartSense: Ajouter des tests pour *MainApp.handleProvisioningMsg()*
- Vérifier le bon fonctionnement de cette méthode
- Me paraît un bon point d'entrée pour mettre en place des tests pour *MainApp*
**** IN-PROGRESS SmartSense: Intégrer la branche *refactor/split-mqtt-interface*
- Maintenant que la branche *feat/autonomous-node-and-add-tools* a été intégrée
- Peut commencer à intégrer les modifications dans mon backlog
- MàJ la branche *refactor/split-mqtt-interface* par rapport à *main*
- Faire la MR
*** Done
- SmartSense: Ajouter des tests pour l'instanciation de *DataToLocalInfluxDB*
  - Déclenche l'exception *requests.exceptions.ConnectionError* lorsque le SGBD est non-disponible
  - Voir comment le process se comporte lorsque l'erreur est levée
    - Est-ce qu'il s'arrête ?
    - Ou tombe dans un état zombie ?
  - Si le SGBD ne répond pas, doit-on stopper *MainApp* ?
- SmartSense: Ajouter des tests pour l'instanciation de *MqttInterface*
  - Même constat que pour *DataToLocalInfluxDB*
  - Juste un problème pour déterminer l'exception levée
    - Obtient un message *ConnectionRefusedError*
    - Mais n'arrive pas à importer de type correspondant
    - Le plus proche que je trouve est *MqttException*
      - https://github.com/eclipse/paho.mqtt.python/blob/47ab9b94d9d1408abd8717e6c2d052ca329d549b/src/paho/mqtt/__init__.py#L4
    - Mais l'utiliser en catch pas l'exception
- SmartSense: Ajouter des tests pour *MainApp.handleProvisioningMsg()*
  - Nécessite plusieurs modifications en amont
  - *handleProvisioningMsg* instancie et démarre le process publiant les données sur le broker une fois le message de provisioning reçu
    - Serait p-e préférable de découpler cette action de la réception du message
    - Et de démarrer ce process en même temps que les autres, i.e. dans *main()* après avoir été provisionné
  - Le constructeur de *MainApp* repose sur une config
    - Celle-ci doit se trouver à un emplacement spécifique
    - Peut facilement modifier le code pour qu'on passe le path de la config en paramètre
    - Mais rencontre la même erreur pour les autres modules
    - Ce qui mène au problème suivant
  - *MainApp* instancie beaucoup d'objets
    - Certains reposent sur la même config
      - Doit donc transmettre le path de la config
      - Mais pourquoi réinstancier la config à chaque fois ?
      - Et non pas partager l'objet de *MainApp* ?
    - Certains dépendent de services potentiellement pas en ligne
      - e.g. *NPTManager*, *OpenOCDManager* qui utilise Telnet
    - Quelle est la bonne approche pour tester *MainApp* ?
      - Rendre optionnel ces différents composants ?
        - En fonction de la config, instancier ou non les composants
      - Mocker ces différents composants ?
      - Décomposer *MainApp* en différents modules, chacun séparément facilement testable ?
    - Config
      - Ne constate pas de raison particulière à ces multiples instances
      - Modification du code pour en utiliser qu'une
    - Instanciation de *MainApp*
      - [ ] Sortir du constructeur l'instanciation des modules problématiques, i.e. les *Manager*
        - Et les mettre dans une méthode appelée immédiatement dans *main()*
        - Permet d'instancier *MainApp* dans les tests en s'en passant
        - Cette approche me semble par contre error prone
          - Faut savoir que *MainApp* n'est pas fonctionnelle à proprement parler tant que cette seconde méthode n'a pas été appelée
        - Après on retrouve déjà un peu ça avec *MainApp.go()*
        - Ou même que c'est *main()* qui sait faire fonctionner le module
      - [ ] Mise en place d'un *MessageRouter*
        - En soit, *MainApp* n'interagit pas directement avec les *Managers*
        - Se contente de lire des messages qu'ils émettent et de les transmettre/d'y répondre
        - Pourrait séparer la logique instanciation/utilisation
          - *MainApp* instancie les objets
          - Ainsi qu'un module en charge des messages
          - Configure ce dernier
          - Et le laisse ensuite gérer les messages
        - Reste à voir comment fournir les *Managers* et *Queues* respectives à ce composant sans trop le coder en dur
          - Après, c'est un trade-off qualité vs. complexité
      - [ ] Suppression du routing par *MainApp*
        - Une approche /Reactive Programming/ me paraitrait plus adaptée en fait
          - Chaque module s'abonne au flux de messages et traite ceux qui le concerne
        - Plus vraiment de routing à ce niveau là
        - Permet de découpler les différents modules de *MainApp*
          - Chorégraphie plutôt qu'orchestration
        - Un peu curieux par contre sur les perfs d'un flux unique
          - Et de la compatibilité avec une appli multi-process
        - Et comment faire évoluer l'appli, i.e. ajouter des couches de traitement
          - E.g. on parle de traiter les données collectées par un capteur
          - Et de ne transmettre/stocker que le résultat de ce traitement
            - Et non pas la donnée d'origine
          - Comment achieve cela ?
            - La couche publication qui filtre la donnée brute ?
            - Le capteur qui marque non-publiable la donnée brute ?
          - Dans tous les cas, me retrouve avec un couplage implicite
            - Un des deux modules sait qu'il existe un module de traitement
            - Même si on peut considérer que le couplage est faible
            - Puisque ne pas avoir le module de traitement fait juste que la donnée brute est ignorée/inexploitée
      - [ ] Mise en place de *Factories*
        - Puisque c'est l'instanciation qui pose problème
        - Serait p-e intéressant de voir si on peut mettre en place des *Factories*
        - Et les injecter à *MainApp*
        - De façon à pouvoir les mocker dans les tests
- SmartSense: Prendre en main le noeud SmartSense
  - Plusieurs services s'occupent de faire fonctionner l'application
    #+BEGIN_PLAIN
UNIT                         LOAD   ACTIVE SUB     DESCRIPTION
alsa-state.service           loaded active running Manage Sound Card State (restore and store)
avahi-daemon.service         loaded active running Avahi mDNS/DNS-SD Stack
collectd.service             loaded active running Statistics collection and monitoring daemon
cron.service                 loaded active running Regular background program processing daemon
dbus.service                 loaded active running D-Bus System Message Bus
dnsmasq.service              loaded active running dnsmasq - A lightweight DHCP and caching DNS server
getty@tty1.service           loaded active running Getty on tty1
grafana-server.service       loaded active running Grafana instance
influxdb.service             loaded active running InfluxDB is an open-source, distributed, time series database
mainApp.service              loaded active running Smartsense Python App
rng-tools.service            loaded active running rng-tools.service
rsyslog.service              loaded active running System Logging Service
serial-getty@ttyAMA0.service loaded active running Serial Getty on ttyAMA0
ssh.service                  loaded active running OpenBSD Secure Shell server
systemd-journald.service     loaded active running Journal Service
systemd-logind.service       loaded active running Login Service
systemd-udevd.service        loaded active running udev Kernel Device Manager
triggerhappy.service         loaded active running triggerhappy global hotkey daemon
user@1000.service            loaded active running User Manager for UID 1000
wpa_supplicant.service       loaded active running WPA supplicant
    #+END_PLAIN
  - mainApp.service
    - Décrit dans /etc/systemd/system/mainApp.service
    - Concrètement, execute la commande suivante : cd /home/root/App; python3 mainApp.py
  - collectd.service
    - Pas de description du service
    - Retrouve sa configuration dans le fichier /etc/collectd/collectd.conf
      #+BEGIN_PLAIN
FQDNLookup true
LoadPlugin syslog
<Plugin syslog>
	LogLevel info
</Plugin>
LoadPlugin battery
LoadPlugin cpu
LoadPlugin df
LoadPlugin disk
LoadPlugin entropy
LoadPlugin interface
LoadPlugin irq
LoadPlugin load
LoadPlugin memory
LoadPlugin network
LoadPlugin processes
LoadPlugin swap
LoadPlugin users
<Plugin df>
	FSType rootfs
	FSType sysfs
	FSType proc
	FSType devtmpfs
	FSType devpts
	FSType tmpfs
	FSType fusectl
	FSType cgroup
	IgnoreSelected true
</Plugin>
<Plugin network>
	Server "127.0.0.1"
</Plugin>
<Include "/etc/collectd/collectd.conf.d">
	Filter "*.conf"
</Include>
      #+END_PLAIN
    - Collecte des métriques sur l'état du système
  - influxd.service
    - Décrit dans /etc/systemd/system/influxd.service
    - Lance influxd avec comme fichier de config /etc/influxdb/influxdb.conf
      #+BEGIN_PLAIN
reporting-enabled = false
[meta]
  dir = "/data/influxdb/meta"
[data]
  dir = "/data/influxdb/data"
  wal-dir = "/data/influxdb/wal"
  index-version = "tsi1"
[coordinator]
[retention]
[shard-precreation]
[monitor]
[http]
  auth-enabled = true
[ifql]
[logging]
[subscriber]
[[graphite]]
[[collectd]]
  enabled = true
  bind-address = ":25826"
  database = "collectd"
  typesdb = "/usr/share/collectd"
[[opentsdb]]
[[udp]]
[continuous_queries]
[tls]
      #+END_PLAIN
    - Important : c'est la config d'influxdb qui active l'enregistrement des données collectées par collectd dans influxdb
      - Cf. https://docs.influxdata.com/influxdb/v1/supported_protocols/collectd/
    - Ça vaudra p-e le coup néanmoins de se pencher sur cette config
      - Peut-on désactiver le mécanisme de shard compression incriminé pour les crashs ?
      - La partie concernant collectd insiste sur le fait de mettre en place une bufferisation, y en a-t-il une par défaut ?
  - grafana-server.service
    - Instance disponible à : http://192.168.1.2:3000
    - Config disponible dans /etc/grafana/grafana.ini
    - Configuration par défaut
    - Mais disposait déjà de deux dashboards
      - Hardware Monitoring
        - Présente les métriques systèmes récupérées par collectd
      - Sensor Data
        - Présente les données récupérées par l'application
  - dnsmasq.service
    - D'après sa description, sert de DHCP
    - Suppose que c'est ce service qui permet au noeud de mettre en place le réseau local
  - Pose la question de comment est setup le noeud
    - De la façon dont Mickaël m'a présenté les choses, j'ai l'impression que c'est un setup manuel
    - Ne vois pas de repo avec un script Ansible ou bash pour installer et configurer le noeud
    - Serait intéressant de documenter la procédure de mise en place et de l'automatiser
    - Pose aussi la question d'installer les différentes applications à même le noeud
    - Ou de conteneuriser l'application
- SmartSense: Explorer les données collectées par le noeud
  - SensorData
    - Measurements
      #+BEGIN_PLAIN
> SHOW MEASUREMENTS
name: measurements
name
----
event_accelerometer
event_airInfo
event_asx340
event_audio
event_bf707
event_cc2650
event_consumption
event_esensor
event_gyroscope
event_magnetometer
event_rfGiga
event_rgbwLuminosity
event_sensor
event_stream
event_telemeter
event_thermal
event_thermalImgHd
event_thermalImgLd
event_uvLuminosity
event_uwb
meta_accelerometer
meta_airInfo
meta_airQuality
meta_asx340
meta_audio
meta_bf707
meta_cc1310
meta_cc2650
meta_consumption
meta_esensor
meta_gyroscope
meta_leds
meta_magnetometer
meta_rfGiga
meta_rgbwLuminosity
meta_sensor
meta_stream
meta_telemeter
meta_thermal
meta_thermalImgHd
meta_thermalImgLd
meta_uvLuminosity
meta_uwb
meta_uwbCfg
meta_uwbData
      #+END_PLAIN
    - event
      - accelerometer
        #+BEGIN_PLAIN
    > SELECT * FROM event_accelerometer LIMIT 10
    name: event_accelerometer
    time                admin board          location mcu     mode       node              sector subsector x   y   z
    ----                ----- -----          -------- ---     ----       ----              ------ --------- -   -   -
    1683307383833892224 p     sensorBoard    External sensor  production b8:27:eb:8c:67:48 1      1         -5  -34 977
    1683307384111892224 p     externalBoard2 External esensor production b8:27:eb:8c:67:48 1      1         -12 480 872
    1683307384814892224 p     sensorBoard    External sensor  production b8:27:eb:8c:67:48 1      1         -2  -37 976
    1683307385102892224 p     externalBoard2 External esensor production b8:27:eb:8c:67:48 1      1         -12 479 874
    1683307385798892224 p     sensorBoard    External sensor  production b8:27:eb:8c:67:48 1      1         -4  -32 977
    1683307386094892224 p     externalBoard2 External esensor production b8:27:eb:8c:67:48 1      1         -11 477 874
    1683307386777892224 p     sensorBoard    External sensor  production b8:27:eb:8c:67:48 1      1         -4  -36 977
    1683307387086892224 p     externalBoard2 External esensor production b8:27:eb:8c:67:48 1      1         -10 479 874
    1683307387758892224 p     sensorBoard    External sensor  production b8:27:eb:8c:67:48 1      1         -2  -35 975
    1683307388082892224 p     externalBoard2 External esensor production b8:27:eb:8c:67:48 1      1         -12 478 872
        #+END_PLAIN
      - airInfo
        #+BEGIN_PLAIN
    > SELECT * FROM event_airInfo LIMIT 10
    name: event_airInfo
    time                admin board          humidity location mcu     mode       node              pressure  sector subsector temperature
    ----                ----- -----          -------- -------- ---     ----       ----              --------  ------ --------- -----------
    1683307383266892224 p     sensorBoard    34.86    External sensor  production b8:27:eb:8c:67:48 101613.41 1      1         33.71
    1683307383716892224 p     externalBoard2 34.24    External esensor production b8:27:eb:8c:67:48 101450.65 1      1         26.01
    1683307384259892224 p     sensorBoard    34.86    External sensor  production b8:27:eb:8c:67:48 101610.69 1      1         33.71
    1683307384708892224 p     externalBoard2 34.24    External esensor production b8:27:eb:8c:67:48 101451.36 1      1         26.03
    1683307385240892224 p     sensorBoard    34.86    External sensor  production b8:27:eb:8c:67:48 101612.33 1      1         33.72
    1683307385703892224 p     externalBoard2 34.25    External esensor production b8:27:eb:8c:67:48 101453.19 1      1         26.03
    1683307386221892224 p     sensorBoard    34.87    External sensor  production b8:27:eb:8c:67:48 101612.33 1      1         33.72
    1683307386694892224 p     externalBoard2 34.23    External esensor production b8:27:eb:8c:67:48 101456    1      1         26.01
    1683307387201892224 p     sensorBoard    34.86    External sensor  production b8:27:eb:8c:67:48 101605.24 1      1         33.71
    1683307387686892224 p     externalBoard2 34.23    External esensor production b8:27:eb:8c:67:48 101457.83 1      1         26.01
        #+END_PLAIN
      - airQuality
        #+BEGIN_PLAIN
    > SELECT * FROM event_airQuality LIMIT 10
    name: event_airQuality
    time                admin board          co2  location mcu     mode       node              resistance sector subsector tvoc
    ----                ----- -----          ---  -------- ---     ----       ----              ---------- ------ --------- ----
    1683307383428892224 p     externalBoard2 450  External esensor production b8:27:eb:8c:67:48 197033     1      1         125
    1683307383970892224 p     sensorBoard    3699 External sensor  production b8:27:eb:8c:67:48 89100      1      1         1020
    1683307384419892224 p     externalBoard2 450  External esensor production b8:27:eb:8c:67:48 196837     1      1         125
    1683307384951892224 p     sensorBoard    3699 External sensor  production b8:27:eb:8c:67:48 89100      1      1         1020
    1683307385411892224 p     externalBoard2 450  External esensor production b8:27:eb:8c:67:48 196837     1      1         125
    1683307385931892224 p     sensorBoard    3699 External sensor  production b8:27:eb:8c:67:48 89100      1      1         1020
    1683307386403892224 p     externalBoard2 450  External esensor production b8:27:eb:8c:67:48 196837     1      1         125
    1683307386913892224 p     sensorBoard    3699 External sensor  production b8:27:eb:8c:67:48 89100      1      1         1020
    1683307387395892224 p     externalBoard2 450  External esensor production b8:27:eb:8c:67:48 197033     1      1         125
    1683307387894892224 p     sensorBoard    3699 External sensor  production b8:27:eb:8c:67:48 89100      1      1         1020
        #+END_PLAIN
      - asx340
        #+BEGIN_PLAIN
> SELECT * FROM event_asx340 LIMIT 10
name: event_asx340
time                admin board       energy estimation estimation_max estimation_min image_sub                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         location mcu   mean   mode       node              occupation sector subsector variance
----                ----- -----       ------ ---------- -------------- -------------- ---------                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         -------- ---   ----   ----       ----              ---------- ------ --------- --------
1683318911060120064 p     sensorBoard 6133   0          0              0              @AAAABIgABEQACEAAAAQAAAAABAAABEQABAAABEAABAAAAAABRAAAAAAABIAABEQAAAQABEAAAAAABAAABAQAAAAAAEAEAEQAAAAAAAABAAAABEAAAAQAAEQAAEQABAAABEQAAEQAAEQABMAAAEAAAAAAAgQAEIAABAAAAEAAAIQABAQABEAABAQABAQAB8QAAAAABAQABEQAAAAAAcAAAAQACAAAAEQAAEAAAAAAAAAAAAAABAAAAEAABEAAAAAABAAAAAAAAAQABMAAAEQABEQABAAABEAAAAQABAAAAAAAAEQAAAAACIQABEwAGEAABEAAAEQABAQABAAAAAQABAQAAAAABAQAAAAABEAACEAARkCBBAQABEQAAEQABAAAAEQAAAQAAAQABEAABAAAAAQABAAABAQAXIhHAIgDCEAAAEQABEQAAAQAAEQAAAAABEAABAQAAEQABEAAAEQAK8QAAEQAAEAACEgABAAAAAQAAAQAAAAABEAAAEQAAAQABEAAAEQAAEAAAAQAAEAABEAABEQAAEQAAQQABAAAAEAABEAAAAAAAAAAAEAAAAQAAEQAAEAABAQABEQACEAABAQAAEQABEQABAQABEAAAEAABEAAAEQABEQABEAABAQAAAQABEQABAAAAAAAAAAAAAAAAEAABEAAAAQABAAABAQAAEAABAQABEAABAQAAAQABAQABEAAAEAABEAAAEAABAQAAAAAAAAAAEAABAAABAAAAEQAAAQAEAQABAQABEAAAAAABAQAAAQAAAAABEAAAAQABAQAAEQABEQABAQABAQABEQCAAQAAEQABAQABAAADEQAAEAABEAAAEAABAAABEAABEAABEQABAQABAAAAAQAAEAAAAQAAAAAAEAABEAAAEAAAEAABEAABIQABEQAAEQAAEQAAAQAAEAAAEQAAEAAAAAABEAABEwAAcgABAAABEQAAEQAAEQABEQABEAAAEAAAEQAAAAAAAAABEAAAAQAAAAAAAQABEAAAEQAAEQAAAQAAAAAAAAAAAQAAAQAAAAAAEAAAAAABEAACAAAAggABEQABEAAAQgABEAAAAAAAAQABAAAAAQAAMAAAAAAAAAAAAQAAEAAAIAAAAAABAAAAEQAAEQAAEQAAAAAAAAAAAAAAAAABAAAAAAAAEQABEgAAEAAAAAABAAAAAQAAAAAAEAAAAAAAEAAAAAAAAAAAAAAAEQABAAABEQAAEAABAQAAAQABEAAAAAABEAAAAQAAAAAAAAAAAAAAAAAAEAAAEQABAQABEAABAQABAAAAAAAAAAAAAAAAAAAAAAABAAABAAAAAAABEAABEAAAAQABEAAAEAAAAAABAAAAAQAAEAAAEAABQAAAEAABAAABEAABEQAAEgABEAABEQABEQAAEQAAAAAAEAAAAAAAEAABAAAAEAAAAQABAAABEQAAEQABAAABAAAAEQAAAAABAQAAAAAAEAABAAAAAAABAAABAgABAgABAQABAAABAQABEQAAAAAAAQABEQAAAAABEQABEAAAEAABAAAAAAABEQABAQAAAQABAAABEAAAAAAAAQABAQAAAAAAAQAAEAAAAAA External bf707 0.7099 production b8:27:eb:8c:67:48 1          1      1         4.5959
1683319143060120064 p     sensorBoard 25922  0          0              0              @AAAABAAAAEAABAAABAAAAAAAAAQAAAQAAAQAAAAABEAABAAAAEQAAAAABAAAAAQAFAAAAAAAAAAABAQABAAAAAAAAAQAAAQAAAAAAAAAAAAEBAAABAAAAEAAAEAAAEAAAEAABAAABEAAAEQAAAAAAAAAAAQABEQAAAAABAQAAAQAAAAAAAAAAAAABAAAAEAABEAAAAAAAAAABAAAAAAQAAQAAAQAAAAAAEAABAAABEAAAAQAAAAAAEQABAQAAAAAAEQAAAQAAAwDAAQAAEAAAEAAAAAAAAAAAAAAAAQABAAAAAQAAAAABAQAAEwAAAQAAAAAAAQAAEAAAAQAAAAABAAABEAAAAQAAAQAAAAACEAAQkCBAAQAAEABBEAAAAAAAAAAAAAABAAAAEAAAAAAAEQAAAQABEQAWAhHAEADBAAABEAAAEAABAAAAAQAAAAAAEAAAAAAAEAABEQABEAAJ4QAAAQAAAAABEAAAAAAAEAABAAAAEAAAAAABAAAAAQAAAAAAAAAAAQAAAAAAAQAAEQABAQABEAABEQAAAAAAAQAAEAAAAAABAAAAEAAAAAAAAAAAAAAAAAAAAQAAEQAAAAAAEAAAAAAAEAABAAAAAQAAAAAAAAAAAAAAAQAAAAAAAAAAEQAAAAAAAAABEAAAAAAAAQABAQAAEAAAAAAAEAAAAAAAAAABAAAAAQAAEAAAAAAAAAABAAAAAAABAAAAEAAAIAAAIAAAEQAAAAABAAABEAABEAABEQAAEAABAAABEAAAEAAAAAAAAQABAQAAEAAAAAAAAAABEAAAEAAAAAAAEQAAAQAAAQAAEAAAAAAAAQAQAQAAAQBBAQAAAQAAEAAAAAAAAAABEAAAEQAAAAAAAQAAAAAAAAAAAAAAAAAAAAAAAQAAAQABAAAAAAAAEAAAEAAAAAAAAQABAAAAAQABAAABEQAAAAAAgwABAQAAEAAAEQABAAAAAQAAEAAAAAAAAAAAAAAAAQAAAQAAAABAAQAB8nAAAAAAAQAAEQAAAQAAAAAAAAABAAAAAAAAAAAAAAABAAAAAAACAAAAgQAAAAAAEAAAEAAAAAAAAAAAAQAAEQAAAAAAEAAEAAAAAQABAAAKAAAAEQAAAgAAAAAAAQAAAQABEQAAEQBAAQABAAAAEAAAAAAAAAAAEAABEAAAEQAAAQABAQAAAAAAAAABAAABAAACAAAAEAAAAAAAAAAAEAABAQAAEAAAAAAAAQAAAAAAEAAAAAAAAAAAAAAAAAAAEAABAAAAAABAAQACAAABEAAAAAAAAAAAEQAAAAAAAAABEAAAEQAAAgAAAABAQAAAAAAAAQAAAQAAAQABAAABEQAAEQAAAQAAAAABAAAAAAABQQAAAAABAQAAAAAAAAABAAAAAAAAAQAAAAAAEAAAAAAAAAAAgQADEAABAAAAEAABAAABAQAAAAAAAQAAAAAAAAAAAAABEQAAAAAAAAAAAAAAEACAAAAAAAABEQABEAAAAAAAAAAAAQAAAAAAEQAEAAACAAAAAAAAAgAAAQAAAAABAQABEAAAAAAAEAAAAAAAEQAAAQAAAQAAEAABAAAAEAABAAAAAAAAAAA External bf707 0.69   production b8:27:eb:8c:67:48 1          1      1         21.1238
        #+END_PLAIN
      - audio
         #+BEGIN_PLAIN
> SELECT * FROM event_audio LIMIT 10
name: event_audio
time                admin board       location mode       node              sector subsector synchronize
----                ----- -----       -------- ----       ----              ------ --------- -----------
1683318692060120064 p     sensorBoard External production b8:27:eb:8c:67:48 1      1         OK
        #+END_PLAIN
      - bf707
        #+BEGIN_PLAIN
> SELECT * FROM event_bf707 LIMIT 10
name: event_bf707
time                admin board       location mode       node              sector subsector synchronize
----                ----- -----       -------- ----       ----              ------ --------- -----------
1683318691060120064 p     sensorBoard External production b8:27:eb:8c:67:48 1      1         OK
        #+END_PLAIN
      - cc2650
        #+BEGIN_PLAIN
> SELECT * FROM event_cc2650 LIMIT 10
name: event_cc2650
time                admin board       location mode       node              sector subsector synchronize
----                ----- -----       -------- ----       ----              ------ --------- -----------
1683318691060120064 p     sensorBoard External production b8:27:eb:8c:67:48 1      1         OK
        #+END_PLAIN
      - consumption
        #+BEGIN_PLAIN
    > SELECT * FROM event_consumption LIMIT 10
    name: event_consumption
    time                A       V      W        admin board    location mcu       mode       node              sector subsector
    ----                -       -      -        ----- -----    -------- ---       ----       ----              ------ ---------
    1683307383096313088 8.5     5      43.126   p     rpiBoard External inaExt1   production b8:27:eb:8c:67:48 1      1
    1683307383197403904 106.003 5      530.641  p     rpiBoard External inaExt2   production b8:27:eb:8c:67:48 1      1
    1683307383399406848 669.989 3.288  2168.816 p     rpiBoard External inaRpi    production b8:27:eb:8c:67:48 1      1
    1683307383601250048 722.491 11.712 8212.751 p     rpiBoard External inaPoe    production b8:27:eb:8c:67:48 1      1
    1683307384004352000 755.492 4.988  3738.864 p     rpiBoard External inaSensor production b8:27:eb:8c:67:48 1      1
    1683307384105933824 8       5.004  43.126   p     rpiBoard External inaExt1   production b8:27:eb:8c:67:48 1      1
    1683307384207138048 102.003 5.004  525.641  p     rpiBoard External inaExt2   production b8:27:eb:8c:67:48 1      1
    1683307384409869056 588.987 3.292  1831.931 p     rpiBoard External inaRpi    production b8:27:eb:8c:67:48 1      1
    1683307384611720192 661.489 11.748 7800.238 p     rpiBoard External inaPoe    production b8:27:eb:8c:67:48 1      1
    1683307385014542080 752.492 4.992  3766.365 p     rpiBoard External inaSensor production b8:27:eb:8c:67:48 1      1
        #+END_PLAIN
      - esensor
        #+BEGIN_PLAIN
> SELECT * FROM event_esensor LIMIT 10
name: event_esensor
time                admin board          location mode       node              sector subsector synchronize
----                ----- -----          -------- ----       ----              ------ --------- -----------
1683318691060120064 p     externalBoard2 External production b8:27:eb:8c:67:48 1      1         OK
        #+END_PLAIN
      - gyroscope
        #+BEGIN_PLAIN
    > SELECT * FROM event_gyroscope LIMIT 10
    name: event_gyroscope
    time                admin board          location mcu     mode       node              sector subsector x  y  z
    ----                ----- -----          -------- ---     ----       ----              ------ --------- -  -  -
    1683309474180892224 p     sensorBoard    External sensor  production b8:27:eb:8c:67:48 1      1         0  -1 -1
    1683309474502892224 p     externalBoard2 External esensor production b8:27:eb:8c:67:48 1      1         0  0  0
    1683309475161892224 p     sensorBoard    External sensor  production b8:27:eb:8c:67:48 1      1         0  -1 -1
    1683309475493892224 p     externalBoard2 External esensor production b8:27:eb:8c:67:48 1      1         -1 0  0
    1683309476143892224 p     sensorBoard    External sensor  production b8:27:eb:8c:67:48 1      1         0  0  0
    1683309476484892224 p     externalBoard2 External esensor production b8:27:eb:8c:67:48 1      1         -1 0  0
    1683309477125892224 p     sensorBoard    External sensor  production b8:27:eb:8c:67:48 1      1         -1 0  0
    1683309477476892224 p     externalBoard2 External esensor production b8:27:eb:8c:67:48 1      1         0  0  0
    1683309478106892224 p     sensorBoard    External sensor  production b8:27:eb:8c:67:48 1      1         0  -1 -1
    1683309478468892224 p     externalBoard2 External esensor production b8:27:eb:8c:67:48 1      1         0  -1 0
        #+END_PLAIN
      - magnetometer
        #+BEGIN_PLAIN
    > SELECT * FROM event_magnetometer LIMIT 10
    name: event_magnetometer
    time                admin board          location mcu     mode       node              sector subsector x   y  z
    ----                ----- -----          -------- ---     ----       ----              ------ --------- -   -  -
    1683309474181892224 p     sensorBoard    External sensor  production b8:27:eb:8c:67:48 1      1         8   48 26
    1683309474335892224 p     externalBoard2 External esensor production b8:27:eb:8c:67:48 1      1         -25 -3 -72
    1683309475161892224 p     sensorBoard    External sensor  production b8:27:eb:8c:67:48 1      1         10  48 26
    1683309475327892224 p     externalBoard2 External esensor production b8:27:eb:8c:67:48 1      1         -24 -3 -73
    1683309476143892224 p     sensorBoard    External sensor  production b8:27:eb:8c:67:48 1      1         10  48 26
    1683309476318892224 p     externalBoard2 External esensor production b8:27:eb:8c:67:48 1      1         -25 -3 -73
    1683309477125892224 p     sensorBoard    External sensor  production b8:27:eb:8c:67:48 1      1         8   47 26
    1683309477310892224 p     externalBoard2 External esensor production b8:27:eb:8c:67:48 1      1         -24 -3 -73
    1683309478106892224 p     sensorBoard    External sensor  production b8:27:eb:8c:67:48 1      1         9   47 26
    1683309478301892224 p     externalBoard2 External esensor production b8:27:eb:8c:67:48 1      1         -25 -3 -73
        #+END_PLAIN
      - rfGiga
        #+BEGIN_PLAIN
    > SELECT * FROM event_rfGiga LIMIT 5
    name: event_rfGiga
    time                admin board       location mcu    mode       node              rssi1                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             sector subsector
    ----                ----- -----       -------- ---    ----       ----              -----                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             ------ ---------
    1683309474101892224 p     sensorBoard External cc2650 production b8:27:eb:8c:67:48 @nmqpb25nbuJnYWJmWWJmYOJoYWJmYOplVWZlVWpmTK5lmiplVOJkVK5lXiZlTKJmSWJmWWZlSWplTm5jTWZlWOpkYa5kVaZlkO5lWeZlVaplVOJkTOpkTiplWW5kVWpkVOplViZlYWZlXO5krWZltKpkVOplSC5jVCpkTWpkSWZlWOZlTyokWiZlXOZlTOJkWepkT+YlWW5kTialWa5kTaZlVm5kVO5jT25kTC5kVK5kVeZlXCpoQaZlSO5kVW5jQW5kPKZlSO5kcKplTy4lVWpmVKZlWCZlTW5kSOZlSOpkWC5kSKZlVOplYOpkWOZlTWJkSWZlPOpkTWZlVWpkPK5kWa5kZWJmVeZlXi5nSO5kSWZlPC5kQKZlVWZlfuZlWOZlWWplVKZnVW5kTWZlT+olTa5kTWpkSWpkVCJkQWpkVWZlYKpkQWplTO5kVepmTWpkVC5kVO5lTKZlSW5kW+YlTaZlVWpkSCpkVOplVOpkTOplVWZlVWpkSOJmTK5kVWJmTCpkSOZlVWZlZa5kVOpkTWZlT+YlX+IkSOJkV+olSO5kQO5kWWZlWaJmVW5lTWZlVWpkSa5kVaZlVKJmVO5kVWZlWaplSaZlnOplTOpkSWJmTO5kZWZlYO5kQW5lWyqlVa5jWiplWWZlSO5jPOZlSW5kT+YlVeZmYeJmXKpkWuZlYKJmQaZlVy4kViZpWepmWi5naiKrVuZlQO5kV2ZnamZnYi5kWmpmZiJmWqJmYqJnVqJmYyZlYWJmZi5lSO5mWWZlVWZlYOpkTaZlPOpkVWZlYC5kTWZmSiZlSOZlVOZlTW5kTWZlVqJmVKZlZW5mWaJmWaZlWa5muWplXOZlQaZlbe5jW25lXiJmZ2ZmYWZlVqpmWO5kVipmYWpl 1      1
    1683309475955892224 p     sensorBoard External cc2650 production b8:27:eb:8c:67:48 @lm6pZuJoai5mba5mVaJmYiJmYu5lbWZmaqZnaiJmcuJmdyJmbiZmVipmauJnZqJmYiZnZqJnYq5mai5mcyJnZqpmYmZlXepmVm5nbaZnWuZmZqplZiJmVW5lWKZlSa5kZOZlnO5kYiZlYiZlVmJmYK5kVq5kYWZlWyZlYCZlVeZlWWplVqpkWq5nWi5mZiJmTiJmYq5lbq5mZiqkWeZmTWZlXuJkTOZlSK5lWapkVyYlaiJmRK5lQO5jWWZlOWZlV+YmVWZlTOZlca5kYeZlTWJmVKZlSe5kQqJkQO5kTKplRK5kTW5kZGalVWZlViZlTupkVK5kVOJkSW5kYqZlWK5mTO5kWaJmVi5mXKplbWZlVa5kVeZlVaZlVy5jQaplVOZlYmJoQa5lVeJmdOZlT6pmWaZlYW5kYW5lVaJmVmZlWO5lTi5kVWpkWq6jWOZlWeZlgWJmSWZmVaJmVCplSiplPW5jTKZlTOplSmplYapkaa5kTaJmWW5jViplWaZmTKZlYmJmTWJnbu5lQW5kYaplWi5lTq5kYaZlVOplT+4kZW5kWa5jViJmXOZlbu5mQaZlZqJmbqJmSKplVOplPW5mW25lYaplYW5kmWZlai5kaa5lSW5kSmJmWWpmW25oWi5lSaZlWaJkWaZlSm5kVK5kSKZlWi5kXapkYmZlTKJmXiJmWOplTiJmdK5kQWpkVWplYWZlVW5lVWZlYmplZW5lV+YlTmplTWpkZaZlTWZlVuJmVWZmSaZlTaZlWWZlVaZmVOplVWZlYOZlXiZlVapmYO5kaiZlXWZlVO5kTWZlSWZlVa5kYW5kYipmVaZlWeplVmZlVOJmVa5kSeplUq5kXW5jYipkVuJmTeZlVeJmWqZmVi5lXa5kSiJmXaZm 1      1
    1683309477809892224 p     sensorBoard External cc2650 production b8:27:eb:8c:67:48 @mi6pd+ZocuZmYiZnYa5kYW5mXaZnTmJmZiJmXWZlWW5kVWJmTOpkZeZlWaJmVm5lWi5mayJmWi5lWiplVWplXqplgipkWWplVi5lYaZlYaplTeJmWiJmPi5lVeJmWiZlYqplYipmVKZlZaJmVWJmWm5kTOplWeplSOZlZipkYq5mVWJmaqJnbqJmceZmYWZmaWZlbWJmXOZmVealViplbqZlYypmdiJmSW5kY2ZlZW5kVm5lSaJmZipkYOplVKJmWWZmSWJmVOZleiZlYmZlXi5mXm5mYWpmVi5mYCpkViJmWOZlVa5lVW5lSmZmViZmViplYe5maqJmZiplZqJnZaZlZiJmbuJmYe5mZi5lbWJmWeplYipmXq5lauZmYmpmauZma2ZmciJmZWJmYipmYaJmaqJmXmJmZipmhuplcaZmYiJnXi5mai5mbaZmYWJmTiJmZOJmbepmXuZmZaJnYipmhiZmZiJmamJmYupmbqZmaupmZapoYq5maqJmauZmau5mZm5mcqpmdmZnbm5lY2plbqpmZqpmYyZmTqJmbqplYmrmYiJmXeJnaeJmdm5maC6mYuZmZqpmYiZmXm5maqpmau5mXKZmWuplnyJmZa5mZuJmauJmWe5mauZnYu5lampmcmpmYuZmaipkZe5maapmbiZlYq5lYipmZupmZmKmYapmYeJmZiZmYeZmWmJmViZmaWZmam5mSaZmYmJmWeplcqZmTqZlaiJmbiJmaiJnXiJmZiJmYaZmZiJmYaplYmZnZqplZaplamJmZiZmYmJmZi5lbWZmZu5lbmJmYiJmYWJmYiJmcmJmZiJnaqqmVWZmbepmWiZmTipmYWpmXaZmYWJmeiplXuJndipmaqJmdi5mVi5mam5mai5lXu5m 1      1
    1683309479663892224 p     sensorBoard External cc2650 production b8:27:eb:8c:67:48 @mqKpeK6ofq5meuJnbqJmduJncuJmb2JnaqJnZqJme2JnbypmYiZmdypmZyJncqZmYyZnXuZldqpmbiZlbqplcupma25lbqJmaiplduJmbepmcqZmaeZmYiZmYipmYupmdypmamZnai5mYy5mZqpmbupmamJmbiJncmJmaiZnYiJmjyZmXu5mV25mZmZmWaZmciZlWiZlauZmZeqmaqpmbiJmcqZmbyZmZepmZqJmamJmYq5maupmWmJmae5lXupmWeZnYu5maWZleapmai5mZi5mYipmZKplTq5mcuJnbq5mbyplWq5mdipmbmJnauJncaJnbmJmdi5lZWpmYqJmYi5mbeZnZyJtam5mcu5mYiplYiJmXq5kbqJnbuZmXiZlVm5lWqpmamJmYq5mWiJmbaJnZqpmZmpmW2plWm5mYiZmna5mYq5laOpmTeZmYWJmYWpmSe5lWqplZiZlZWJmYqJmXaZmamZlhWJmYmJmbi5lWWppYuZmaiJmWy5layZmYuZmYmplViJmYaJmYqpmXqJncupmce5lZe5maqJmbiJmZmJmciZlbeJmZyZnZuJmau5mZmJmaaplYiJmbq5mWeZmcm5lZipmYiJnmuJmZi5maaplXK6mWuplYmJmZe5mau5mY2JmZW5lYiZmWm5lampmYuJmaaJmZaplYq5mXm5lbmJmZuZmYmpmaepkYeZmYe5mYWpmYeZnampmZWZma+pmYi5lYmZmYqJmZaplYqZlam5laqplWm5lYWplViJmamJmVaJmViJmaa5lZWZmaipmamZlamJmaqplaepmVm5lVaJnWipmZaJmZqZmbi5mVWZmYqplYKqmbiJmWeZmaaJmXuZlYqpmcuJmaOJmZmZmaipmZupmauJmZmplYuZl 1      1
    1683309481517892224 p     sensorBoard External cc2650 production b8:27:eb:8c:67:48 @nmapbC6ocWJnau5mdqJmaupmbapmYmZrZ2pmY2JmauJmeuJnYmZmWCamam5mYqpmYipmaiJnbmpmfqJmcqJnZmJma2Zmbm5lcipmZyZnY2ZmduJnVupmZq5mjipmba5mbuJnVqpmdq5mbuZnbmplYqJmYiZnYiJnb25mXyJmViKmYa5paqplbe5mau5ka2JnQqpmfqpmbipmceqmaWpldqJmaqJmZmpmaqJmYuJmcqZmZmJnZupmdupmZe5mamJmVyZnbuJmdmpmfi5mZWZnbiplaeJmai5mceZmaq5mYqJmZqpmXuZnYyZmauZnYqJmXypmYm5oXWpmZqZlYWZmVqJmWipmXi5mWqpmTuplauZmYipmYm5lamJmaeZmYa5mYuJmem5mYqZmYiZmWiplam5mZupldqZmamplYyJmZiZmbi5oZm5mZiZmbipmYOJmZm5mXWJmaqplaaZmbe5mWWpkXipmYiJmYmZmYq5lYmZmYiplbqpmZm5lY2JmVeplVa5mWKKmai5kYiJmWWpmZiJnYe5mZqZlYmZmZuJmQqZmSiJmWiJmZuZmZm5mcWJmYq5maipmWuplYqJmZiZmaeJnVy5laiJnbmpklq5mbqpmZi5mbupmYuJmZiJmZiJmduplXmJmZypmYuZmYu5laiplZeZmpqZmYiJmbqJmceZmWWJmYq5mYiplYiZmZiJmZiZlay5mWuZmZipkYOplYWJmWq5maiplYm5mYapmXiplaa5mXiplZiJmYaZlZiplYipmZi5lZmplYipmbmJmYipmXaJmeaZmXi5laiZmXqplZiJmZWZmYuJmYypmaiJmaqpmbq5mZm5mZepmYqJmYiJmYqJmYqpmcyZnZqpmXmZlWqZnau5lYiZmbq5lYqZn 1      1
        #+END_PLAIN
      - rgbwLuminosity
        #+BEGIN_PLAIN
    > SELECT * FROM event_rgbwLuminosity LIMIT 10
    name: event_rgbwLuminosity
    time                B    G    R    W    admin board          location mcu     mode       node              sector subsector
    ----                -    -    -    -    ----- -----          -------- ---     ----       ----              ------ ---------
    1683309473963892224 1783 3493 3645 6421 p     externalBoard2 External esensor production b8:27:eb:8c:67:48 1      1
    1683309474514892224 1636 3749 3541 5868 p     sensorBoard    External sensor  production b8:27:eb:8c:67:48 1      1
    1683309474955892224 1783 3488 3642 6418 p     externalBoard2 External esensor production b8:27:eb:8c:67:48 1      1
    1683309475495892224 1637 3754 3545 5875 p     sensorBoard    External sensor  production b8:27:eb:8c:67:48 1      1
    1683309475947892224 1785 3496 3645 6426 p     externalBoard2 External esensor production b8:27:eb:8c:67:48 1      1
    1683309476476892224 1637 3751 3543 5872 p     sensorBoard    External sensor  production b8:27:eb:8c:67:48 1      1
    1683309476938892224 1785 3498 3646 6427 p     externalBoard2 External esensor production b8:27:eb:8c:67:48 1      1
    1683309477458892224 1639 3753 3544 5876 p     sensorBoard    External sensor  production b8:27:eb:8c:67:48 1      1
    1683309477930892224 1786 3499 3647 6428 p     externalBoard2 External esensor production b8:27:eb:8c:67:48 1      1
    1683309478440892224 1635 3747 3544 5874 p     sensorBoard    External sensor  production b8:27:eb:8c:67:48 1      1
        #+END_PLAIN
      - sensor
        #+BEGIN_PLAIN
> SELECT * FROM event_sensor LIMIT 10
name: event_sensor
time                admin board       location mode       node              sector subsector synchronize
----                ----- -----       -------- ----       ----              ------ --------- -----------
1683318692060120064 p     sensorBoard External production b8:27:eb:8c:67:48 1      1         OK
        #+END_PLAIN
      - stream
        - Trop lourd
      - telemeter
        #+BEGIN_PLAIN
    > SELECT * FROM event_telemeter LIMIT 10
    name: event_telemeter
    time                admin board          data location mcu     mode       node              sector subsector
    ----                ----- -----          ---- -------- ---     ----       ----              ------ ---------
    1683309473840892224 p     sensorBoard    3    External sensor  production b8:27:eb:8c:67:48 1      1
    1683309473902892224 p     externalBoard2 1867 External esensor production b8:27:eb:8c:67:48 1      1
    1683309474829892224 p     sensorBoard    3    External sensor  production b8:27:eb:8c:67:48 1      1
    1683309474893892224 p     externalBoard2 1861 External esensor production b8:27:eb:8c:67:48 1      1
    1683309475810892224 p     sensorBoard    4    External sensor  production b8:27:eb:8c:67:48 1      1
    1683309475885892224 p     externalBoard2 1863 External esensor production b8:27:eb:8c:67:48 1      1
    1683309476791892224 p     sensorBoard    4    External sensor  production b8:27:eb:8c:67:48 1      1
    1683309476877892224 p     externalBoard2 1864 External esensor production b8:27:eb:8c:67:48 1      1
    1683309477773892224 p     sensorBoard    3    External sensor  production b8:27:eb:8c:67:48 1      1
    1683309477868892224 p     externalBoard2 1867 External esensor production b8:27:eb:8c:67:48 1      1
        #+END_PLAIN
      - thermalImgHd
        - Trop lourd
      - thermalImgLd
        #+BEGIN_PLAIN
    > SELECT * FROM event_thermalImgLd LIMIT 1
    name: event_thermalImgLd
    time                admin board       image                                                                                                                                                                         location mcu     mode       node              sector subsector thermistor
    ----                ----- -----       -----                                                                                                                                                                         -------- ---     ----       ----              ------ --------- ----------
    1683309474354892224 p     sensorBoard @PBATAsEASBgTAEFASBQVA4EAQBAUA8EAWBwUAsFANBgVA8EAQBQUAMFAYBQVAgFAPBQUA4EATBwTAMFAXBgVA8EAOBgTA8EATBwUAcFAQBwVA4EAQBwTAIFAUBQUAQFAPBwTAwEATBwTAQFAWBwUAQFAPBQUAAFAQBgVAIFAAAQV External thermal production b8:27:eb:8c:67:48 1      1         743
        #+END_PLAIN
      - uvLuminosity
        #+BEGIN_PLAIN
    > SELECT * FROM event_uvLuminosity LIMIT 10
    name: event_uvLuminosity
    time                UVA UVB UVCOMP1 UVCOMP2 admin board          location mcu     mode       node              sector subsector
    ----                --- --- ------- ------- ----- -----          -------- ---     ----       ----              ------ ---------
    1683309473552892224 32  39  9       0       p     externalBoard2 External esensor production b8:27:eb:8c:67:48 1      1
    1683309473771892224 21  25  5       0       p     sensorBoard    External sensor  production b8:27:eb:8c:67:48 1      1
    1683309474543892224 32  39  9       0       p     externalBoard2 External esensor production b8:27:eb:8c:67:48 1      1
    1683309474752892224 20  25  5       0       p     sensorBoard    External sensor  production b8:27:eb:8c:67:48 1      1
    1683309475535892224 32  39  9       0       p     externalBoard2 External esensor production b8:27:eb:8c:67:48 1      1
    1683309475733892224 21  25  5       0       p     sensorBoard    External sensor  production b8:27:eb:8c:67:48 1      1
    1683309476527892224 32  39  10      0       p     externalBoard2 External esensor production b8:27:eb:8c:67:48 1      1
    1683309476715892224 21  25  5       0       p     sensorBoard    External sensor  production b8:27:eb:8c:67:48 1      1
    1683309477518892224 32  39  10      0       p     externalBoard2 External esensor production b8:27:eb:8c:67:48 1      1
    1683309477697892224 20  25  4       0       p     sensorBoard    External sensor  production b8:27:eb:8c:67:48 1      1
        #+END_PLAIN
      - uwb
        #+BEGIN_PLAIN
> SELECT * FROM event_uwb LIMIT 10
name: event_uwb
time                admin board       location mode       node              sector subsector synchronize
----                ----- -----       -------- ----       ----              ------ --------- -----------
1683318692060120064 p     sensorBoard External production b8:27:eb:8c:67:48 1      1         OK
        #+END_PLAIN
    - meta
      - acceleremoter
        #+BEGIN_PLAIN
> SELECT * FROM meta_accelerometer LIMIT 5
name: meta_accelerometer
time                admin board          location mcu     mode       name          node              period revision sector status subsector
----                ----- -----          -------- ---     ----       ----          ----              ------ -------- ------ ------ ---------
1683318691204120064 p     externalBoard2 External esensor production accelerometer b8:27:eb:8c:67:48 1000   2.03.01  1      NOK    1
1683318692193120064 p     sensorBoard    External sensor  production accelerometer b8:27:eb:8c:67:48 1000   2.03.01  1      NOK    1
1683318692810120064 p     externalBoard2 External esensor production               b8:27:eb:8c:67:48                 1      OK     1
1683318693670120064 p     sensorBoard    External sensor  production               b8:27:eb:8c:67:48                 1      OK     1
        #+END_PLAIN
      - airInfo
        #+BEGIN_PLAIN
> SELECT * FROM meta_airInfo LIMIT 5
name: meta_airInfo
time                admin board          location mcu     mode       name    node              period revision sector status subsector
----                ----- -----          -------- ---     ----       ----    ----              ------ -------- ------ ------ ---------
1683318691199120064 p     externalBoard2 External esensor production airInfo b8:27:eb:8c:67:48 1000   2.03.00  1      NOK    1
1683318691952120064 p     externalBoard2 External esensor production         b8:27:eb:8c:67:48                 1      OK     1
1683318692186120064 p     sensorBoard    External sensor  production airInfo b8:27:eb:8c:67:48 1000   2.03.00  1      NOK    1
1683318692840120064 p     sensorBoard    External sensor  production         b8:27:eb:8c:67:48                 1      OK     1
        #+END_PLAIN
      - airQuality
        #+BEGIN_PLAIN
> SELECT * FROM meta_airQuality LIMIT 10
name: meta_airQuality
time                admin board          location mcu     mode       name       node              period revision sector status subsector
----                ----- -----          -------- ---     ----       ----       ----              ------ -------- ------ ------ ---------
1683318691200120064 p     externalBoard2 External esensor production airQuality b8:27:eb:8c:67:48 1000   2.04.00  1      NOK    1
1683318692188120064 p     sensorBoard    External sensor  production airQuality b8:27:eb:8c:67:48 1000   2.03.00  1      INIT   1
1683318692926120064 p     externalBoard2 External esensor production            b8:27:eb:8c:67:48                 1      INIT   1
1683318704842120064 p     sensorBoard    External sensor  production            b8:27:eb:8c:67:48                 1      INIT   1
1683318980604120064 p     sensorBoard    External sensor  production            b8:27:eb:8c:67:48                 1      OK     1
1683319006473120064 p     externalBoard2 External esensor production            b8:27:eb:8c:67:48                 1      OK     1
        #+END_PLAIN
      - asx340
        #+BEGIN_PLAIN
> SELECT * FROM meta_asx340 LIMIT 10
name: meta_asx340
time                admin board       capture_rate coasting_time columns decimation_factor detection_rate enable_decimation height_person location mcu   mode       name   node              occupancy_estimation period  revision rows sector status subsector width_person
----                ----- -----       ------------ ------------- ------- ----------------- -------------- ----------------- ------------- -------- ---   ----       ----   ----              -------------------- ------  -------- ---- ------ ------ --------- ------------
1683318691060120064 p     sensorBoard 30           30            40      4                 30             1                 80            External bf707 production asx340 b8:27:eb:8c:67:48 0                    1000000 1.02.02  30   1      OK     1         40
        #+END_PLAIN
      - audio
        #+BEGIN_PLAIN
> SELECT * FROM meta_audio LIMIT 10
name: meta_audio
time                admin board       child  enable location mode       name  node              revision sector subsector
----                ----- -----       -----  ------ -------- ----       ----  ----              -------- ------ ---------
1683318692166120064 p     sensorBoard stream 1      External production audio b8:27:eb:8c:67:48 3.00.00  1      1
        #+END_PLAIN
      - bf707
        #+BEGIN_PLAIN
> SELECT * FROM meta_bf707 LIMIT 10
name: meta_bf707
time                admin board       child  enable location mode       name  node              revision sector subsector
----                ----- -----       -----  ------ -------- ----       ----  ----              -------- ------ ---------
1683318691060120064 p     sensorBoard asx340 1      External production bf707 b8:27:eb:8c:67:48 1.02.02  1      1
        #+END_PLAIN
      - cc1310
        #+BEGIN_PLAIN
> SELECT * FROM meta_cc1310 LIMIT 10
name: meta_cc1310
time                admin board       enable location mode       name   node              revision sector subsector
----                ----- -----       ------ -------- ----       ----   ----              -------- ------ ---------
1683318691060120064 p     sensorBoard 1      External production cc1310 b8:27:eb:8c:67:48 1.01.00  1      1
        #+END_PLAIN
      - cc2650
        #+BEGIN_PLAIN
> SELECT * FROM meta_cc2650 LIMIT 10
name: meta_cc2650
time                admin board       child  enable location mode       name   node              revision sector subsector
----                ----- -----       -----  ------ -------- ----       ----   ----              -------- ------ ---------
1683318691167120064 p     sensorBoard rfGiga 1      External production cc2650 b8:27:eb:8c:67:48 1.01.00  1      1
        #+END_PLAIN
      - consumption
        #+BEGIN_PLAIN
> SELECT * FROM meta_consumption LIMIT 10
name: meta_consumption
time                admin board    location mcu       mode       node              period revision sector status subsector
----                ----- -----    -------- ---       ----       ----              ------ -------- ------ ------ ---------
1683318690630868992 p     rpiBoard External inaExt1   production b8:27:eb:8c:67:48 1000   1.0.0    1      OK     1
1683318690631166976 p     rpiBoard External inaPoe    production b8:27:eb:8c:67:48 10000  1.0.0    1      OK     1
1683318690631275008 p     rpiBoard External inaRpi    production b8:27:eb:8c:67:48 10000  1.0.0    1      OK     1
1683318690631360000 p     rpiBoard External inaSensor production b8:27:eb:8c:67:48 10000  1.0.0    1      OK     1
1683318690631442944 p     rpiBoard External inaExt2   production b8:27:eb:8c:67:48 10000  1.0.0    1      OK     1
1683318690633387008 p     rpiBoard External inaExt1   production b8:27:eb:8c:67:48 1000   1.0.0    1      OK     1
1683318690633591040 p     rpiBoard External inaPoe    production b8:27:eb:8c:67:48 1000   1.0.0    1      OK     1
1683318690633684992 p     rpiBoard External inaRpi    production b8:27:eb:8c:67:48 10000  1.0.0    1      OK     1
1683318690633766912 p     rpiBoard External inaSensor production b8:27:eb:8c:67:48 10000  1.0.0    1      OK     1
1683318690633862912 p     rpiBoard External inaExt2   production b8:27:eb:8c:67:48 10000  1.0.0    1      OK     1
        #+END_PLAIN
      - esensor
        #+BEGIN_PLAIN
> SELECT * FROM meta_esensor LIMIT 10
name: meta_esensor
time                admin board          child                                                                                         enable location mode       name    node              revision sector subsector
----                ----- -----          -----                                                                                         ------ -------- ----       ----    ----              -------- ------ ---------
1683318691167120064 p     externalBoard2 airInfo;airQuality;rgbwLuminosity;uvLuminosity;telemeter;accelerometer;gyroscope;magnetometer 1      External production esensor b8:27:eb:8c:67:48 3.00.00  1      1
        #+END_PLAIN
      - gyroscope
        #+BEGIN_PLAIN
> SELECT * FROM meta_gyroscope LIMIT 10
name: meta_gyroscope
time                admin board          location mcu     mode       name      node              period revision sector status subsector
----                ----- -----          -------- ---     ----       ----      ----              ------ -------- ------ ------ ---------
1683318691206120064 p     externalBoard2 External esensor production gyroscope b8:27:eb:8c:67:48 1000   2.03.01  1      NOK    1
1683318692195120064 p     sensorBoard    External sensor  production gyroscope b8:27:eb:8c:67:48 1000   2.03.01  1      NOK    1
1683318692810120064 p     externalBoard2 External esensor production           b8:27:eb:8c:67:48                 1      OK     1
1683318693670120064 p     sensorBoard    External sensor  production           b8:27:eb:8c:67:48                 1      OK     1
        #+END_PLAIN
      - leds
        #+BEGIN_PLAIN
> SELECT * FROM meta_leds LIMIT 10
name: meta_leds
time                B G R Y admin board    duty location mcu      mode       node              period               revision sector status subsector
----                - - - - ----- -----    ---- -------- ---      ----       ----              ------               -------- ------ ------ ---------
1683318690618007040 0 0 0 0 a     rpiBoard 0    External TLC59108 production b8:27:eb:8c:67:48 0.041666666666666664 1.0.0    1      OK     1
        #+END_PLAIN
      - sensor
        #+BEGIN_PLAIN
> SELECT * FROM meta_sensor LIMIT 10
name: meta_sensor
time                admin board       child                                                                                         enable location mode       name   node              revision sector subsector
----                ----- -----       -----                                                                                         ------ -------- ----       ----   ----              -------- ------ ---------
1683350298380319936 p     sensorBoard airInfo;airQuality;rgbwLuminosity;uvLuminosity;telemeter;accelerometer;gyroscope;magnetometer 1      External production sensor b8:27:eb:8c:67:48 3.00.00  1      1
        #+END_PLAIN
      - stream
        #+BEGIN_PLAIN
> SELECT * FROM meta_stream LIMIT 10
name: meta_stream
time                admin board       divider fs    location mcu   microphone mode       name   node              revision sector status subsector
----                ----- -----       ------- --    -------- ---   ---------- ----       ----   ----              -------- ------ ------ ---------
1683350298389319936 p     sensorBoard 2       48000 External audio 0          production stream b8:27:eb:8c:67:48 2.03.00  1      NOK    1
1683350301129319936 p     sensorBoard               External audio            production        b8:27:eb:8c:67:48          1      OK     1
        #+END_PLAIN
      - uwb
        #+BEGIN_PLAIN
> SELECT * FROM meta_uwb LIMIT 10
name: meta_uwb
time                admin board       child          enable location mode       name node              revision sector subsector
----                ----- -----       -----          ------ -------- ----       ---- ----              -------- ------ ---------
1683318692166120064 p     sensorBoard uwbCfg;uwbData 1      External production uwb  b8:27:eb:8c:67:48 3.00.00  1      1
        #+END_PLAIN
      - uwbCfg
        #+BEGIN_PLAIN
> SELECT * FROM meta_uwbCfg LIMIT 10
name: meta_uwbCfg
time                address admin antdelayrx antdelaytx board       channel datarate location mcu mode   mode_1     name   node              nssfd pac panid power prf prflength revision sector status subsector trim
----                ------- ----- ---------- ---------- -----       ------- -------- -------- --- ----   ------     ----   ----              ----- --- ----- ----- --- --------- -------- ------ ------ --------- ----
1683318692181120064 2       p     0          0          sensorBoard 5       6800     External uwb normal production uwbCfg b8:27:eb:8c:67:48 0     8   1     255   64  128       2.04.00  1      INIT   1         15
1683318692324120064         p                           sensorBoard                  External uwb        production        b8:27:eb:8c:67:48                                              1      OK     1
        #+END_PLAIN
      - uwbData
        #+BEGIN_PLAIN
> SELECT * FROM meta_uwbData LIMIT 10
name: meta_uwbData
time                admin board       location mcu mode       name    node              revision sector status subsector
----                ----- -----       -------- --- ----       ----    ----              -------- ------ ------ ---------
1683318692183120064 p     sensorBoard External uwb production uwbData b8:27:eb:8c:67:48 2.03.00  1      NOK    1
1683318692324120064 p     sensorBoard External uwb production         b8:27:eb:8c:67:48          1      OK     1
        #+END_PLAIN
      - thermalImgHd
        #+BEGIN_PLAIN
  > SELECT * FROM meta_thermalImgHd LIMIT 10
  name: meta_thermalImgHd
  time                admin board       leptonVersion location mcu     mode       node              sector status subsector
  ----                ----- -----       ------------- -------- ---     ----       ----              ------ ------ ---------
  1683309571571892224 p     sensorBoard 2             External thermal production b8:27:eb:8c:67:48 1      OK     1
        #+END_PLAIN
  - collectd
    - Measurements
      #+BEGIN_PLAIN
> SHOW MEASUREMENTS
name: measurements
name
----
cpu_value
df_value
disk_io_time
disk_read
disk_value
disk_weighted_io_time
disk_write
entropy_value
interface_rx
interface_tx
irq_value
load_longterm
load_midterm
load_shortterm
memory_value
processes_value
swap_value
users_value
      #+END_PLAIN
  - En consultant le dashboard, observe qu'on a l'air de produire 1Mo de données par minute
    - Donc 1Go toutes les 16h40
    - Dispose de ~100Go d'espace de stockage
    - Peut donc tourner ~70 jours avant de run out of space
    - Donc pas le facteur le plus limitant du système
  - Plusieurs measurements ne sont que peu utilisés en mode autonome
    - Les measurements correspondants aux events des mcu ayant des children (audio, bf707, cc2650, esensor, sensor, uwb)
      - Dans ce cas, n'a qu'un event pour indiquer que le mcu s'est sync
    - event_asx340 a l'air à déclenchement unique
      - Deux events enregistrés au même timestamp (au lancement?), depuis rien
      - J'ai rien dit, les timestamps sont différents
      - Vu sa période d'après les infos stockées dans meta, i.e. 1000000, ça a du sens
        - Donc toutes les 16min40 si je me trompe pas
    - Les measurements meta_
      - Indiquent seulement la mise en route et la configuration initiale des capteurs
  - Ces measurements disparaissent donc après la 1ère réinitialisation de la BD
  - Les différentes entrées permettent de retrouver de quel mcu dépend chaque capteur
    - audio gère stream
    - bf707 gère asx340
    - cc2650 gère rfGiga
    - esensor gère airInfo;airQuality;rgbwLuminosity;uvLuminosity;telemeter;accelerometer;gyroscope;magnetometer
    - sensor gère airInfo;airQuality;rgbwLuminosity;uvLuminosity;telemeter;accelerometer;gyroscope;magnetometer
    - thermal gère thermalImgHd;thermalImgLd;
    - uwb gère uwbCfg;uwbData
      - Qui n'ont pas d'avoir leurs propres events
  - Plusieurs questions
    - Qu'est-ce asx340 ?
      - Quel est son rôle ?
    - Qu'est-ce que cc1310 ?
      - Un mcu sans enfant, qui ne se sync pas
      - Ne voit cependant pas d'entrées correspondantes
      - Quel est son rôle ?
    - Qu'est-ce que uwb ?
      - Un mcu avec des enfants ? uwbCfg et uwbData me semblent pas être des capteurs
      - Pas réussi à observer le moindre event de ce côté
    - Pourquoi doubler une partie des capteurs ?
      - Les mcu esensor et sensor ont l'air de proposer le même ensemble de capteurs
      - Sauf que sensor est sous une vitre de plexiglas, donc quid de la pertinence des données sur l'air, la luminosité ?
      - Mickaël m'a répondu que désactiver ces capteurs lors du fonctionnement en mode autonome fait partie des choses à faire
  - asx340
    - S'agit d'un capteur d'image
  - cc1310
    - Est censé avoir un enfant rfSubGiga
    - Donc un module radio émettant sur des fréquences < 1GHz ?
    - Un module similaire à cc2650/rfGiga je suppose
  - uwb
    - Semble être un module de communication
    - Certains champs de ses messages meta font mention d'antennes d'après la doc
    - Ultra-Wideband (UWB)
  - On retrouve donc plusieurs modules liés aux communications
    - rfGiga
    - rfSubGiga
    - uwb
    - Aucun n'a jamais été utilisé ?
- SmartSense: Intégrer les modifications effectuées à la branche *main*
  - Mickaël a validé la MR, mais a fait plusieurs retours
  - Faudrait soit
    - Y répondre
    - Créer des issues qui correspondent
      - Histoire d'en conserver une trace
      - Et de lister les tâches restantes
  - [X] Problème de crash d'InfluxDB
    - Création d'une issue
  - [X] Ajout de tests pour la réception et le traitement de commandes
    - Création d'une issue
  - [X] Ajout de tests pour la BD "collectd"
    - Pas trop sûr de quoi ajouter comme tests
    - À part check l'existence de la BD
  - [X] Correction du secteur, sous-secteur et nodeId pour les topics de test
- SmartSense: Intégrer la branche *refactor/split-mqtt-interface*
  - [X] Rebase sur *main*
  - [X] Ouvrir la MR
- SmartSense: Préparer la branche *refactor/main-app* pour intégration
  - Rebase sur *refactor/split-mqtt-interface*
  - En a profité pour
    - Drop les commits équivalents
    - Rework le commit rendant optionnelles les instances de *MqttInterface* au passage
      - Préfère continuer à les instancier dans *mainApp.startProcess()*
** Semaine du <2024-01-29 Mon> au <2024-02-02 Fri>
*** Planned
**** DONE SmartSense: Rechercher outil pour refactorer le code du projet vis-à-vis des conventions Python
CLOSED: [2024-01-29 Mon 17:00]
- Pas mal de retours de Pylint sur le code correspondent à un non-respect des conventions Python
  - Noms de modules, de fonctions et de méthodes
- Voir si un outil permet d'automatiser ce type de correctifs
**** DONE SmartSense: Refactor *MqttInterface*
CLOSED: [2024-01-31 Wed 10:26]
- Ne plus hériter de *mqtt.Client*
  - Actuellement composant dans un état bizarre
  - Hérite des méthodes mais pas des attributs car n'appelle jamais le constructeur de *Mqtt.Client*
  - Déclenche des warnings et même erreurs si on manipule un peu trop le composant
    - e.g. ne peut pas surcharger /on_subscribe()/
- Mettre en place un Enum pour gérer le niveau de QoS des messages
  - Passe directement les valeurs 0, 1 et 2 pour le moment
  - MQTT n'offre pas du tout les mêmes garanties en fonction du niveau choisi
  - 0: At most once
  - 1: At least once
  - 2: Exactly once
  - Serait plus parlant d'utiliser un type qui précise cela
- Refactor /processDataForInfluxDBEmbedded()/
  - Décomposer en fonctions
  - Ne fait rien si /not self.subscriber/
    - Vérifier que c'est un comportement souhaité
    - Me parait error-prone perso
- Séparer l'interface avec le broker MQTT avec l'interface avec la BDD InfluxDB
  - IMO *MqttInterface* devrait servir à interagir avec le broker MQTT
  - L'interaction avec la BDD InfluxDB devrait être géré par un autre composant spécialisé
    - Qui plus est, l'interaction avec la BDD est plus limitée, i.e. write-only
  - Permettrait ainsi de rendre plus clair le code
  - E.g. composant n'interagit pas avec la BDD si le flag /mqttEnabled/ est activé
    - Même dans le cas où le flag /influxDbEmbedded/ est lui aussi activé
**** DONE SmartSense: Nettoyer *mainApp*
CLOSED: [2024-02-01 Thu 14:39]
- Déplacer le code du scope global du module dans une fonction /main()/
- Factoriser le code de /startProcess()/
- Décomposer en fonctions /routeIncomingMqttMessages()/
**** IN-PROGRESS SmartSense: Ajouter des tests pour *DataToLocalInfluxDB.parseMqtt()*
- J'ai atteint la limite du refactoring que je peux faire dans le projet en ayant à peu près confiance
- Et n'ayant pas de précisions sur les évolutions à apporter
- Serait préférable de consolider les tests pour le moment
- Pour le moment, vérifie le bon fonctionnement de *parseMqtt()* avec un payload correct
- Vérifier pour d'autres payloads corrects
- Vérifier pour payload invalide
**** IN-PROGRESS SmartSense: Intégrer les modifications effectuées à la branche *main*
- J'ai fini de mettre en place une première série de tests
- Avant d'aller plus loin et d'effectuer de nouvelles modifs
- Serait bien de me poser avec Mickaël pour reviewer mes branches et discuter de ce que j'ai fait
**** IN-PROGRESS SmartSense: Intégrer la branche *autonomous-node* à la branche *main*
- Cette branche a l'air d'apporter des workarounds pour les problèmes constatés lors de l'utilisation du noeud SmartSense en mode autonome
- Notamment, exporte et reset la DB périodiquement pour éviter le mécanisme de compression des shards
  - Qui a l'air d'être trop couteux pour tourner sur le noeud
- Améliore aussi la lisibilité du code
*** Done
- SmartSense: Intégrer les modifications effectuées à la branche *main*
  - J'ai notifié Mickaël que j'ai push mes modifications
  - Et lui ai précisé que j'aimerais une validation
- SmartSense: Intégrer la branche *autonomous-node* à la branche *main*
  - J'ai rebase ma branche *add-tests* sur la branche *autonomous-node*
  - Notamment en modifiant les commits impactés par les modifications de cette branche, i.e.
    - feat(formatter): add ruff
    - test(dataToLocalInfluxDB): edit test to verify buffering mechanism
  - Pushé cette branche sur le repo
  - Ouvert une MR décrivant les modifs globales de la branche
  - Ai demandé à Mickaël de valider cette MR
- SmartSense: Rechercher outil pour refactorer le code du projet vis-à-vis des conventions Python
  - Pas trouvé d'outils qui fait cela
  - Le plus proche que j'ai vu est : https://github.com/hhatto/autopep8
  - Mais ne fait que des transformations autour des whitespaces
    - A un mode aggressif
    - Mais ne propose d'autofix que les règles E711 et E712
    - Moi c'est la C0103 qui m'intéresse
  - Je pense que je suis bon pour faire ça "à la main"
- SmartSense: Refactor *MqttInterface*
  - [X] Séparer l'interface avec le broker MQTT avec l'interface avec la BDD InfluxDB
    - [X] Déplacement de /processDataForInfluxDBEmbedded()/ dans son propre module
      - Permet de simplifier les deux composants
        - Pas de mode "subscriber" pour *influxdb_interface*
        - Plus de mode "mqttEnabled" et "influxDbEmbedded" pour *MqttInterface*
      - Pose plusieurs questions cependant
        - Où déplacer/comment retravailler le code simulant une réponse du service de provisioning ?
          - Permettrait de se débarrasser de la *Queue* dans le sens *influxdb_interface* -> *mainApp*
          - Nécessite de comprendre le but de ce morceau de code
        - Où instancier et dans quelles conditions instancier *influxdb_interface* ?
          - Liée à cette question de provisioning
        - Nécessite aussi de savoir s'il est possible de cumuler un broker MQTT et une base de données locale InfluxDB
        - I.e. doit-on découpler le provisioning (setup?) du noeud du broker MQTT ?
    - Concernant le provisioning
      - Plusieurs messages concernent le provisioning
      - Le message sur le topic "provisioning/{self.currentSite}/node/{self.id}"
        - Permet de set les valeurs requises pour générer les topics
          - mode, location, secteur, sous-secteur
        - Et de préciser le port du broker MQTT lié au secteur
          - Y a plusieurs brokers ?
        - Dans le cas du mode autonome, utilise une valeur dummy (0) comme port pour le broker MQTT
      - Les messages sur les topics "provisioning/p/{MyApp.locationPath}/{MyApp.id}/rpiBoard/{ina}/consumption"
        - Plusieurs devices INA
          - Poe
          - Rpi
          - Sensor
          - Ext1
          - Ext2
        - Chacun va être à l'origine d'un message de provisioning sur son topic dédié
        - Permet d'obtenir la fréquence d'échantillonnage à utiliser si je pige bien
      - Les messages sur les topics "provisioning/a/{MyApp.locationPath}/{MyApp.id}/{tcl}"
        - Semble être conçu pour supporter plusieurs devices
        - En pratique, y en a qu'un seul pour le moment
          - Et donc qu'un seul suffixe pour le topic : "rpiBoard/TLC59108/leds"
        - De même, semble concerner la fréquence d'échantillonnage
    - [X] Découplage du self provisioning et de *MQTT/InfluxDBInterface*
      - Ajout d'un flag isProvisioningViaMQTT
      - Spécifie si le noeud doit se provisionner via MQTT
        - Dans ce cas envoie le 1er message
          - Sur le topic "provisioning/{self.currentSite}/node/{self.id}"
        - Et attend la réponse avant de passer à la suite
      - Sinon utilise d'emblée les valeurs dummy pour la génération de topics et le broker du secteur
      - Et intercepte les messages des modules *InaManager* et *TclManager* concernant le provisioning
        - Simule lui-même les réponses au lieu de déléguer au module *MQTT/InfluxDBInterface*
  - [X] Mettre en place un Enum pour gérer le niveau de QoS des messages
  - [X] Ne plus hériter de *mqtt.Client*
    - J'en ai profité pour refactorer le code et virer le code inutilisé
      - Par exemple /setCallbackOnMessage()/
    - Un cas particulier est celui du paramètre /protocol/ du constructeur
    - Dans la version d'origine de *MqttInterface*, s'agit à la fois
      - D'une propriété de la classe héritée de *mqtt.Client*, qui a pour valeurs possibles MQTTv31, MQTTv311 ou MQTTv5
        - Constantes fournies par *mqtt*
      - D'un flag pour déterminer la valeur d'une autre propriété, /transport/, qui a pour valeurs possibles "tcp" et "websocket"
    - Sauf que /protocol/ n'est jamais spécifié dans le moindre appel au constructeur de *MqttInterface*
      - Donc inutilisé et on repose donc sur la valeur par défaut de /transport/ : "tcp"
    - A donc supprimé le paramètre /protocol/
      - Voir si cela pose un problème
    - Rencontré quelques difficultés à faire fonctionner les tests
      - La connexion au broker se faisait correctement
      - L'envoi de messages aussi
      - Mais la réception, via *MqttInterface* ou *mqtt.Client* directement, ne se produisait pas
      - C'est tombé en marche, mais sans que je n'arrive à identifier le problème exactement
      - Trouvé, il s'agissait d'une erreur dans la callback /on_connect()/
      - Essayais d'accéder à /self.nodeId/, qui correspond à cet instant à /client.nodeId/ et non à /mqttInterface.nodeId/
        - Propriété non définie
      - Étrangement, fait planter silencieusement le client MQTT
      - De plus, utilisais dans tous mes tests une instance de *MqttInterface* en mode subscriber
        - Et donc exécutant les lignes de code problématiques
    - A pu en profiter pour rework le test "receive_msg_from_broker"
      - Surcharge callback /on_subscribe()/ pour déclencher l'envoi du message de test
      - Au lieu d'utiliser le workaround non-déterministe "j'attends un peu que l'instance *MqttInterface* se soit connectée au broker"
  - J'ai mes différentes modifs prêtes sous la forme de branches
  - Reste plus qu'à les faire valider
    - En profiter pour voir ce paramètre étrange /protocol/ et son impact sur /transport/
- SmartSense: Refactor *mainApp*
  - [X] Déplacer le code du scope global du module dans une fonction /main()/
  - [X] Utilisation de f-strings
  - [X] Rendre l'instanciation des *MqttInterface* dépendantes de la config
  - [X] Ajout d'une vérification de la config
    - Stop l'application si les sorties MQTT et InfluxDB sont toutes deux désactivées
    - Quel intérêt dans ce cas là ?
  - [X] Ajout d'un Enum pour représenter les différents noms de process
    - Permet l'utilisation de strings codées en dur
    - Et les erreurs que ça peut engendrait
  - [X] Utilisation de match/case au lieu de if/elif
  - [X] Factorisation du code de /startProcess()/
    - Mise en place d'une méthode /startAndRegisterProcess()/ qui se charge de créer, démarrer et enregistrer le process
    - Utilisation de cette méthode dans /startProcess()/
  - [X] Décomposition de /routeIncomingMqttMessages()/
    - Identifie 2 types de messages
      - provisioning
      - cmd
    - Provisioning
      - Pas bien conséquent, peut faire l'objet d'une fonction
    - Cmd
      - Là par contre, y a de multiples cas à gérer
      - À décomposer en sous-fonctions
    - Par contre, remarque un détail étrange
      - Certains messages peuvent déclencher plusieurs réactions
      - Par exemple, un même message peut déclencher les bouts de code suivants
        - https://gitlab.inria.fr/smartsense/3douest/node-app/smartsense-node-app/-/blob/main/mainApp.py?ref_type=heads#L144-147
        - https://gitlab.inria.fr/smartsense/3douest/node-app/smartsense-node-app/-/blob/main/mainApp.py?ref_type=heads#L194-205
      - Est-ce volontaire ?
    - Autre point étrange
      - Pour certains messages, le topic n'a aucun rôle
      - E.g. https://gitlab.inria.fr/smartsense/3douest/node-app/smartsense-node-app/-/blob/main/mainApp.py?ref_type=heads#L218-240
        - Le seul requirement au niveau du topic est sa taille, i.e. >=8
        - Mais le reste, osef
      - En fait, je pense que c'est bien ça : on s'en fout du topic
      - Le check de la taille du topic ne sert qu'à se protéger d'une erreur lors de la récupération de la target
    - Interrogation supplémentaire
      - Les commandes ne ciblent pas des noeuds spécifiques ?
      - À aucun moment on ne fait un filtre sur le topic par rapport à l'identifiant du noeud
      - Ah si, c'est implicite
      - Puisque le noeud s'abonne qu'aux topics avec son nodeId, ne reçoit que les messages le concernant
- SmartSense : Ajouter des tests pour *DataToLocalInfluxDB.parseMqtt()*
  - Pensais juste ajouter des topics à liste des valeurs possibles que peut retourner /getTopic()/
  - Mais vu que j'utilise cet helper à plusieurs reprises, doit faire gaffe aux valeurs proposées
  - Peut pas proposer des topics concernant
    - Le provisioning
    - Des cmd
    - Étant dans un autre mode que production
  - Donc reste des events
  - /getTopic()/ est utilisé conjointement avec /getPayload()/
    - Les fusionner ?
- SmartSense: Réflexions
  - Comment mettre en place et maintenir un protocole de communication ?
    - SmartSense utilise des messages pour communiquer à la fois entre les noeuds et le serveur
    - Mais aussi entre les différents modules du noeud
      - e.g. via les topics et payload
    - Pour le moment, ces messages sont construits et parsés à la main à différents endroits du code
      - Les modules tels que *InaManager* créent les messages
      - Les transmettent à *MainApp* qui les parse pour les router
      - Ou se self-provision
      - Idem pour les messages du serveur que *MainApp* reçoit via le client MQTT
    - Approche me parait error-prone
      - Quid de quand on décide de modifier le topic utilisé pour un type de message ?
        - E.g. pourquoi les messages provenant du module INA sont marqués comme niveau public ?
        - Et les messages du module TLC sont marqués comme niveau admin ?
        - Uniformiser ?
      - Comment vérifier que l'on a traité tous les cas ?
    - Et peu lisible
      - Détermine le message reçu en checkant des morceaux de ce dernier
      - Et déclenche des actions en réaction
      - Mais sans vraiment nommer/expliciter ce qui est fait
      - Au lecteur de recoller les morceaux
  - Est-ce vraiment InfluxDB qu'on veut utiliser dans le cas du mode autonome ?
    - À voir les traitements que l'on va effectuer
    - Et les données qu'ils requièrent
    - Mais plusieurs questions se posent
    - Est-ce que la dimension temporelle va être très importante ?
    - Est-ce qu'on ne peut pas reproduire cette dimension temporelle avec une fenêtre glissante des entrées ?
    - Grosso modo, va-t-on à un moment query la BDD locale ?
    - Pour quelles raisons le choix s'est porté sur InfluxDB pour le SGBD embarqué lors de la conception du mode autonome ?
      - Si c'est juste une question de compatibilité/d'export avec la BDD qui tourne sur le serveur, est-ce qu'un fichier ne suffit pas ?
- SmartSense: Compréhension du code
  - *TlcManager*
    - Gère les leds du noeud SmartSense
    - Reçoit des commandes pour paramétrer les leds
    - Envoie ses données meta au provisioning puis lors d'une commande ("cmd/.../nodeId", "action=get *")
  - *InaManager*
    - Monitore l'alimentation des différents ports de la raspberry
      - poe: Power Over Ethernet ?
      - rpi: Pourquoi il y a aussi la rpi elle-même dans la liste ?
    - Reçoit des commandes pour
      - get sa configuration
      - set sa période
    - À l'issue d'une période, envoie les données meta de la conso électrique de chaque port
  - *UsbBoardManager*
    - Gère les différentes sondes du capteur
    - Reçoit des commandes pour interagir avec les sondes
      - Démarrer ou stop les sondes
      - Lire les métas des sondes
      - Transmettre les commandes aux sondes
      - Aussi une commande "timestamp"
        - Permet de décaler les timestamps sur les mesures futures
        - Soit à partir d'un offset fourni
        - Soit à partir d'un timestamp calculé sur le coup
        - Comprends pas trop comment cela fonctionne
    - Envoie dès que possible les données collectées par les sondes
  - *RpiGpioManager*
    - Deux instances
      - Une de *MainApp*
      - Une de *UsbBoardManager*
      - Safe ?
    - Gère l'alimentation des différents ports de la raspberry
      - SensorBoard
      - Ext1 et Ext2
      - GlobalSync ?
  - *SensorBoardManager*
    - N'est plus utilisé actuellement
    - Ancêtre de *UsbBoardManager* ?
    - Lui ressemble beaucoup en tout cas
  - *Syncer*
    - Étant donné un timestamp, attends que celui-ci arrive
    - Utilisé par *MainApp.syncAll()*
    - Pige pas trop son fonctionnement
      - Instancie un *RpiGpioManager* avant de se mettre en attente
      - Utilise *RpiGpioManager.globalSync()* une fois le timestamp arrivé
      - Comment cet appel peut-il aider à synchro les différentes sondes ?
      - Sert p-e plutôt à synchroniser les différents noeuds
** Semaine du <2024-01-22 Mon> au <2024-01-26 Fri>
*** Planned
**** DONE S'approprier SmartSense/smartsense-node-app
CLOSED: [2024-01-29 Mon 10:33]
- On est en train de revoir les priorités avec l'arrêt maladie de Guillaume
- Serait préférable que je passe sur SmartSense en attendant son retour
- Les différentes tâches identifiées concernent principalement le capteur SmartSense
  - Ajout d'une couche réseau WiFi
  - Ajout de traitements en local des données collectées
    - i.e. sur le capteur
  - Correction des bugs rencontrés lors de l'utilisation de la BDD en local
    - Dans la version du capteur pour un fonctionnement autonome
    - Le capteur fait tourner une instance de la BD, InfluxDB, en local
    - Mais lors de leurs expériences, ont constaté des crashs de la BDD
      - Parfois au bout de plusieurs heures
- Il serait donc intéressant que je rentre rapidement dans ce projet
**** DONE Mettre en place des tests pour MQTTInterface
CLOSED: [2024-01-29 Mon 10:33]
- Pistes de tests possibles
  - Se connecte bien au broker
    - Variante not subscriber
  - Si je fournis un message valide à l'instance MQTTInterface, il est bien publié sur le broker
    - Variante avec message invalide
  - Si je fournis un message valide au broker, il est bien récupéré par l'instance MQTTInterface
    - Variante avec message invalide
*** Done
- S'approprier SmartSense/smartsense-node-app
  - Pistes de travail
    - Identifier les dépendances
    - Mettre en place un gestionnaire de dépendances
      - Poetry est un bon candidat
    - Mettre en place un linter
      - pylint : https://www.pylint.org/
      - Ruff : https://astral.sh/ruff
    - Setup un task runner
      - poethepoet : https://github.com/nat-n/poethepoet
    - Automatiser le déclenchement des outils
      - Hook pré-commit
      - Config VSCode
    - Mettre en place un autoformatter
      - Ruff : https://astral.sh/ruff
      - Black : https://github.com/psf/black
      - isort: https://github.com/PyCQA/isort
    - Mettre en place un outil d'analyse des types
      - mypy : https://www.mypy-lang.org/
      - pyright : https://github.com/microsoft/pyright
  - Getting started
    - pipx install poetry
    - pipx install poethepoet
    - poetry add --group dev pylint
    - poetry add --group dev ruff
    - poetry add --group dev isort
  - Dépendances identifiées
    - influxdb
      - https://pypi.org/project/influxdb/
    - intelhex
      - https://pypi.org/project/intelhex/
    - ntplib
      - https://pypi.org/project/ntplib/
    - paho-mqqt
      - https://pypi.org/project/paho-mqtt/
    - pi-ina219
      - https://pypi.org/project/pi-ina219/
    - pyftdi
      - https://pypi.org/project/pyftdi/
    - pyserial
      - https://pyserial.readthedocs.io/en/latest/pyserial.html
    - RPi.GPIO
      - https://pypi.org/project/RPi.GPIO/
    - smbus2
      - https://pypi.org/project/smbus2/
    - spiflash
      - https://pypi.org/project/pyspiflash/
      - A pour dépendance pyftdi et pyserial
  - Rencontre une erreur lors de l'installation de
    - pi-ina219
    - RPi.GPIO
  - Une brève recherche m'informe que c'est parce qu'il me manquait une dépendance système
    - sudo dnf install python3-devel
  - J'ai pu installer les dépendances restantes
  - Obtient les erreurs suivantes
    - Module RPi.GPIO has no setmode/BCM/setup/.../OUT member
    - No name 'SMBusWrapper' in module 'smbus2'
  - SMBusWrapper
    - Le changelog de la librairie indique la suppression de la classe SMBusWrapper au profit de SMBus
    - Voir https://github.com/kplindegaard/smbus2/blob/master/CHANGELOG.md#040---2020-12-05
    - Remplacer toutes les occurrences de SMBusWrapper par SMBus corrige l'erreur relevée par pylint
  - RPi.GPIO
    - A tenté de lancer l'interpréteur python pour jouer avec ce module et voir ce qui pouvait poser problème
      - poetry shell
      - python3
      - import RPi.GPIO as GPIO
    - Mais rencontre l'erreur suivante
      - RuntimeError: This module can only be run on a Raspberry Pi!
    - Ce qui peut p-e expliquer l'erreur rencontrée
      - Le linter rencontre p-e des difficultés à gérer le module, si ne peut pas importer ce dernier
  - Comment résoudre ça ?
    - Dev sur raspberry/VM ?
  - J'ai tenté d'émuler la raspberry et de déployer l'environnement de dev dessus, histoire de tenter le coup
    - Setup la raspberry et les deps de dev
      - sudo apt update; sudo apt full-upgrade
      - sudo apt install python3-pip
      - sudo apt install python3-venv
      - python3 -m pip install --user pipx
      - python3 -m pipx ensurepath
      - source ~/.bashrc
      - pipx install poetry
      - pipx install poethepoet
      - poetry install --no-root --with dev
    - Transférer le projet sur la raspberry emulée
      - scp -P 2222 -r . pi@localhost:/home/pi/smartsense-node
  - Mais lors de l'install de poetry, j'ai rencontré un problème de stockage
    - La raspberry émulée ne dispose que de 1.7Go de stockage, et il ne m'en restait que 20Mo à ce stade
  - Comment allouer plus d'espace à la machine ?
    - Création d'une image à partir de l'image raspbian
      - qemu-img dd -f raw -O qcow2 if=2023-12-05-raspios-bullseye-arm64-lite.img of=rpi-bullseye.qcow2
      - qemu-img resize rpi-bullseye.qcow2 8G
    - Resize de la partition / une fois le système démarré
      - Suivi le tuto : https://raspberrypi.stackexchange.com/questions/499/how-can-i-resize-my-root-partition
      - Commandes
        - sudo fdisk /dev/mmcblk0
        - Suppression de la partition / (d)
        - Création d'une nouvelle partition / (n)
          - Bien préciser comme start number le start number de la partition existant précédemment
        - sudo reboot
        - sudo resize2fs /dev/mmcblk0p2 (p2 correspondant à la partition /)
        - sudo reboot
        - df -h (pour vérifier le résultat)
  - J'ai pu exécuter le linter sur la raspberry emulée
  - Malheureusement, rencontre la même erreur que sur ma machine
    - En démarrant l'interpréteur python et en essayant d'importer le module RPi.GPIO, retrouve le même message d'erreur
    - i.e. "Module can only be run on a Raspberry Pi!"
  - Pose la question de la librairie utilisée/à utiliser
    - Projet utilise : https://sourceforge.net/projects/raspberry-gpio-python/
      - Mais qui n'a pas l'air fonctionnel sur un autre système que RPi
        - Et encore, n'a pas l'air de fonctionner dans mon env virtuel
      - Dernier commit date de février 2022
      - Mais projet a l'air encore d'actualité
        - Des issues sont encore crées et attribuées au maintainer
        - Notamment pour dev en dehors de l'env RPi
    - Existe RPIO : https://github.com/metachris/RPIO
      - Une alternative à RPi.GPIO
        - Peut être utilisée à sa place sans modifications
      - Mais plus maintenue depuis fin 2022
      - 'Fin, repo archivé depuis fin 2022
      - Le dernier commit date de 2013
      - Est-ce compatible avec les modèles de RPi utilisés ?
    - Des posts abordent la question des différentes librairies
      - e.g. https://raspberrypi.stackexchange.com/questions/58820/compare-and-contrast-python-gpio-apis
    - Voir si on peut s'en sortir avec RPi.GPIO, i.e. dev facilement sur un autre env que RPi
    - Ou s'il est préférable de remplacer la librairie utilisée pour une plus adaptée à un process de CI
  - Suis tombé sur la librairie Mock.GPIO : https://github.com/codenio/mock.gpio
    - Via ce post https://stackoverflow.com/questions/51879185/how-to-mock-rpi-gpio-in-python
  - Permet de résoudre le problème posé par RPi.GPIO
  - J'ai expliqué à Mickaël ce que j'ai fait et que j'aimerais poursuivre par l'ajout de tests
    - Semble d'accord
    - Souhaite juste limiter le temps passé à faire des tests
  - Pour le moment, j'ai fait une branche regroupant mes changements et ouvert une MR
  - Ok maintenant, que faire ?
  - Serait intéressant de retravailler mainApp et MQTTInterface
  - MQTTInterface
    - A trop de responsabilité IMO
      - Permet au noeud d'interagir avec le broker MQTT
        - Établir la connexion, publier les données collectées sur le broker, gérer les messages reçus via le broker et gérer les déconnexions
      - Permet au noeud d'interagir avec la base de données InfluxDB
        - Instancie le composant se connectant avec la DB et permettant d'y enregistrer les données collectées
      - Détermine à quel endpoint transmettre les données en fonction de sa config
        - Si mqttEnabled, envoie les données au broker
        - Sinon si influxDbEmbedded et subscriber, i.e. si influxDbEmbedded and subscriber and not mqttEnabled, envoie les données à influxDB
      - Gère aussi la config des capteurs dans le cas où le noeud est en mode autonome, i.e. il n'y a pas de communication via le broker pour indiquer les settings du noeud
    - IMO, devrait juste permettre l'interaction avec le broker MQTT
    - L'interaction avec la BDD devrait se faire via un autre composant
    - Et la config du noeud en mode autonome devrait être gérée par mainApp
  - mainApp
    - Pourquoi instancie deux MQTTInterface ?
      - Une subscriber, l'autre non
      - L'instance subscriber permet d'envoyer les messages ayant trait au provisioning
        - Donc utile qu'en mode connecté
        - Ah non, permet dans le cas du mode autonome de simuler une réponse du serveur
      - Les autres messages sont diffusés via l'instance not subscriber
        - Concerne le mode connecté et le mode autonome
    - Y a-t-il un intérêt à conserver ces deux instances ?
      - Permet d'avoir des threads séparés pour les types de message
      - Est-ce nécessaire ?
    - Pourquoi ce n'est pas lui qui instancie le composant pour interagir avec la BDD ?
    - Et qui détermine où sont propagées les données ?
  - Comment procéder ?
    - Avant de modifier le code et sa logique, je voudrais bien mettre en place des tests
  - Qu'est-ce que je veux tester ?
    - MQTTInterface
      - Se connecte bien au broker
        - Variante not subscriber
      - Si je fournis un message valide à l'instance MQTTInterface, il est bien publié sur le broker
        - Variante avec message invalide
      - Si je fournis un message valide au broker, il est bien récupéré par l'instance MQTTInterface
        - Variante avec message invalide
- Mettre en place des tests pour MQTTInterface
  - Peut faire les questions réponses dans un même test
    - i.e. demander à MQTTInterface d'envoyer un message sur un topic donné
    - M'être abonné au préalable à ce topic
    - Et ainsi vérifier que j'ai bien reçu le message
    - Et inversement
  - J'ai réussi à mettre en place un test simple
    - Créé un client MQTT
    - L'abonne à un topic
    - Publie un message sur ce topic via le helper de la librairie
    - Assert si mon client a bien reçu le message
  - Par contre, un test n'échoue pas par défaut si aucun assert n'est exécuté
    - Voir si on peut modifier ce comportement dans la config
  - [X] Ajout test publier sur le broker
    - But est de tester que si je fournis un message dans la Queue de l'instance MqttInterface, celui-ci est bien envoyé au broker
  - [X] Ajout test lire depuis le broker
    - But est de tester que si je poste un message sur le broker, MqttInterface transmet bien ce message à l'application via la Queue
    - Juste un problème pour publier le message sur le broker à temps
    - Si l'envoie dès que possible, MqttInterface loupe le message car n'a pas encore subscribe
    - Peut pas déclencher l'envoi du message en utilisant on_subscribe() de MqttInterface
      - Rencontre une erreur lorsque j'essaie de set on_subscribe()
      - Semble lié au fait que MqttInterface n'appelle jamais le constructeur de Client
      - Et ne dispose pas de tous les attributs qu'il est censé posséder
    - Workaround pour le moment en ajoutant une attente avant la publication du message
    - À corriger dans MqttInterface
  - [ ] Ajout test publier dans InfluxDB
    - But est de tester que si je fournis un message dans la Queue de l'instance MqqtInterface, celui-ci est bien envoyé à la BDD
    - Plusieurs détails à gérer
    - Tous les messages ne sont pas envoyés à la BDD
      - Un filtre est effectué pour ne stocker que les données intéressantes/pertinentes
      - Reste à comprendre les données attendues et les topics associés
    - L'écriture en BDD ne se fait pas à chaque donnée, mais une fois que le buffer dépasse un certain seuil
      - Doit donc procéder à l'envoi de suffisamment de données pour déclencher une écriture
      - Le seuil a l'air d'être 100 messages
      - P-e rendre paramètrisable cette donnée de façon à simplifier la validation des tests
    - Pour procéder par étapes, je vais déjà ajouter des tests sur DataToLocalInfluxDB
      - Composant chargé d'interagir avec InfluxDB, de formater la trame MQTT en une trame InfluxDB et de gérer l'écriture en BD des données
  - [X] Ajout test création de DB
    - But est de tester que DataToLocalInfluxDB créé bien une base de données à son instanciation
  - [X] Ajout test parseMqtt
    - But est de tester que DataToLocalInfluxDB transforme bien une trame MQTT en une trame InfluxDB
  - [X] Ajout test sendPointsToInfluxDB
    - But est de tester que DataToLocalInfluxDB écrit bien les données en BDD
      - Doit par contre atteindre la taille minimum pour déclencher une écriture
      - En attendant, données bufferisées
** Semaine du <2024-01-15 Mon> au <2024-01-19 Fri>
*** Planned
**** DONE Prendre en main le script de connexion aux brokers MQTT
CLOSED: [2024-01-16 Tue 13:38]
- Mickaël m'a partagé son script de test
- Voir pour l'essayer et le comprendre
**** DONE Résoudre problème d'accès aux fichiers sur WordPress
CLOSED: [2024-01-18 Thu 10:25]
- Lorsque je réplique mon serveur d'application WordPress, je constate que WordPress rencontre régulièrement un problème pour afficher les images
  - Les fichiers que j'ai uploadé moi-même pour créer la page
- Probablement dû au fait que le volume qui stocke ces images n'est pas partagé par l'ensemble des instances
- Étudier comment faire évoluer l'application pour corriger cela
**** DONE Résoudre problème d'authentification sur WordPress
CLOSED: [2024-01-18 Thu 10:25]
- Lorsque je réplique mon serveur d'application WordPress, je me retrouve à devoir me reconnecter à chaque changement de page
- Probablement dû au fait que mon cookie d'auth est valide pour une instance donnée, et est invalidé par les autres
- Étudier comment faire évoluer l'application pour corriger cela
**** DONE Se familiariser avec le concept d'Infrastructure as Code (IaC)
CLOSED: [2024-01-22 Mon 09:11]
- Plutôt que de setup manuellement Kubernetes sur ses machines
- Semblerait que la pratique soit d'automatiser son setup
- Process connu comme l'Infrastructure as Code
- Se renseigner et documenter à ce sujet
- Ressources rapides
  - https://learn.microsoft.com/en-us/devops/deliver/what-is-infrastructure-as-code
  - https://aws.amazon.com/what-is/iac/
- Guillaume mentionnait notamment l'outil Vagrant
  - https://www.vagrantup.com
**** IN-PROGRESS Automatiser la création d'un cluster de Raspberry virtualisé
- Plutôt que de travailler avec le cluster physique
- Serait intéressant d'apprendre à provisionner un cluster virtuel de Raspberry
- Permettrait de pouvoir créer un environnement pour dev et effectuer des tests
  - Notamment comment setup k8s ou équivalent sur un tel système
**** IN-PROGRESS S'approprier SmartSense/smartsense-node-app
- On est en train de revoir les priorités avec l'arrêt maladie de Guillaume
- Serait préférable que je passe sur SmartSense en attendant son retour
- Les différentes tâches identifiées concernent principalement le capteur SmartSense
  - Ajout d'une couche réseau WiFi
  - Ajout de traitements en local des données collectées
    - i.e. sur le capteur
  - Correction des bugs rencontrés lors de l'utilisation de la BDD en local
    - Dans la version du capteur pour un fonctionnement autonome
    - Le capteur fait tourner une instance de la BD, InfluxDB, en local
    - Mais lors de leurs expériences, ont constaté des crashs de la BDD
      - Parfois au bout de plusieurs heures
- Il serait donc intéressant que je rentre rapidement dans ce projet
*** Done
- Résoudre problème d'accès aux fichiers sur WordPress
  - Plusieurs pistes possibles
  - Modification de la config du volume dans la description de l'application
    - La configuration que j'ai utilisé précise que le volume persistant est en ReadWriteOnce
    - i.e. qu'un seul noeud peut l'utiliser en mode RW
    - Cohérent avec le problème constaté
    - L'option ReadWriteMany permet de spécifier qu'il sera utilisé par plusieurs noeuds
      - Permettrait ainsi de rendre accessible les fichiers à tous les noeuds en ayant besoin
      - Solution limitée cependant
        - En interne, fonctionne avec un NFS d'après ce que m'explique Guillaume
        - Les performances sont donc pas adaptées à une charge conséquente
    - A testé avec cette configuration, sans succès
      - L'image, de nouveau, n'est pas chargée régulièrement lors de l'affichage de la page
    - La raison m'échappe
    - Je commence à avoir un doute sur le fait que le volume soit correctement partagé par les noeuds
    - En explorant les containers, j'ai observé des différences
      - kubetcl exec -it <container> -- /bin/bash
      - Seul un container possède le fichier uploadé
      - Un fichier créé manuellement dans un container n'apparait pas dans l'arborescence des autres
    - En creusant, j'ai remarqué que la configuration pour la gestion des volumes pour un cluster avait été perdue
    - En réactivant cette config, cela fonctionne
      - Fichier uploadé bien disponible sur l'ensemble des instances
      - Modifications manuelles sont bien observables sur l'ensemble des instances
    - Effet de bord intéressant mais intriguant : corrige aussi le problème d'authentification
      - Comprend pas la logique derrière
  - Ajout d'un service de sync des volumes
    - Je suis surpris de ne pas trouver d'articles qui présentent cette solution et détaillent comment la mettre en place
    - Ni de trouver un outil/composant qui assumerait ce rôle
    - Les gens reposent sur des solutions customs à base de rsync ?
    - Tombé sur *VolSync*, un système de réplication async entre volumes dans ou entre clusters
      - Disponible ici : https://github.com/backube/volsync
      - Projet Red Hat
      - Pas sûr que l'outil soit adapté pour répliquer des fichiers en temps réel vu les cas d'usages présentés
        - Plutôt  l'impression que c'est pour propager des données à terme, pour résilience ou traitements à posteriori sur les données
        - cf. https://next.redhat.com/2021/08/23/introducing-volsync-your-data-anywhere/
    - Demander des précisions sur le service mentionné par Guillaume
  - Est-il sinon possible d'override le fonctionnement de WordPress pour héberger les fichiers ?
    - Plutôt que d'essayer de retomber sur nos pattes en ajoutant des rustines
      - Partager un même volume entre pods
      - Sync les volumes de nos pods
    - Serait-il pas mieux et possible de faire déléguer à WordPress la gestion des fichiers à un service tiers, dédié à cela ?
      - Je suppose que WordPress est assez flexible pour cela
    - Genre mettre en place son propre CDN
- Résoudre problème d'authentification sur WordPress
  - Comme évoqué précédemment, partager un même volume entre les instances WordPress a pour effet de bord de résoudre ce problème
    - D'une manière que je ne comprends pas
    - Quoique
    - Cela s'explique si le serveur ne conserve aucune donnée en mémoire entre 2 requêtes
    - Et recréé l'état, e.g. de la session, à partir des infos fournies par la requête, e.g. cookies, et de fichiers
    - Une rapide recherche confirme ce mode de fonctionnement
  - Donc c'est une approche pour résoudre le problème des sessions, mais p-e pas la plus adaptée/conseillée
  - Guillaume m'a conseillé de regarder du côté des sessions PHP partagées
- Formation SED - Bonnes Pratiques du Dev Logiciel
  - Dans toute équipe de recherche, y a correspondant SED pour entrer en contact
  - Existe un GitLab pour projets avec données confidentielles
    - https://gitlab-int.inria.fr
  - Tests et intégration continue
    - Tests métiers
      - Vérifier que son logiciel est compris et utilisé par les personnes devant l'utiliser
    - Analyse statique
      - Mettre en place linter/convention de codage
      - Utiliser des outils de vérification de la qualité du code
        - e.g. sonarqube
        - Voir si facilement pluggable/hookable au gitlab
    - Documentation
      - Existe gitlab-pages
      - Recommandations sur comment écrire la doc : https://smartbear.com/blog/13-things-people-hate-about-your-open-source-docs/
  - Licences conseillées
    - MIT
    - GPL/LGPL
    - BSD
- Prendre en main le script de connexion aux brokers MQTT
  - Dépendance
    - Ce script nécessite pour fonctionner l'installation d'une librairie pour instancier un client MQTT
      - pip install paho-mqtt
  - Ce script se connecte à ou plusieurs brokers MQTT de la plateforme SmartSense
    - Choix codé en dur
  - Il récupère les messages postés sur le topic, les décode et les affiche
  - S'interrompt au bout d'une minute
  - Fonctionne
  - Particulier au niveau de la connexion aux topics
    - Tous les topics n'ont pas l'air d'utiliser la même configuration
    - Notamment au niveau du port utilisé
      - Pourquoi ?
  - Semble associer un port différent à chaque zone
    - Plusieurs zones à Lannion, plusieurs à Rennes
  - Semble retrouver cette information directement dans l'URL d'un topic
    - e.g. "event/p/production/Rennes/0/E/..." indique un topic d'un capteur situé à Rennes, dans le secteur 0
    - Si j'ai bien pigé
    - Un peu confus sur la signification du champ suivant
      - Vaut 3 ou E en fonction des topics présentés
    - D'après la doc, indique le sous-secteur
      - Doc dispo ici, page 8 : https://gitlab.inria.fr/smartsense/3douest/documents/conception/-/blob/master/CDC-20190806-Design%20MQTT%20et%20InfluxDB-V1.4.pdf
- Se familiariser avec le concept d'Infrastructure as Code (IaC)
  - Un peu de mal à piger si Vagrant est adapté à notre use-case
    - Outil permettant de déployer des environnements
      - Instancie des VMs selon la configuration donnée
      - Exécute le/les scripts fournis
      - Copie une partie du FS dans la VM
    - On a probablement pas envie de déployer notre application à même les raspberry du serveur
    - Mais voulons-nous utiliser des VMs pour autant ?
    - Préférions-nous pas utiliser simplement des conteneurs ?
      - Installer k8s (ou plutôt k3s probablement) sur les raspberry
      - Configurer le cluster pour définir les noeuds et leurs rôles
      - Déployer l'application
  - A un intérêt donc, mais plutôt pour la partie dev/testing IMO
    - Pour provisionner/recréer le cluster de raspberry localement
  - Est-ce que son rôle s'arrête là ?
    - i.e. utiliser un autre outil pour setup le cluster k8s ?
      - Ansible ?
  - Serait intéressant de voir avec Khaled ce qu'il fait dans le cadre de ses expériences
    - Est-ce qu'il simule des raspberry ?
    - Quels outils il utilise ?
      - Pour provisionner les machines virtuelles
    - Est-ce qu'il a mis ses ressources, configurations et scripts à disposition ?
      - Genre dans son article
  - Il m'a donné accès à son repo avec tout les scripts pour son setup experimental
    - Dispo ici : https://gitlab.inria.fr/stream-processing-autoscaling/scalehub
  - De ce que j'ai compris
    - Réserve des noeuds sur g5k
    - En utilisant les IPs des machines attribuées, lance un script Ansible qui les préparent à la configuration
      - Setup SSH
    - Puis exécute un script Ansible qui installe k3s sur les noeuds
      - A rencontré des problèmes pour setup k3s, mais ne se souvient plus quoi
      - A commencé à creuser l'alternative mini-k0s
      - Mais a résolu son problème sur k3s
    - À partir de là, fait tout par le biais de k3s
      - Déploie des services supplémentaires en fonction de ses besoins
      - Prometheus, Grafana, Kafka, Flink
  - Par contre, ne fait aucune virtualisation des machines
    - Va falloir que je me débrouille pour cette partie là
  - Parcours le livre *Infrastructure as Code* de Kief Morris
- Automatiser la création d'un cluster de Raspberry virtualisé
  - Ressources :
    - Tuto suivant a l'air plutôt complet sur comment virtualiser une Raspberry : https://linuxconfig.org/how-to-run-the-raspberry-pi-os-in-a-virtual-machine-with-qemu-and-kvm
    - Celui-ci a l'air plus d'actualité : https://interrupt.memfault.com/blog/emulating-raspberry-pi-in-qemu
    - Ou celui-ci : https://brettops.io/blog/custom-raspberry-pi-image-no-hardware/
  - Pose la question de l'outil VM à utiliser
    - Souhaite émuler du ARM
    - Est-ce une bonne idée ?
  - Essayons, on jugera à l'essai
  - Pars donc sur l'émulation d'une Raspberry à l'aide de QEMU
  - Suivi du tuto https://interrupt.memfault.com/blog/emulating-raspberry-pi-in-qemu
  - Erreur rencontrée avec la dernière image de Raspberry OS
    - OS : https://downloads.raspberrypi.com/raspios_lite_arm64/images/raspios_lite_arm64-2023-12-11/2023-12-11-raspios-bookworm-arm64-lite.img.xz
    - usbnet: failed control transaction: request 0x8006 value 0x600 index 0x0 length 0xa
    - Aucun autre message ne s'affiche dans le terminal, qui ne répond plus
  - J'ai re-essayé en utilisant cette fois-ci la version précédente de l'OS
    - OS : https://downloads.raspberrypi.com/raspios_oldstable_lite_arm64/images/raspios_oldstable_lite_arm64-2023-12-06/2023-12-05-raspios-bullseye-arm64-lite.img.xz
    - Cette fois-ci, la Raspberry a l'air de se lancer
      - Retrouve l'erreur parmi les logs, mais n'a pas l'air bloquante
    - Pu in fine me logger au système
  - Ok, comment on instrumentalise ça avec Vagrant maintenant ?
  - Plutôt voir déjà comment on automatise le lancement de VMs avec Vagrant
  - Suivi le tuto : https://developer.hashicorp.com/vagrant/tutorials/getting-started
    - Étrangement, n'utilise pas virtualbox en provider par défaut
      - Je ne l'avais pas installé au moment où j'ai installé vagrant, probablement pour cela
      - Utilise donc libvirt à la place
      - Sauf que libvirt n'a pas l'air compatible avec toutes les boxes
        - Genre, celle du tuto
      - J'ai set la variable d'env
        - VAGRANT_DEFAULT_PROVIDER="virtualbox"
      - Mais n'a aucun impact à la création d'un env
    - Doit donc spécifier le provider au démarrage
      - vagrant up --provider="virtualbox"
    - A pu démarrer la VM, une Ubuntu 18.04.3, et s'y connecter en SSH
      - vagrant ssh
  - Maintenant, comment on lance plusieurs VMs avec Vagrant ?
  - Ça se fait bien, suffit d'en définir plusieurs dans le fichier de config
    - https://gitlab.inria.fr/mnicolas/vagrant-getting-started/-/blob/382bfe988d13dfbe450cb0b5e3ee459bfd70cdbd/Vagrantfile
  - Temps de s'intéresser à la partie réseau maintenant
  - Notamment comment SSH une VM depuis l'autre, et inversement
- S'approprier SmartSense/smartsense-node-app
  - Point d'entrée est mainApp
    - Instancie les différents composants logiciels du capteur
  - Pour le moment, identifie les composants suivants
    - ClientMQQT & PublisherMQQT
    - INAManager
    - USBManager
    - NTPManager
    - RpiGpioManager
    - FirmwareUpdateManager
    - TLCManager
    - Syncer
  - MQQT
    - Système de message brokers
    - Permet au noeud de communiquer avec le serveur
      - Remonter les données collectées
      - Mais aussi de recevoir des instructions
        - e.g. changement de configuration
  - NTPManager
    - Système de synchronisation d'horloges du noeud avec le serveur central
    - Du sens si on utilise une timeseries database
  - RpiGpioManager
    - Gère l'alimentation des ports GPIO de la Raspberry
    - À quoi correspondent ces ports ?
    - Dans le code, on retrouve la mention de
      - Sensor Board
      - Ext1 et Ext2
    - Peut supposer que la sensor board est la carte sur laquelle sont branchés les différents capteurs
    - Tandis que Ext1 et Ext2 correspondent aux ports disponibles pour brancher des extensions supplémentaires
      - cf. https://gitlab.inria.fr/smartsense/3douest/documents/conception/-/blob/master/SMARTSENSE-Module%20d'extension%20pour%20noeud.pdf
  - mainApp
    - Instancie l'ensemble des composants
    - Met en place les processus de contrôle périodique du bon fonctionnement du capteur
    - Sync son horloge
    - Alimente les différents capteurs du noeud
    - Démarre le process mqttManagerCmdProvisioning
      - Lit en boucle la Queue donnée en entrée en quête de messages
      - Dès qu'un message est détecté, le publie au MQTT Broker
    - Pas sûr de comprendre les lignes suivantes
      - https://gitlab.inria.fr/smartsense/3douest/node-app/smartsense-node-app/-/blob/main/mainApp.py?ref_type=heads#L378-381
      - Envoie par le biais du message broker un message au serveur
      - Mais dans quel but ? Que signifie provisioning dans ce contexte ?
  - Plusieurs réflexions sur le code et projet
    - Absence d'un gestionnaire de dépendances
    - Absence de tests
      - Pose la question de comment développer et tester
      - Est-ce que développe à même le capteur ?
      - Ou fait tourner le programme dans une VM pour tester ?
      - Est-ce que l'architecture différente ARM implique des étapes supplémentaires ?
      - Globalement, qu'est-ce qu'il faudrait faire pour mettre en place un process de CI ?
    - Absence de linter
    - Quelle méthodologie de travail ?
      - Si je veux faire des modifs
    - Absence de doc
  - Réflexions sur mainApp
    - startProcess()
      - Pourrait utiliser un enum plutôt que des chaines de caractères pour spécifier le process à démarrer
        - cf. https://docs.python.org/3/library/enum.html
      - À l'exception de usbBoardManager, la logique est la même pour chaque process
        - Seul l'instanciation du process change
        - Pourrait factoriser le code
  - Réflexions sur MqttInterface
    - run()
      - Pourquoi while True and self.stopLoop is False ?
        - Et non pas while not self.stopLoop ?
      - Pourquoi une attente active sur la Queue ?
        - Pas possible d'utiliser de l'event-based ?
        - Semblerait que non, d'après l'API
          - cf. https://docs.python.org/3/library/multiprocessing.html
      - Pourquoi le type Queue ?
        - Permet de passer des messages
        - Canal de diffusion avec plusieurs producers et subscribers possibles
          - cf. https://docs.python.org/3/library/multiprocessing.html#pipes-and-queues
        - Mais à messages à usage unique
          - i.e. lire un message le consomme
        - Ne supporte pas le pattern fan out du coup
        - Pas le plus pratique si on veut déclencher plusieurs traitements pour un même message
          - e.g. stocker en local et diffuser sur le réseau
        - L'utilisation que j'en vois pour le moment est d'un composant à un autre
          - De mainApp à mqqtInterface par exemple
        - Voir si c'est la structure de données la plus adaptée à notre use case finalement
      - Pourquoi prend en paramètre queueDataIn et queueDataOut ?
        - Puisque n'utilise pas queueDataOut de toute la méthode
        - Ne pourrait-on pas passer ces attributs au constructeur plutôt ?
** Semaine du <2024-01-08 Mon> au <2024-01-12 Fri>
*** Planned
**** DONE Régulariser situation du 02/01
CLOSED: [2024-01-09 Tue 14:36]
**** DONE Suivre cours de Guillaume sur les technologies cloud
CLOSED: [2024-01-10 Wed 11:47]
- Disponible ici : https://gitlab.inria.fr/pierre/sct-m1info
**** DONE Trouver des ressources sur Docker & Kubernetes
CLOSED: [2024-01-10 Wed 13:49]
- Au-delà du cours de Guillaume, existe des ressources pour rentrer plus en détails sur ces outils (talks, livres)
- Voir pour en trouver et les consulter
**** DONE Regarder *Kubernetes Design Principles: Understand the Why*
CLOSED: [2024-01-11 Thu 15:57]
- Talk en 2018 de Saad Ali, ingé Google de l'équipe sur k8s
  - Dispo ici : https://www.youtube.com/watch?v=ZuIQurh_kDk
**** DONE Adapter la configuration réseau pour clusters multi-nodes
CLOSED: [2024-01-11 Thu 16:42]
- Lors de l'ajout du 2nd Node à mon cluster minikube, j'ai eu le warning suivant
  - Cluster was created without any CNI, adding a node to it might cause broken networking.
- Voir ce que cela signifie et ce que je dois modifier
**** DONE Utiliser un driver pour Volume adapté aux clusters multi-nodes
CLOSED: [2024-01-11 Thu 17:01]
- La page tuto de k8s indiquant comment lancer un cluster multi-nodes mentionne un problème avec le driver pour Volume par défaut
  - https://minikube.sigs.k8s.io/docs/tutorials/multi_node/
- Renvoie à la page suivante :
  - https://minikube.sigs.k8s.io/docs/tutorials/volume_snapshots_and_csi/
- Voir si le problème est toujours d'actualité et si c'est bien la solution conseillée
**** DONE Prendre en main Kubernetes
CLOSED: [2024-01-12 Fri 13:15]
- J'ai atteint la partie du cours de Guillaume présentant Kubernetes
- Voir maintenant pour expérimenter avec histoire de creuser l'outil
- Ressources disponibles :
  - Le TP du cours de Guillaume : [[file:~/Documents/sct-m1info/support/pdf/tp08.orchestration.pdf]]
  - Le tuto de Digital Ocean sur faire fonctionner Kubernetes en local : https://www.digitalocean.com/community/tutorials/how-to-use-minikube-for-local-kubernetes-development-and-testing
**** DONE Déployer une application complexe avec k8s
CLOSED: [2024-01-15 Mon 08:45]
- Les tutos que je suis pour le moment se contentent de déployer des applications simples
  - I.e. Un pod faisant tourner un nginx
- Pour apprendre correctement k8s, serait intéressant de déployer une application composée de
  - Serveurs d'applications, répliqués
    - Avec un load balancer pour répartir la charge
  - Interagissant avec une BDD
    - Elle aussi répliquée ?
- Cela permettrait de creuser
  - La configuration et le déploiement de pods différents
  - Les interactions entre ces pods, potentiellement sur des noeuds différents
  - L'utilisation de volumes
  - L'utilisation de fichiers de description
- Exemple
  - *Deployment of multiple apps on Kubernetes cluster — Walkthrough* : https://wkrzywiec.medium.com/deployment-of-multiple-apps-on-kubernetes-cluster-walkthrough-e05d37ed63d1
*** Done
- Suivre cours de Guillaume sur les technologies cloud
  - CM5 - Services cloud réseau
    - S'intéresse aux différents services réseau mis à disposition par les cloud providers
    - Bien beau d'instancier des VMs/conteneurs
    - Mais doit leur attribuer une adresse IP privée
      - Et une adresse IP publique pour ceux qui doivent pouvoir être contactés de l'extérieur
      - Possède un pool d'adresses IPs qui vont être attribuées dynamiquement aux instances
    - Doit créer les routes de communication entre ces instances, et entre ces instances et le monde extérieur
      - Utilise des VLANs et probablement des techniques de SDNs
    - Doit aussi considérer l'aspect sécurité
      - Mettre en place des pare-feux, VPNs
      - Provider clouds proposent des services de pare-feux
        - FWaaS : FireWall as a Service
    - Finalement, pour la scalabilité, doit généralement mettre en place du load balancing
      - LBaaS : Load-Balancing as a Service
    - Questions
      - C'est quoi exactement la différence entre VLANs et SDNs ?
        - P-e lire un peu à ce sujet
        - *Cloud Network Virtualization: Benefits of SDN over VLAN*
          - Blogpost disponible ici : https://cloudsecurityalliance.org/blog/2021/06/25/cloud-network-virtualization-benefits-of-sdn-over-vlan/
          - De ce que je comprends, les VLANs ont initialement été conçus pour créer plusieurs réseaux virtuels au sein d'un même réseau local
            - Limité au sein du LAN
          - Pas les mêmes conditions que le cloud
            - Un single-tenant vs. multi-tenant
            - Pas la même échelle
          - Ne sont donc pas adaptés à ce nouveau cas d'usage
            - Particulièrement d'un point de vue sécu/isolation
          - L'approche SDN répond à ce nouveau besoin
            - Découple le /control plane/ du /data plane/, i.e. découple le routing de l'envoi effectif des messages
              - Un peu de mal à piger les implications de cela
              - Cela me paraît évident que ça doit être découplé
              - Ne dois pas comprendre les contraintes matérielles
            - Permet de configurer plus finement et simplement les firewalls
              - Adopte la politique du /default deny/, contrairement à l'existant
            - Protège d'attaques nativement
            - Conçu pour l'élasticité
        - *Network Virtualisation and the difference with VLANs, SDNs*
          - Blogpost disponible ici : https://craigread.cloud/network-virtualisation-and-the-difference-with-vlans-sdns/
          - Re-explique qu'un VLAN permet de diviser un LAN en de multiples réseaux
          - Explique que le VLAN n'est pas de la virtualisation de réseau
            - Pas moyen de prendre une snapshot du réseau, de le cloner ou déplacer
            - Pas sûr de comprendre de ce qu'on entend par cloner un réseau concrètement
              - Et de l'usage qu'est fait de cette fonctionnalité
          - Précise aussi que SDN n'est pas de la virtualisation non plus
            - Ne virtualise pas les composants, e.g. switchs et routeurs
            - Mais permet de les contrôler logiciellement
          - Mais que la virtualisation de réseau existe belle & bien
            - Permet de virtualiser le réseau complet, hardware compris
          - Quand utiliser SDN vs. Network Virtualisation ?
  - CM6 - Microservices
    - Porte sur l'évolution de l'architecture système des applications
    - Anciennement, architecture monolithique
      - Simple
      - Mais des limites
        - Pas de contrôle de droits d'accès sur les données par domaine/métier
        - Un bug d'un domaine/métier de l'application peut la faire crasher dans son entièreté
          - i.e. pas d'isolation
        - Difficile à scale
          - La base de données est un bottleneck
          - De part le fonctionnement des writes et des transactions
    - Architecture orientée micro-services
      - Décompose l'application en multitude de services
      - Chaque service doit avoir une fonctionnalité précise
        - Separation of Concern
      - Les services peuvent communiquer entre eux, si nécessaire, par le biais de leur API
      - Chaque service est responsable de ses données
        - Chaque service peut ainsi choisir ses outils, i.e. son SGBD, en fonction de ses use cases
      - Principes d'une architecture orientée micro-services
        - Se base sur : https://nirmata.com/2015/02/02/microservices-five-architectural-constraints/
        - Elastic : chaque service doit pouvoir scale up/down de manière indépendante des autres services
        - Resilient : un service doit crasher sans impacter les autres services
        - Composable : les services doivent proposer des APIs uniformes et conçues pour la composition
        - Minimal : un microservice doit être composé uniquement des entités fortement liées
        - Complete : un microservice doit être fonctionnellement complet
      - Pour la communication entre services, une approche éprouvée est d'utiliser un message broker
        - Permet de découpler les composants
        - Pas de blocage pour l'initiateur d'une requête pendant le calcul de la réponse
        - Permet de scale le service produisant la réponse en fonction de la workload de manière transparente
    - Aborde ensuite l'approche DevOps
      - Là aussi, devrais lire plus à ce sujet
      - *What is DevOps*
        - Disponible ici : https://about.gitlab.com/topics/devops/
        - Méthodologie consistant à coupler les tâches des équipes de développement et d'opérations (déploiement)
        - A pour but de
          - Mettre en place un cycle de développement incrémental
          - Livrer rapidement les nouvelles versions du logiciel
          - Améliorer la qualité du logiciel
        - Cela passe par
          - Collaboration approfondie entre les équipes dev et ops
            - Des équipes à objectifs intrinséquemment différents et parfois contraire
              - Dev : Faire évoluer rapidement l'application pour répondre aux retours
              - Ops : Garantir le bon fonctionnement de l'application
            - L'idée est ici de les faire faire cause commune
          - Incorporation et automatisation de bonnes pratiques
            - Tests, Livraison, Déploiement
        - Se base sur les 4 principes suivant
          - Automatisation des phases du cycle de vie du logiciel
            - Test, build, release
          - Collaboration et communication
            - Entre les anciennes différentes équipes
          - Amélioration continue et minimisation des pertes de temps
            - Automatisation des tâches répétitives
            - Identification perpétuelle de pistes d'amélioration
          - Focalisation sur les besoins des utilisateur-rices
            - L'automatisation des tâches permet de se focaliser sur les retours des utilisateur-rices
            - Et livrer rapidement une nouvelle version y répondant grâce à l'accélération du cycle de vie de l'application
  - CM7 - Conteneurs et Docker
    - Présente Docker
    - Rappelle qu'on a un intérêt à virtualiser
      - Permet d'isoler les différents composants d'une application
      - D'embarquer l'ensemble des dépendances
      - Et d'éviter les potentiels conflits, e.g. dépendances incompatibles
    - Mais que les VMs sont volumineuses, lentes à instancier et ajoutent un surcoût computationnel
    - Les conteneurs répondent aux mêmes problématiques
    - Mais de manière plus efficace
      - Reposent sur l'OS de la machine
        - Permet d'éviter l'utilisation coûteuse d'un hyperviseur
      - Reposent sur le système de layers
        - Permet de partager/factoriser des mêmes layers entre conteneurs
    - Précise cependant que Docker n'est un outil nativement conçu pour un usage dans le cloud
      - Conçu plutôt pour tourner sur une machine donné
    - Un orchestrateur est nécessaire pour cela
  - CM8 - Kubernetes et Orchestration de conteneurs
    - Les conteneurs, c'est bien
    - Mais dans un environnement cloud, ils ne sont pas suffisants par eux-mêmes
    - Entre autres, des besoins de
      - Scaling automatique
      - Détecter et redémarrer les conteneurs ayant une panne
      - Mettre en place des configurations réseaux avancées
    - Kubernetes permet de répondre à ces besoins
    - Notion de pod
      - Kubernetes permet de créer des pods
      - Un pod contient un ou plusieurs conteneurs et volumes
      - Et possède une adresse IP pour le tout
      - *NOTE* Si un élément du pod rencontre une panne, Kubernetes tue le pod entier
      - Pour créer pods, se basent sur des fichiers de description
        - À la *docker-compose*
    - Insiste sur le fait qu'il *ne faut pas utiliser un unique pod*
      - Pod peu gourmand, n'utilise qu'une fraction des ressources du noeud
      - Pod éphémère, peut être tué par Kubernetes de manière inopinée, sans sommation
    - À la place, *utiliser un groupe de pods identiques*
    - Notion de Controller
      - Kubernetes est un outil déclaratif
        - Users n'indiquent pas quelles commandes effectuer
        - Mais quel est l'état désiré
        - Kubernetes se charge de transitionner de l'état courant à cet état cible
          - [[file:img/kubernetes-reconciliation-loop.png]]
      - Propose plusieurs types de controllers
        - /Deployment/ a l'air d'être le controller "par défaut"
        - /StatefulSet/ pour les applications stateful
          - À la mort d'un pod, le recréé en réutilisant le même volume
        - /Job/ pour les tâches courtes
        - /DeamonSet/ pour que tous les noeuds matchant un critère démarre une instance d'un pod
          - Prend en compte les noeuds qui apparaissent au cours de la vie de l'application
        - Possibilité de créer de nouveaux controllers si besoin
      - Commandes existent pour manipuler directement les controllers
        - E.g. pour déployer une application
        - Étrangement, le niveau de granularité a l'air d'être sur l'image Docker et non pas le pod
      - Mais fonctionne aussi via des fichiers de description
      - Comment ça marche si application nécessitent de combiner plusieurs controllers ?
        - Un fichier unique ?
        - Ou un ensemble de fichiers de descriptions ?
    - Controllers incorporent des mécanismes supplémentaires
      - E.g. *Rolling Updates* : déploie progressivement de nouveaux pods se basant sur une nouvelle image puis interrompt les anciens pods
    - Kubernetes déploient aussi des Services
      - Sert de front-end pour les pods
      - Observe les pods pour déterminer à quel pod transmettre une requête
      - Se base pour cela sur un (des?) Selector(s)
        - Comment fonctionnent-ils ?
        - Possibilité/Besoin d'en faire des customs ?
    - D'un point de vue réseau
      - Communications entre containers se font via localhost
      - Communications entre pods (d'un même noeud) se font via les adresses IPs uniques des pods
      - Communications entre pod et service se font via l'adresse IP unique du service
      - Comment un container découvre l'adresse IP d'un pod/du service ?
    - Précise que Kubernetes ne repose sur le runtime Docker depuis sa v1.20
      - Utilise toujours les images Docker
      - Mais utilise un (des?) runtime(s) plus efficaces et standardisés
      - Quid des volumes et networks ?
        - Ne reposent pas du tout sur les solutions proposées par Docker ?
      - Est-ce que ça a un impact sur la façon de créer ses images Docker ?
- Réunion avec Guillaume le <2024-01-10 Wed>
  - Préparation
    - HS RH
      - A fait une demande de régularisation de congé pour le 02/01
      - A permis de détecter quelques problèmes
        - Personne qui valide mes demandes de congés
        - Jours reportés de l'an dernier
    - Technologies Cloud
      - Suivi le cours jusqu'au CM sur l'orchestration
        - M'a permis de revoir les bases
          - I/P/SaaS
            - Un peu de mal à délimiter PaaS
          - Infrastructures et Services
            - Ne connaissais pas OpenStack
            - Et que certaines organisations mettaient en place leur cloud privé
            - Par contre, est-ce qu'on retrouve les mêmes outils dans le fog ?
              - Ou est-ce trop gourmand ?
          - Services de stockage
            - Les SGBDs relationnels sont si peu adaptés au cloud ?
            - Pas trop creusé le sujet, mais j'entendais parler de NewSQL
      - Commence à expérimenter avec k8s
        - Installé minikube sur ma machine
        - En train de parcourir les tutos sur créer cluster, déployer simple application web
        - Et d'apprendre les concepts (Pods, Nodes, Services, Deployment...)
        - Curieux du fonctionnement du Control Plane pour qu'il ne soit pas un SPOF
        - Surpris que k8s soit pas un environnement unique, mais une multitude de distribution
          - Ai vu qu'il y a des distribs faites pour l'IoT : k3s, k0s
      - As-tu des ressources que tu conseilles, notamment sur Docker & Kubernetes ?
        - Understanding Docker/Kubernetes in a visual way par Aurélie Vache
    - Observatoires
      - Consulté le site d'Ammar sur les résultats de son questionnaire
        - Et débriefé avec lui
      - M'a permis de constater la grande hétérogénéité des observatoires
        - Source d'énergie, réseau disponible, etc.
      - Quels sont nos objectifs ?
        - À qui on s'adresse ?
        - Quelles sont nos contraintes ?
      - Ammar m'a parlé d'OZCAR et m'a linké un article
        - Prévois de le lire pour mieux comprendre les enjeux des observatoires
  - Notes
    - Deployment
      - Outil de base de k8s
    - Peut associer un Service LoadBalancer à un Deployment
    - k8s se focalise sur l'état desiré et l'état observé
      - Enregistre dans BDD l'état désiré
      - Puis observe son état
        - Outil de monitoring souvent ajouté : Prometheus
    - Voir du côté de Vagrant
      - Infrastructure as Code
        - Décrit l'infrastructure que l'on souhaite déployer via des services Cloud
      - Vagrant est l'équivalent local
        - Utilisé dans LivingFog
      - Permet de déployer Kubernetes et consorts
    - Observatoires
      - Nous nous intéressons aux observatoires
        - Isolés
        - Variétés de capteurs
        - Variétés d'utilisateurs
        - Contraintes sur énergie et bande-passante
      - Mais aurons quand même grande hétérogénéité
        - Type de tâches
        - Volume de données
      - Sujet à considérer est la problématique du changement
        - Comment accompagner les scientifiques dans l'adoption de la solution que l'on va proposer ?
        - P-e voir avec les ingés du service d'hydrologie pour déployer nos essais
          - Ont mis en place un petit observatoire au niveau du ruisseau
            - Avec capteurs
            - Et autres ?
- Régulariser situation du 02/01
  - A envoyé une demande de régularisation
  - Sur les conseils de Myriam, en a profité pour notifier des problèmes de
    - Personne qui valide mes demandes de congés
    - Jours reportés de l'an dernier
  - Demande a été traitée
- Prendre en main Kubernetes
  - Plutôt que de faire tourner l'environnement kubernetes en complet sur sa machine
  - Semble plus commun d'utiliser un outil pour virtualiser le cluster et les différents composants de k8s
  - Plusieurs outils existent
    - minikube : https://github.com/kubernetes/minikube
      - Outil dev par l'équipe de k8s
    - kind : https://github.com/kubernetes-sigs/kind
      - Outil dev par l'équipe de k8s
      - Conçu initialement pour tester k8s
      - Indiqué comme pouvant être aussi utilisé pour le dev d'applis locales
  - Plusieurs blogposts font des comparaisons entre ces outils
    - https://www.blueshoe.io/blog/minikube-vs-k3d-vs-kind-vs-getdeck-beiboot/
    - https://shipit.dev/posts/minikube-vs-kind-vs-k3s.html
    - https://alperenbayramoglu2.medium.com/simple-comparison-of-lightweight-k8s-implementations-7c07c4e6e95f
    - Pour prendre en main k8s, les différentes options semblent se valoir
      - [[file:img/kubernetes-distrib-comparaison.png]]
  - Je croyais que k8s était un logiciel/environnement unique
  - Mais il semble y avoir une multitude de distributions différentes
    - Notamment des distribs conçues pour/orientées IoT & Edge
    - K3s : https://github.com/k3s-io/k3s
    - MicroK8s : https://github.com/canonical/microk8s
  - Pour démarrer, suis le tuto : https://www.digitalocean.com/community/tutorials/how-to-use-minikube-for-local-kubernetes-development-and-testing
    - Quelques difficultés à la première étape
      - minikube plantait silencieusement
      - Ajouter l'option /--driver=docker/ a permis de dépasser l'erreur rencontrée
        - Ai ajouté l'option à ma config par défaut
          - minikube config set driver docker
    - Ai pu suivre le reste du tuto sans erreurs
    - Pas trop compris les points suivants
      - kubectl create deployment web --image=gcr.io/google-samples/hello-app:1.0
        - Permet de créer un deployment nommé web en utilisant l'image passée en option
        - Mais c'est quoi un deployment ?
        - Options notables de la commande create deployment
          - --replicas=X : permet d'indiquer un nombre de replicas initial
          - --port=Y : permet d'exposer le port donné
        - C'est créé sur un ou plusieurs noeuds ?
      - kubetcl expose deployment web --type=NodePort --port=8080
        - Permet de créer un service qui expose la ressource demandée
        - À quoi correspondent les options --type et --port ?
        - --port
          - Le port sur lequel écoute l'application du ou des pods
          - Des pods ou des noeuds ?
        - --type
          - Le type de service qui va être créé
          - Ici, je suppose que c'est un service simple qui se contente de faire du port forwarding
          - Plus d'infos ici : https://kubernetes.io/docs/concepts/services-networking/service/#type-nodeport
          - Cette page précise qu'on peut aussi passer comme valeur LoadBalancer
          - Permet de provisionner un load balancer fournit par le cloud provider
          - Quid dans minikube ?
            - Le tuto de k8s le fait faire
            - Pas d'erreur reportée, service fonctionnel
  - Passe maintenant à : https://kubernetes.io/docs/tutorials/kubernetes-basics/
    - Cluster
      - Ensemble composé de Nodes et du Control Plane
      - Node
        - Machine, potentiellement virtuelle, qui sert de worker pour l'application
        - Va faire tourner des Pods
        - Chaque noeud possède un Kubelet
          - Agent qui gère le noeud et sa communication avec le Control Plane
      - Control Plane
        - Orchestrateur qui gère la maintenance de l'état de l'application, son passage à l'échelle et ses rolling updates
        - Quelles garanties sont offertes par le Control Plane ?
          - Est-il distribué ? Comment fonctionne-t-il ? Quel impact sur son comportement en cas de panne d'une des répliques ?
    - Deployment
      - Permet de décrire l'état souhaité
      - Un Deployment Controller, géré par le (faisant partie du ?) Control Plane, va ensuite monitorer l'état de l'application et instancier/retirer des Pods au besoin pour obtenir l'état souhaité
    - Le tuto fait remarquer que, au moment de lancer une application, on a un seul Node de disponible
      - Le Node qui fait aussi tourner le Control Plane
    - On peut cependant lancer une application
      - Celle-ci tournera alors sur le même Node que le Control Plane
    - Me paraît mieux de modifier le setup de base pour avoir plusieurs noeuds
      - Au moins 2, le Control Plane et un Worker
      - Pour cela, suis tutos :
        - https://minikube.sigs.k8s.io/docs/tutorials/multi_node/
        - https://medium.com/cloudnloud/how-to-minikube-with-multi-node-setup-1159006fc80e
      - Commandes
        - Créer cluster : minikube start --nodes=2
        - Ajouter Node à cluster existant : minikube node add
          - À voir comment on précise à quel cluster on ajoute ce Node
      - Semble y avoir un problème avec le driver par défaut pour les Volumes dans un cluster multi-nodes
        - https://minikube.sigs.k8s.io/docs/tutorials/volume_snapshots_and_csi/
        - Voir ce que cela implique et corriger si besoin
      - Minikube m'a aussi affiché un warning lors de l'ajout du Node
        - Cluster was created without any CNI, adding a node to it might cause broken networking.
        - Voir ce que cela implique et corriger si besoin
- Trouver des ressources Docker & Kubernetes
  - Guillaume m'a passé le livre *Understanding Kubernetes in a visual way* par Aurélie Vache
  - Elle a aussi fait une série de vidéos sur le sujet :
    - https://www.youtube.com/watch?v=a1Uwoq1Yv6U&list=PLmw3X80dPdlzksg6X9s23LEkLMWFGGUn5
  - Aussi trouvé les vidéos suivantes qui ont l'air pertinentes
    - *Kubernetes Design Principles: Understand the Why* : https://www.youtube.com/watch?v=ZuIQurh_kDk
    - *Kubernetes Explained in 6 Minutes | k8s Architecture* : https://www.youtube.com/watch?v=TlHvYWVUZyc
  - Me parait un bon début
- Discussion avec Éric Poiseau et Olivier Sentieys
  - En réponse au mail de Guillaume informant les autres membres du projet SmartOps, Éric m'a proposé de passer le voir
  - Il m'a présenté le SED et s'est occupé de quelques démarches
    - Ajout à la mailing list ingedev
    - Ajout au mattermost devel
    - Ajout au groupe gitlab SmartSense
    - Présentation rapide de l'AGOS
  - A insisté sur le fait que je passe le voir si je rencontre des difficultés ou ai besoin d'un avis extérieur
  - M'a ensuite présenté à Olivier Sentieys
    - Pensais qu'il était basé à Lannion
    - Mais semble être revenu à Rennes
    - Seul Mickael Le Gentil est basé à Lannion donc
  - M'ont parlé du projet SmartSense
  - Présenté les capteurs SmartSense
    - Carte sur laquelle les capteurs sont branchés
    - Interfacée avec une Raspberry Pi (version 3 si j'ai bien suivi)
      - Permet d'avoir un peu de moyen de calculs localement
      - Et appliquer des traitements sur les données avant de les remonter
      - Notamment, plutôt que de transmettre le flux vidéo/audio
      - Peut traiter ces flux pour remonter des métriques telles que présence de personnes dans la salle, nombre de personnes, type de sons identifiés
      - Permet ainsi de préserver l'anonymat et de limiter l'usage de bande-passante
    - Branché sur secteur pour l'alim électrique
    - Connecté en ethernet pour remonter les données
    - Existe une version adaptée pour l'extérieur
      - Fonctionne sur batterie
      - Et stocke les données collectées sur carte SD, à récupérer manuellement
  - Montré https://co2.irisa.fr/
    - Permet de suivre l'évolution de métriques remontées par les capteurs SmartSense d'une salle donnée
      - e.g. taux de CO2, la température
    - Surprenamment, n'interroge pas la BDD
    - Mais récupère et présente les métriques seulement à partir de l'instant T
    - N'a plus trop l'air de fonctionner
      - Affiche les données à un instant donné au moment où j'accède à l'application
      - Mais n'a pas l'air de récupérer/d'afficher de nouvelles données si je reste sur la page
        - Temps réel ? Fréquence d'échantillonnage ?
      - Plus troublant, la date donnée par le capteur SmartSense est incorrecte
        - [[file:img/2024-01-11-screen-co2-irisa.png]]
      - Et n'a pas l'air de s'actualiser régulièrement
      - Une slide de l'ADT mentionne que les données collectées sont transmises à une time series DB, InfluxDB
    - Est-ce que ça ne pose pas de problème d'avoir des données estampillées incorrectement ?
    - J'ai rien dit
    - En me reconnectant sur le site, je suis tombé sur une salle dans laquelle il y avait une réunion au même moment
    - J'ai ainsi pu faire les capteurs en cours de fonctionnement
      - [[file:img/2024-01-11-screen-2-co2-irisa.png]]
    - L'interface affiche ainsi les nouvelles entrées
      - Une mesure toutes les 20s semblerait
    - Les capteurs sont donc inactifs entre les réunions ?
      - Comment cela fonctionne ?
  - Montré https://smartsense-gest.inria.fr/
    - A l'air d'être une interface de gestion des capteurs
    - M'ont créé un compte, mais ne dispose d'aucun droit
  - Premières pistes de travail concernant SmartSense
    - Rencontrer Guillermo Andrade-Barroso
      - Ingénieur du SED qui a été impliqué de manière plus importante dans le projet SmartSense
      - Aura probablement une meilleure compréhension des différents repos qui composent le projet
        - De leur fonction, état et pistes de travail
    - Une piste déjà identifiée consiste en l'ajout du support du WiFi aux capteurs SmartSense
      - Permettrait dans un contexte en extérieur de transmettre les données
      - Et de me faire découvrir le système
    - Puis voir pour faire interagir les capteurs SmartSense avec la plateforme LivingFog
- Regarder *Kubernetes Design Principles: Understand the Why*
  - Pourquoi k8s ?
    - Souhaite déployer des conteneurs sur noeuds
    - Méthode traditionnelle consiste à se log en SSH sur la machine et exécuter la commande
    - Mais doit ensuite vérifier que tout se déroule correctement
      - Conteneur n'a pas crash
      - Noeud n'a pas crash
      - Connexion SSH a bien fonctionné
    - Besoin d'un outil de monitoring pour cela
    - Et de mécanismes de catch up pour gérer tous ces edge cases
    - Rejoint ce que m'expliquait Guillaume
      - Se retrouve avec une base de code complexe & lourde pour gérer tous les scénarios étranges
  - Approche déclarative
    - Permet en tant qu'user de ne plus se complexifier la tâche avec le "comment"
    - Se concentre juste sur le "quoi", l'état désiré
    - Et l'outil est en charge de réaliser ce "quoi", de mettre en place cet état
  - Pourquoi approche déclarative ?
    - Auto-recovery
      - Si une panne survient, c'est k8s qui est en charge de détecter la panne et de re-converger vers l'état désiré
      - Sans que l'user soit concerné/impliqué dans le "comment"
  - Comment déployer les containers ?
    - Approche naïve est que le Control Plane, à partir de la description de l'état désiré
      - Choisisse un noeud adapté
      - Commande à ce noeud de démarrer le container
    - Reproduirait le pattern qu'on aurait avec l'approche impérative
      - Control Plane devrait alors monitorer et incorporer des mécanismes de catch up en cas de défaillance
  - Pour éviter cela, ré-utilise une approche déclarative en interne
    - Control plane définit l'état désiré de chaque noeud
    - Chaque composant (les noeuds, le scheduler...) va alors oeuvrer pour converger vers l'état indiqué
    - Approche nommée Level Triggered (vs. Event Triggered)
      - Event Triggered : approche event-based
        - Les composants réagissent aux events propagés pour déterminer leurs actions
        - Si un composant a eu une défaillance et a manqué un event, doit mettre en place un mécanisme pour lui re-propager cet event
      - Level Triggered : approche par niveaux
        - Events font progresser de niveau
        - Niveau mis à disposition des composants
        - C'est à partir de son niveau courant et du niveau désiré qu'un composant détermine ses actions
    - Permet de concevoir un système plus simple et robuste
    - Clame cependant qu'aucun composant n'est un SPOF dans ce système
      - Quid du Control Plane ?
      - C'est lui qui conserve l'état désiré du système
      - Et qui reçoit/gère les demandes de MàJ de l'état
        - e.g. scheduler a décidé du noeud qui allait être responsable d'un pod donné
      - Comment il ne peut pas être un SPOF ?
    - Justifie cela de la manière suivante
      - Si le Control Plane rencontre une panne
      - Les différents composants du système continueront à tourner à partir des dernières informations obtenues sur l'état désiré
      - Si un autre composant a une panne
      - Le reste du système continue de fonctionner de manière indépendante
    - Curieux de la charge de travail du Control Plane et du Scheduler
      - Et de l'impact d'une panne du Scheduler
    - Cette approche permet aussi de faciliter l'ajout d'add-ons/l'implémentation de composants customs
      - Doit juste interagir avec le Control Plane pour mettre à jour le niveau comme souhaité
  - Comment fournir les secrets et autres données de config à l'application ?
    - L'API k8s fournit plusieurs objets pour représenter ces données
    - L'API étant transparente, peut modifier son application pour fetch ces données
    - Mais quid des applications legacy qui récupèrent ces données via un fichier ou des variables d'env depuis des temps immémoriaux ?
    - k8s permet de fournir ces données aux pods sous la forme de fichiers ou de variable d'env
  - Comment sont gérés les volumes distants ?
    - i.e. volumes fournis par des services cloud
    - Renseigné directement dans la définition du pod
    - Une fois que le pod schedulé pour un node, le storage controller vérifie si le volume indiqué est attaché au node
      - Effectue les démarches nécessaires si besoin
    - Et MàJ l'état du node
    - Mais c'est une erreur de référencer le type de stockage directement dans la config du pod
      - Pod plus portable, vendor-locked
    - Ont mis en place des abstractions pour répondre à ce problème
      - PersistentVolume et PersistentVolumeClaim
      - Référence une claim dans la config d'un pod
      - Une Claim est un objet k8s aussi
        - Décrit les caractéristiques du volume demandé
        - e.g. accès read-only/rw, type de stockage
      - Et un Controller, le Persistent Volume Controller se charge d'allouer un volume correspond aux besoins par rapport aux services disponibles
  - Pourquoi rendre l'application portable ?
    - Permet de découpler le dev de l'application du cluster/service cloud sur lequel elle va tourner
    - Fait la comparaison suivante : k8s, c'est comme un OS pour les applications distribués
      - Permet de ne plus se soucier lors du dev d'une appli distribué de l'environnement dans lequel cette dernière va tourner
- Adapter la configuration réseau pour clusters multi-nodes
  - Pas particulièrement réussi à trouver des ressources sur le sujet
  - J'ai redémarré minikube cette fois-ci avec 2 nodes d'entrée de jeu
    - minikube start --nodes 2
  - Le log au démarrage ne m'a pas indiqué le moindre warning
  - On va considérer que c'est bon du coup
    - Jusqu'à preuve du contraire
- Utiliser un driver pour Volume adapté aux clusters multi-nodes
  - L'issue indiquée ne propose pas d'autres solutions/d'alternatives à celle présentée
    - Issue : https://github.com/kubernetes/minikube/issues/12360
  - Et semble assez récente
    - Correctif courant février 2023
    - Des users qui confirment la correction du problème courant août 2023
  - J'ai donc suivi les étapes indiquées
  - Le setup de la classe de storage semble s'être effectué correctement
- Réunion SmartSense
  - Réunion ayant pour objectifs principaux de
    - Me présenter la plateforme SmartSense
    - Me présenter les problématiques/pistes de travail que Mickaël & Olivier souhaiteraient qu'on explore au cours de l'ADT
  - Préparation
    - Olivier & Eric m'ont déjà présenté les capteurs SmartSense
      - Le fait qu'ils sont équipés d'une Raspberry Pi 3 pour avoir un peu d'intelligence/puissance de calcul en local
    - M'ont aussi parlé de Guillermo Andrade-Barroso
      - Attendais un peu explorer les repos de mon côté pour le contacter
      - Et d'avoir eu cette réunion
    - M'ont parlé de 2 applications principalement
      - https://co2.irisa.fr/
      - https://smartsense-gest.inria.fr/dashboard
    - CO2
      - Permet de suivre les relevés de données par les capteurs dans une salle à partir d'un instant T
      - Ne voyant pas d'évolution, et les données datant de l'an dernier, pensais qu'il était planté
      - Mais j'ai eu la chance de tomber sur une réunion lors d'un test
      - Et pu voir son fonctionnement
    - Gest
      - Dashboard du système
      - Avait l'air de rencontrer des problèmes de certificats quand Eric a souhaité me le présenter
      - J'ai un compte, mais sans droits d'accès
    - Curieux de mieux comprendre la galaxie de repos du groupe GitLab
      - Quels sont les principaux projets ?
      - Quel est leur rôle respectif ?
      - Est-ce que certains ne sont plus d'actualité ?
      - Y a-t-il un document récapitulant l'architecture globale du système ?
  - Notes
    - Actuellement, raspberry peu utilisée
      - Sert juste à passer les données au réseau
    - Idée serait d'utiliser cette carte pour ajouter des traitements
      - E.g. préparer les données pour permettre la désaggrégation des données
        - Histoire de suivre la consommation énergétique de chaque équipement
      - Détecter la présence de personne
        - Peut utiliser la vidéo
        - Mais aussi le CO2
          - Semblerait qu'il est possible d'estimer le nombre de personnes présentes dans une pièce en fonction de la croissance du taux de CO2
    - Objectif
      - Mettre des traitements à chaque tier de l'architecture
      - Tout au long de la vie de la donnée
        - De la collecte au cloud
    - Axe de travail SmartOps
      - Mettre en place la communication sans-fil
      - Pour permettre interaction avec Living Fog
    - Bug d'InfluxDB sur version en extérieur de SmartSense
      - Fait tourner sur la raspberry une instance InfluxDB
        - Puisque pas de connexion pour remonter les données
      - Mais rencontraient des problèmes de stabilité de l'instance
        - Tâches trop couteuses ?
        - Serait intéressant de creuser et d'identifier l'origine du problème
    - Dernière étape
      - Utiliser du hardware spécialisé, un Digital Software Processor, pour faire un pré-traitement sur les flux (audio/vidéo)
        - Flux trop important/trop coûteux à traiter par les microprocesseurs équipés
      - Actuellement, déjà un DSP d'équipé sur les capteurs SmartSense
      - Un étudiant travaille actuellement sur un projet de cette nature
        - But est de router les micros sur le DSP pour traiter leurs entrées
        - Dans le but de faire par ex de la spatialisation de sources sonores
    - CO2
      - Application réalisée dans le cadre d'un stage
        - Pas vraiment testée/validée
      - Mais bon point d'entrée pour comprendre comment on interagit avec le système pour récupérer les données et effectuer des traitements
    - Gest
      - Possibilité de récupérer les données via un export de la BDD
      - Sinon possibilité de se connecter directement au broker pour récupérer les données en temps réel
        - Mickaël a un script python qui fait ça
  - Prochaines étapes
    - Continuer à me former sur la partie Fog
    - Et découvrir SmartSense
      - Consulter les documents d'architectures dans 3Douest/Documents
      - Consulter le script python permettant en local de consulter les données remontées par SmartSense
      - Consulter le projet CO2 pour creuser plus loin
    - En parallèle, Mickaël voit comment setup l'environnement de dev pour SmartSense
      - Et m'apportera le matériel nécessaire
    - Une fois l'environnement mis en place, première étape sera probablement de mettre en place une communication WiFi
      - Puis d'ajouter des traitements en local sur le capteur
- Déployer une application complexe avec k8s
  - Plusieurs points à creuser au préalable
    - Gestion des volumes
      - On ne créé pas directement les volumes
      - Les abstractions Persistent Volume et Persistent Volume Claim sont là pour permettre de découpler les volumes des cloud providers
      - Indique via une Claim les caractéristiques du volume que l'on souhaite obtenir/mis à disposition de notre application
      - k8s se charge d'allouer un volume fittant ces critères
      - Et on indique dans la specification d'un pod le ou les volumes qui doivent être montés
      - Comment ça se passe si on re-déploie l'application ?
        - Comment garantir que le même volume soit alloué à la même claim ?
          - Est-ce que k8s gère ça de son côté ?
        - On associe un nom de volume à une claim dans la specification du deployment
    - Gestion des services
      - Lors de la création d'un service, plusieurs données sont récupérées
        - L'adresse IP du service
        - Le port sur lequel il accepte les connexions
      - Ses données sont accessibles aux pods par le biais de variables d'env
        - <NAME>_SERVICE_HOST/PORT
      - Mais ces variables d'env ne seront set que pour les pods créés après le service
      - Recommandé donc de créer les services avant les deployments correspondants
      - Comment on fait ce mapping service/deployment dans le fichier de config ?
        - Via les labels ?
        - Le nom plus probablement
      - Utiliser le nom du service ou les variables d'env définies du coup ?
    - Configuration et secrets
      - Possible de définir un fichier de configuration où centraliser les informations
      - e.g. mot de passe de la BDD
      - Quelle est la bonne pratique vis-à-vis de ces fichiers ?
        - Si une donnée est utilisée plusieurs fois dans le/les fichiers de description, la déplacer dans le fichier de configuration ?
    - Gestion des labels et selectors
      - Les fichiers de config que je rencontre renseignent régulièrement des métadonnées pour chaque objet k8s
        - labels
        - selectors
          - matchLabels
          - app
          - tier
      - Quelle est la liste de ces métadonnées ?
      - Quelle est leur rôle respectif ?
    - Organisation du fichier de description
      - Possible de faire un fichier de description par entité k8s
        - service, deployment, etc
      - Un peu lourd et peu pratique
      - Possible de regrouper plusieurs descriptions dans un même fichier
        - En séparant les descriptions respectives par des ---
    - Possible de lier les fichiers entre eux ?
      - Avoir un fichier index en quelque sorte
      - Ou ce n'est pas la bonne pratique ?
  - J'ai suivi le tuto suivant *Example: Deploying WordPress and MySQL with Persistent Volumes*
    - Dispo ici : https://kubernetes.io/docs/tutorials/stateful-application/mysql-wordpress-persistent-volume/
    - Définit un Secret pour indiquer le mot de passe de la BDD
    - Définit des fichiers de description distincts pour
      - L'instance de MySQL et les composants associés
        - PVC, Service, Deployment
      - L'instance de WordPress et les composants associés
        - PVC, Service, Deployment
    - Regroupe la description du système par le biais du fichier Kustomization
    - Déploie le système via la commande suivante
      - kubetcl apply -k ./
      - Warning : option -k et non pas -f vu qu'on passe par un objet Kustomization
    - Fonctionne nickel
      - A pu administrer et modifier le site par défaut proposé par WordPress
        - Création d'une page
        - Ajout d'une image uploadée
  - J'ai voulu répliquer ensuite l'instance de WordPress
    - kubetcl scale replicas=3 deployment wordpress
  - Obtient un pod correspondant sur chaque noeud
  - J'ai alors rencontré les problèmes évoqués par Guillaume
    - Déconnexions intempestives
      - Si on est dirigé vers une instance autre que celle qui a issue notre cookie d'authentification
      - Ce dernier est invalidé
      - Besoin de se reconnecter
      - Mais ne dure que si on continue d'interagir par chance avec la même instance
    - Contenu indisponible
      - Les pages sont bien partagées entre instances
      - Puisqu'elles doivent être décrites en BDD
        - Qui elle est commune à l'ensemble des instances
      - Par contre, l'image uploadée est elle indisponible régulièrement
      - Doit être conservée que par une des instances de WordPress
      - Le volume n'est donc pas partagé par l'ensemble des noeuds
  - Comment les corriger ?
** Semaine du <2024-01-03 Wed> au <2024-01-05 Fri>
*** Planned
**** DONE Installer logiciels
CLOSED: [2024-01-03 Wed 14:39]
- Emacs, VSCode, Git, Docker
**** DONE Configurer Org-mode
CLOSED: [2024-01-03 Wed 14:39]
**** DONE Résoudre problème ethernet
CLOSED: [2024-01-04 Thu 14:09]
- Semblerait que la connexion ethernet échoue à mon bureau
- Trouver et corriger du problème
**** DONE Consulter résultats questionnaire de Ammar
CLOSED: [2024-01-04 Thu 16:37]
- Ammar a produit et envoyé un questionnaire aux gestionnaires d'observatoires d'environnements naturels
  - Afin de comprendre l'existant, leurs usages et besoins
- Disponible ici : https://survey-results.kazem.fr/protected-routes/survey_stats
- Consulter cette ressource pour en apprendre plus sur l'existant et les problèmes rencontrés par les gestionnaires d'observatoires
  - Permettrait ensuite d'en discuter avec Ammar
**** DONE Apprendre raccourcis clavier de Fedora
CLOSED: [2024-01-05 Fri 07:57]
- Ouvrir terminal
- Gérer bureaux virtuels
  - Se déplacer entre bureaux
  - Déplacer applications entre bureaux
- Augmenter/Diminuer volume
- Mettre en veille
- Prendre en screenshot une zone de l'écran
**** IN-PROGRESS Suivre cours de Guillaume sur les technologies cloud
- Disponible ici : https://gitlab.inria.fr/pierre/sct-m1info
**** IN-PROGRESS Régulariser situation du 02/01
**** TODO Trouver des ressources sur Docker & Kubernetes
- Au-delà du cours de Guillaume, existe des ressources pour rentrer plus en détails sur ces outils (talks, livres)
- Voir pour en trouver et les consulter
*** Done
- Installer logiciels
  - Emacs & Git étaient déjà installé
  - A ajouté le repo officiel pour Docker
  - VSCode, c'était un fichier à installer
- Configurer Org-mode
  - Pour org-mode, je suis retourner lire la page de Martin sur la méthodo :
    - https://people.irisa.fr/Martin.Quinson/Research/Students/Methodo/
  - Il y parle de spacemacs, une configuration préfaite d'emacs
    - https://www.spacemacs.org/
  - Je l'ai installé et fait son tuto
  - Un peu pertubante initialement puisque cette config combine les commandes de vim & celles d'emacs
  - À voir ce que cela donne à l'usage
- Réunion avec Guillaume <2024-01-03 Wed> à 15h00
  - Questions
    - Par où commencer ?
      - Documents à lire ?
      - Code ?
    - Comment communiquer ?
      - Mattermost ?
  - Notes
    - Olivier s'intéresse aux capteurs Smartsense
    - Travaille avec Guillaume sur le projet Terra Forma
    - Projet coordonné par membre du département de géo-sciences de l'univ de Rennes
    - Majorité des membres du projet sont non-informaticiens, étudient les sciences de l'environnement
    - Intéressés par des observatoires de l'environnement naturels
    - Délimitent des territoires intéressants et les équipent de capteurs intelligents
    - Solution de base nécessite de récupérer les données sur le terrain après temps de collecte
      - Mais sujets de recherche peuvent nécessiter de traiter les données régulièrement
      - Mais territoires pas forcément accessibles
      - Mettent donc des stations de calculs au sein des environnements
    - Mais stations de calculs existantes répondent pas au besoin
      - Généralement propriétaires
      - Ne permettent que l'archivage des données et la transmission à un cloud
      - Souhaiteraient mettre en place leurs propres applications
        - Déclencher des actions (mettre en route capteurs, changer fréquence d'échantillonnage...) suite à un évenement en temps réel
        - Faire tourner des modèles de l'environnement et les comparer aux données réelles pour les valider/invalider
          - Et potentiellement évaluer l'état de l'environnement si on joue sur un de ces paramètres
      - Mais les solutions ne le permettent pas
    - Utilisation de plateformes de calcul en milieu naturel isolés posent des questions
      - Où trouver l'énergie pour les alimenter ?
        - Solaire probablement, mais s'agit d'une ressource intermittente (jour/nuit, été/hiver)
      - L'énergie étant limitée, comment adapter les traitements en fonction de la quantité à disposition (allumer/éteindre capteurs) ?
      - Comment relancer la plateforme si à court de jus momentanément ?
    - Ammar travaille sur ces problématiques
      - A rencontré et fait un questionnaire à l'attention des gestionnaires d'observatoires
        - Sur l'existant, leurs besoins, leurs attentes
      - Aurait récupérer et mis en forme les résultats de ce questionnaire
      - Voir avec lui à ce sujet
    - En ce qui me concerne, but du projet est de prendre en main la plateforme LivingFog
      - Plateforme développée par plusieurs doctorant-es
        - Probablement pas parfait d'un point de vue technique
        - Mais de la doc existe (livrables pour projet européen, doc technique)
      - A été déployée à Valence dans le cadre d'un hackathon
        - Consistait à proposer des applis de smart city (application de suivi de l'ensablement du port, application de détection de la fréquentation des différentes activités proposées)
        - Résultats très satisfaisants semblerait
      - But est d'évaluer cette plateforme pour notre nouvel usage
        - De déterminer ce qui nous intéresse et non
        - De virer ce qui nous est inutile
        - De consolider ce qui existe et intéressant pour nous
        - Et de l'adapter à notre usage
    - LivingFog repose sur la techno LoRaWAN pour la communication
      - Pratique pour échanger à longue distance en utilisant peu d'énergie
      - Mais faible bande-passante
      - Et qui pose des contraintes supplémentaires
        - Capteurs envoient les données à des gateways qui relaient les messages
        - Mais pas d'association entre capteurs et gateways
          - Les messages sont donc dupliqués
        - La déduplication des messages est effectuée de manière centralisée
      - Des gens de Terra Forma se penchent dessus, nous, on ne va pas se concentrer dessus
    - On va plutôt se pencher sur la partie cluster
      - Utilise des clusters de raspberry
      - Fait tourner kubernetes dessus pour gérer un ensemble d'applications sur un cluster
        - Existe des versions allégées de kubernetes k3s pour cluster de raspberry
    - Première étape est donc de monter en compétence sur les technos correspondantes
    - Guillaume a un cours sur les technos Cloud
      - Va m'y donner accès pour que je le suive et que je monte en compétence là-dessus
    - Creuser plus particulièrement Docker & Kubernetes
- Résoudre problème ethernet
  - Guillaume m'a explique que les prises Ethernet ne sont pas toutes rattachées au même réseau
  - Peut être nécessaire de changer la prise sur laquelle je suis branché
  - Cela n'a rien changé
  - Après discussion avec les membres de la DSI, m'ont dit d'ouvrir un ticket pour qu'ils affectent en dur l'adresse mac du dock à ma machine
  - Ça a résolu mon problème de connexion
- Suivre cours de Guillaume sur les technologies cloud
  - CM1 - Introduction au Cloud
    - Pour offrir un service plutôt qu'un produit, nécessité d'une infrastructure
    - Cloud offre plusieurs bénéfices aux users
      - Comparé à un système traditionnel, permet de déléguer la gestion de l'environnement au provider
      - Permet d'utiliser uniquement les ressources dont l'on a besoin à un instant T
        - Et non pas perpetuellement les ressources dont l'on a besoin pour tenir la charge lors des pics d'activité
      - Permet donc de scale de manière flexible en fonction des besoin
    - Différences entre IaaS, PaaS et SaaS
      - [[file:img/iaas-paas-saas.png]]
      - IaaS
        - Provider ne fournit que les machines virtuelles
        - C'est aux users de setup leurs machines à partir de l'install de l'OS
      - PaaS
        - Ici la machine est déjà installée
        - Il ne reste plus qu'à installer son ou ses applications
      - SaaS
        - Ici, aucune installation nécessaire
        - On souscrit directement une instance de l'application désirée
    - Mentionne que certaines entreprises créent leur propre cloud privé
      - Détaillé par : https://www.datamation.com/cloud/private-vs-public-cloud/
    - Cloud public
      - Cloud tel que je l'imagine et connais
      - Géré par un provider
      - Les entreprises ont recours à ses services et se "contentent" de l'utiliser
    - Cloud privé sur site
      - L'entreprise recrée un cloud chez elle
        - Data-center, machines, gestion
      - Pour cela, peut reposer sur des outils mis à disposition par les cloud providers ou des projets OS (OpenStack)
      - Offre la confidentialité et souveraineté des données
      - Mais en échange, introduit
        - Une charge de travail (setup et manage le cloud)
        - Des coûts à priori (data center, machines)
        - Une limitation de la scalability (doit acheter des machines supplémentaires lorsque atteint la charge limite)
    - Cloud privé hébergé
      - Possible aussi de demander à un provider de s'occuper de notre cloud privé
      - Caractéristiques similaires à un cloud public
        - Même si nécessite plus de préparations et de coûts en amonts qu'une offre publique
      - Mais permet de reposer sur des machines dédiées à notre usage, offrant ainsi sécurité et confidentialité
    - Majorité des entreprises ont un usage hybride entre cloud public et privé
      - [[file:img/usage-cloud.png]]
    - Et rien n'empêche d'utiliser plusieurs clouds d'un même type
      - Pour silo-er les apps, avoir de la redondance en cas de panne d'un provider
    - Questions
      - Un peu de mal à formaliser le PaaS et ce qu'il comprend
        - Je vois ça comme une machine avec déjà son OS de setup
        - Il ne reste plus qu'à installer son application
        - Mais le cours mentionne la couche middleware
        - Qu'est-ce qu'elle couvre et peut offrir comme services ?
          - Mention de DBs et frameworks HPC
  - CM2 - Virtualisation
    - Définition
      - Un logiciel qui imite un appareil physique
      - Fournit au moins les mêmes fonctionnalités
      - Utilise une interface identique
    - Avantages
      - Peut être créé et supprimé à la volée
      - Peut être facilement modifié/configuré
      - Peut proposer des fonctionnalités supplémentaires à la version physique
    - Exemples
      - Clavier virtuel
      - Disque virtuel
      - Système de stockage virtuel (NAS, SAN)
        - Un peu de mal à piger la différence entre ces technos
      - Réseau virtuel (VLAN, SDN)
      - Machine virtuelle
      - Conteneur
    - Remarques
      - Slide 8, opération /take snapshot/ : c'est pas /head = new_snapshot/ plutôt ?
        - Ou /head = empty/ plutôt ?
  - CM3 - Infrastructures cloud VM-based
    - Porte principalement sur la description de l'architecture système d'un cloud
    - Prend pour cela comme exemple OpenStack
      - Se base sur la présentation qui en est faite lors de la *Cloud Architect Alliance #15*
      - Disponible ici : https://www.slideshare.net/alessandrovozza/cloud-architect-alliance-15-openstack
    - Globalement, une multitude de différents services
      - [[file:img/2015-open-stack-architecture.png]]
    - Chacun ayant son rôle et ses responsabilités
      - E.g. Keystone
        - Service d'authentification et d'autorisation
        - Fournit aussi la liste des autres services
    - Composants autonomes, pouvant être indépendamment répliqués pour répondre aux besoins (charge, disponibilité...)
  - CM4 - Services cloud de stockage
    - Présente les différents types de service de stockage offerts par les cloud providers
    - Object storage
      - Niveau de granularité est le fichier
      - Permet de créer,lire et supprimer des fichiers
      - Mais pas de les modifier
        - Les fichiers sont donc immuables
      - E.g. Amazon S3 (Simple Storage Service)
    - Block storage
      - Niveau de granularité est le volume, i.e. des partitions disques virtuelles
      - Permet de créer, modifier les caractéristiques (taille, type de stockage), et d'attacher des volumes aux VMs
      - E.g. Amazon EBS (Elastic Block Store)
      - Propose généralement services supplémentaires
        - Snapshotting, et sauvegarde/réplication des snapshots effectuées
    - Relationnal storage
      - Indique qu'on peut démarrer et gérer son propre SGBD relationnel sur une VM
      - Mais que les cloud providers proposent directement des services de BDDs relationnelles
      - Insiste cependant sur les limites de ce type de système
        - Ne tolèrent pas les partitions réseaux généralement
        - Deviennent soit indisponible, soit incohérente de manière non-maitrisée
    - NoSQL storage
      - Présente les bases de données NoSQL
      - Précise qu'elles ont été conçues pour les besoins des applications cloud, notamment
        - Scalable, i.e. supporter un dataset de très grande taille et une charge importante
        - Elastic, i.e. faciliter l'ajout & la suppression d'instances à la volée
        - Partition tolerant
      - Détaille ensuite différents SGBDs NoSQL
        - DynamoDB (KV-Store)
        - MongoDB (Document-based)
        - Apache Cassandra (Column-based)
    - Remarques
      - Je sais pas si Guillaume mentionne la vague NewSQL dans la partie sur les services de SGBDs relationnels
        - Mais le constat est peu élogieux sous la forme actuelle
        - Est-ce que les SGBDs relationnels sont si inadaptées aux applications distribuées ?
          - Notamment, les systèmes appartenant à la vague NewSQL ne sont pas partition tolerant ?
        - Peut aussi s'intéresser à ce qui se fait du côté de ElectricSQL
- Régulariser situation du 02/01
  - J'ai essayé de déposer mon jour de congé pour le 02/01 le <2024-01-04 Thu>
  - Mais Casa m'empêche de le faire car la date est antérieure à la date du jour
  - À voir au retour de l'assistante d'équipe
- Consulter résultats questionnaire de Ammar
  - Résultats obtenus via 59 réponses (17 complètes, 42 incomplètes), couvrant 25 observatoires
  - Systèmes existants
    - Sources d'énergie
      - Principalement du solaire et de la batterie
      - Comment les observatoires gèrent-ils les limites de ces sources (nuit, batterie vide...) ?
      - Mentionne une source "Autres", des exemples ?
    - Techniques de communication
      - Principalement via mémoire interne (?) et 4G
        - Qu'est-ce qu'on entend par mémoire interne ?
        - C'est pas trop coûteux la 4G ?
    - Équipement info sur site
      - 47% déclarent que les observatoires incluent de l'équipement info en plus des capteurs
      - On a une idée du type d'équipement ?
    - Nombre de capteurs par site
      - La majorité des sites est regroupée (gausienne) dans les tranches 10-20, 20-50 et 50-100
    - Utilisation des données par des /operational players/
      - C'est quoi ?
    - Détection d'évènements automatique
      - Peu de détection automatique (17,65%), et encore moins de réponse automatique (29,41%)
      - À quoi sert la détection sans réponse ?
      - Exemples de réponse ?
    - Pré-processing sur site
      - Peu répandu (29,41%)
      - À quoi sert ce pré-processing ?
      - Les données invalides détectées lors de ce pré-processing sont généralement supprimées (81,82%)
      - Choix ou contrainte ?
    - Détection automatique de capteurs défectueux
      - Seulement 5,88% de sites avec cette fonctionnalité
      - Que font-ils dans ce cas ? Coupure du capteur incriminé ?
  - Systèmes futurs
    - Ammar a abordé dans une seconde du questionnaire la question des systèmes futurs et des fonctionnalités désirées
    - Intégration de données extérieures
      - En plus des données collectées, une partie considérable des réponses montre un intérêt pour intégrer dans les données d'un site des données extérieures
      - Plusieurs provenances suggérées
        - Autres sites/observatoires (52,94%)
        - Services tiers, e.g. Météo France (76,74%)
        - Données manuelles (47,06%)
    - Monitoring de la santé du système
      - Les réponses au questionnaire montre un intérêt/besoin à ce sujet
      - Pistes indiquées
        - Intégrer des données d'autres sources pour aider la détection
          - Quelles données ?
          - Comment cela fonctionne ?
        - Notification
  - Questions globales
    - Les figures présentent-elles les résultats des observatoires des 17 réponses complètes uniquement ?
    - Y a-t-il différents niveaux de réponses aux questions "intéressé-e/non-intéressé-e" ?
      - Y a une différence entre vouloir "pourquoi pas" une fonctionnalité et avoir besoin d'une fonctionnalité
      - Cela peut avoir un impact sur les contraintes du système quand la fonctionnalité considérée est "intégrer des données de services tiers" ou "données en temps réel"
    - Quel est le but de ces observatoires ?
      - Est-ce qu'ils sont là pour simplement observer ?
      - Ou certains ont vocation à agir sur l'environnement observé en cas d'évènement (sécheresse, inondation...) ?
    - Y avait-il des questions libres à ce questionnaire ?
      - Ammar pose des questions sur des aspects précis des observatoires et définit des pistes d'améliorations de par son formulaire
      - Mais est-ce que d'autres aspects sont importants pour les gestionnaires d'observatoires et n'étaient pas abordés dans le questionnaire ?
  - Remarques
    - Choix des couleurs
      - L'utilisation du vert pour indiqué "non" me paraît non-intuitif
        - Une couleur proche du vert pour indiquer un résultat positif et une autre proche du rouge pour indiquer un résultat négatif me semble plus commun
    - Graphique sur l'âge des données collectées
      - L'unité me paraît pas super adaptée
  - Discussion
    - Source d'énergie
      - Batterie interne = piles
      - Mais nécessite tournée régulière pour la maintenance
        - Nécessaire de toute façon pour récupérer les données
    - Équipement IT
      - Ammar n'est pas convaincu qu'il y ait tant d'observatoires sans data logger
        - À creuser avec les gestionnaires d'observatoires
    - Operational Players
      - Organisations tierces, généralement services publiques, qui pourraient utiliser les données collectées par les observatoires pour leur tâche, e.g. alerter la population sur un risque d'inondation, de sécheresse
      - Mais l'utilisation des données des observatoires par les acteurs opérationnels n'est pas le but de tous les observatoires
    - Réponse automatique à un event
      - C'est 30% global, pas juste en réponse à la détection automatique
      - Y a des events manuels, e.g. le passage en hiver
    - Hétérogénéité des observatoires
      - L'ensemble des observatoires montre une grande hétérogénéité de leurs buts, contraintes et besoins
        - Certains ont accès à la 4G, au réseau électrique
      - Tous ne nous intéressent pas dans le cadre de ce projet
      - But du questionnaire est d'identifier les observatoires auxquels nous pouvons apporter quelque chose
* Autres
** Commandes utiles
- Mettre à jour paquets
  - dnf check-update
  - sudo dnf upgrade
- Kubernetes
  - Pod
    - Créer un pod : kubectl create -f pod.yaml
    - Lister les pods existants : kubectl get pods
    - Inspecter un pod : kubectl describe pod mysmallpod
- Noeud SmartSense
  - Lister l'espace disque disponible : df -h
  - Récupérer la liste des services en route : systemctl --type=service --state=running
  - Se connecter à la DB : influx -username <username> -password <password> -database <database>
  - Suivre les logs de l'application : tail -f /var/log/syslog
    - Ou : journalctl -f -u mainApp/influxdb
  - Récupérer la conf d'un fichier, sans les commentaires : grep '^[[:blank:]]*[^[:blank:]#;]' <file>
  - Arrêter le noeud : sudo shutdown -h now / sudo halt
- Influx
  - Lister les bases de données : SHOW DATABASES
  - Utiliser une base de donneés : USE <database>
  - Lister les measurements : SHOW MEASUREMENTS
  - Récupérer les entrées d'un measurement : SELECT * FROM <measurement> LIMIT <nb>
  - Quitter le shell InfluxDB : exit
  - Créer l'utilisation admin
    - CREATE USER admin WITH PASSWORD '<password>' WITH ALL PRIVILEGES
  - Créer les bases de données avec les bonnes retention policies
    - DROP DATABASE collectd; CREATE DATABASE collectd WITH SHARD DURATION 1h NAME autogen
    - CREATE DATABASE SensorData WITH SHARD DURATION 1h NAME short_duration
- Resize le swap sur Raspberry
  - sudo dphys-swapfile swapoff
  - sudo nano /etc/dphys-swapfile et modifier CONF_SWAPSIZE
    - Ou CONF_SWAPFACTOR
  - sudo dphys-swapfile swapon
  - sudo reboot
- Réquisitionner une partie de la RAM
  - Réquisitionner 500Mo : head -c 500m /dev/zero | tail
- Remplacer une chaîne dans un fichier
  - sed -i 's/<old string>/<new string>/g' <file>
** Raccourcis utiles
*** Fedora
- Se déplacer entre bureaux virtuels
  - Ctrl + Alt + Left/Right
- Déplacer application courante entre bureaux virtuels
  - Ctrl + Alt + Shift + Left/Right
- Redimensionner l'application courante
  - Super + Left/Right/Up/Down
- Déplacer application courante entre écrans
  - Super + Shift + Left/Right/Up/Down
- Prendre en screenshot une zone de l'écran
  - Print Screen
- Verrouiller l'écran
  - Super + l
*** Emacs
- Naviguer dans le fichier
  - Haut/bas : k/j
  - Gauche/droite : h/l
  - Début/fin : g-g/G
- Copier/Coller
  - Sélection : C-SPC
  - Copier (yank) : y (ou M-y pour capturer la ligne entière et le retour à la ligne précédent)
  - Coller : p (après) ou P (avant)
- Afficher image
  - Insérer lien vers image : C-c C-l
  - Toggle inline image : C-c C-x C-v
- Recherche
  - /mot RET pour déclencher la recherche
  - n pour avancer jusqu'à l'occurrence suivante
  - N pour l'occurrence précédente
  - ?mot RET pour déclencher la recherche en sens inverse
- Buffer
  - Revenir au buffer précédent : SPC TAB
*** Terminal
- Ouvrir un nouvel onglet
  - Ctrl + Shift + T
- Changer d'onglet
  - Alt + 1/2/3
- Fermer onglet
  - Ctrl + Shift + W
