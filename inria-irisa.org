#+TODO: TODO IN-PROGRESS DONE
#+ORG-IMAGE-ACTUAL-WIDTH: 500px

* Journal de bord
** Backlog
*** IN-PROGRESS Automatiser la création d'un cluster de Raspberry virtualisé
- Plutôt que de travailler avec le cluster physique
- Serait intéressant d'apprendre à provisionner un cluster virtuel de Raspberry
- Permettrait de pouvoir créer un environnement pour dev et effectuer des tests
  - Notamment comment setup k8s ou équivalent sur un tel système
*** TODO Automatiser le setup de k3s sur un cluster de Raspberry
- Permettrait de graduate de minikube
  - Et d'apprendre ainsi tout ce qu'il gère pour moi automatiquement
- Et d'aller plus loin dans notre mise en place automatique d'environnements de dev et de test
- Ressources
  - Blogpost: https://medium.com/@stevenhoang/step-by-step-guide-installing-k3s-on-a-raspberry-pi-4-cluster-8c12243800b9
*** TODO Déployer WordPress sur un cluster de Raspberry
- Permettrait de valider le résultat obtenu dans les étapes précédentes
- Et de rencontrer les problèmes éventuels liés aux architectures processeurs
*** TODO SmartSense: Étudier la mise en place d'un schéma de fichier de config
- L'initialisation des noeuds reposent sur un fichier config.ini
- Pour le moment, je le retro-ingénérie pour identifier les différentes options possibles
  - Et leurs valeurs respectives
- Serait intéressant d'avoir un schéma précisant son format
  - Qui pourrait s'accompagner d'un outil de validation de fichier de config
  - Et p-e d'Enums utilisable dans le code Python
    - Plutôt que des chaînes de caractères error-prones
*** TODO SmartSense: Étudier la migration vers une nouvelle version d'InfluxDB
- Le projet utilise pour le moment la version 1.6.4 d'InfluxDB
- On en est actuellement à la version 2.7.5
- Voir si y a un intérêt à migrer à cette nouvelle version
  - Features, performances
- Voir surtout si ça peut tourner sur la rpi qui équipe le noeud SmartSense
*** TODO Lire *OZCAR: The French Network of Critical Zone Observatories*
- Ammar m'a recommandé cet article pour en apprendre plus sur les observatoires et leurs buts
- Le consulter
** Semaine du <2024-01-29 Mon> au <2024-02-02 Fri>
*** Planned
**** DONE SmartSense: Rechercher outil pour refactorer le code du projet vis-à-vis des conventions Python
CLOSED: [2024-01-29 Mon 17:00]
- Pas mal de retours de Pylint sur le code correspondent à un non-respect des conventions Python
  - Noms de modules, de fonctions et de méthodes
- Voir si un outil permet d'automatiser ce type de correctifs
**** DONE SmartSense: Refactor *MqttInterface*
CLOSED: [2024-01-31 Wed 10:26]
- Ne plus hériter de *mqtt.Client*
  - Actuellement composant dans un état bizarre
  - Hérite des méthodes mais pas des attributs car n'appelle jamais le constructeur de *Mqtt.Client*
  - Déclenche des warnings et même erreurs si on manipule un peu trop le composant
    - e.g. ne peut pas surcharger /on_subscribe()/
- Mettre en place un Enum pour gérer le niveau de QoS des messages
  - Passe directement les valeurs 0, 1 et 2 pour le moment
  - MQTT n'offre pas du tout les mêmes garanties en fonction du niveau choisi
  - 0: At most once
  - 1: At least once
  - 2: Exactly once
  - Serait plus parlant d'utiliser un type qui précise cela
- Refactor /processDataForInfluxDBEmbedded()/
  - Décomposer en fonctions
  - Ne fait rien si /not self.subscriber/
    - Vérifier que c'est un comportement souhaité
    - Me parait error-prone perso
- Séparer l'interface avec le broker MQTT avec l'interface avec la BDD InfluxDB
  - IMO *MqttInterface* devrait servir à interagir avec le broker MQTT
  - L'interaction avec la BDD InfluxDB devrait être géré par un autre composant spécialisé
    - Qui plus est, l'interaction avec la BDD est plus limitée, i.e. write-only
  - Permettrait ainsi de rendre plus clair le code
  - E.g. composant n'interagit pas avec la BDD si le flag /mqttEnabled/ est activé
    - Même dans le cas où le flag /influxDbEmbedded/ est lui aussi activé
**** IN-PROGRESS SmartSense: Nettoyer *mainApp*
- Déplacer le code du scope global du module dans une fonction /main()/
- Factoriser le code de /startProcess()/
- Décomposer en fonctions /routeIncomingMqttMessages()/
**** IN-PROGRESS SmartSense: Intégrer les modifications effectuées à la branche *main*
- J'ai fini de mettre en place une première série de tests
- Avant d'aller plus loin et d'effectuer de nouvelles modifs
- Serait bien de me poser avec Mickaël pour reviewer mes branches et discuter de ce que j'ai fait
**** IN-PROGRESS SmartSense: Intégrer la branche *autonomous-node* à la branche *main*
- Cette branche a l'air d'apporter des workarounds pour les problèmes constatés lors de l'utilisation du noeud SmartSense en mode autonome
- Notamment, exporte et reset la DB périodiquement pour éviter le mécanisme de compression des shards
  - Qui a l'air d'être trop couteux pour tourner sur le noeud
- Améliore aussi la lisibilité du code
*** Done
- SmartSense: Intégrer les modifications effectuées à la branche *main*
  - J'ai notifié Mickaël que j'ai push mes modifications
  - Et lui ai précisé que j'aimerais une validation
- SmartSense: Intégrer la branche *autonomous-node* à la branche *main*
  - J'ai rebase ma branche *add-tests* sur la branche *autonomous-node*
  - Notamment en modifiant les commits impactés par les modifications de cette branche, i.e.
    - feat(formatter): add ruff
    - test(dataToLocalInfluxDB): edit test to verify buffering mechanism
  - Pushé cette branche sur le repo
  - Ouvert une MR décrivant les modifs globales de la branche
  - Ai demandé à Mickaël de valider cette MR
- SmartSense: Rechercher outil pour refactorer le code du projet vis-à-vis des conventions Python
  - Pas trouvé d'outils qui fait cela
  - Le plus proche que j'ai vu est : https://github.com/hhatto/autopep8
  - Mais ne fait que des transformations autour des whitespaces
    - A un mode aggressif
    - Mais ne propose d'autofix que les règles E711 et E712
    - Moi c'est la C0103 qui m'intéresse
  - Je pense que je suis bon pour faire ça "à la main"
- SmartSense: Refactor *MqttInterface*
  - [X] Séparer l'interface avec le broker MQTT avec l'interface avec la BDD InfluxDB
    - [X] Déplacement de /processDataForInfluxDBEmbedded()/ dans son propre module
      - Permet de simplifier les deux composants
        - Pas de mode "subscriber" pour *influxdb_interface*
        - Plus de mode "mqttEnabled" et "influxDbEmbedded" pour *MqttInterface*
      - Pose plusieurs questions cependant
        - Où déplacer/comment retravailler le code simulant une réponse du service de provisioning ?
          - Permettrait de se débarrasser de la *Queue* dans le sens *influxdb_interface* -> *mainApp*
          - Nécessite de comprendre le but de ce morceau de code
        - Où instancier et dans quelles conditions instancier *influxdb_interface* ?
          - Liée à cette question de provisioning
        - Nécessite aussi de savoir s'il est possible de cumuler un broker MQTT et une base de données locale InfluxDB
        - I.e. doit-on découpler le provisioning (setup?) du noeud du broker MQTT ?
    - Concernant le provisioning
      - Plusieurs messages concernent le provisioning
      - Le message sur le topic "provisioning/{self.currentSite}/node/{self.id}"
        - Permet de set les valeurs requises pour générer les topics
          - mode, location, secteur, sous-secteur
        - Et de préciser le port du broker MQTT lié au secteur
          - Y a plusieurs brokers ?
        - Dans le cas du mode autonome, utilise une valeur dummy (0) comme port pour le broker MQTT
      - Les messages sur les topics "provisioning/p/{MyApp.locationPath}/{MyApp.id}/rpiBoard/{ina}/consumption"
        - Plusieurs devices INA
          - Poe
          - Rpi
          - Sensor
          - Ext1
          - Ext2
        - Chacun va être à l'origine d'un message de provisioning sur son topic dédié
        - Permet d'obtenir la fréquence d'échantillonnage à utiliser si je pige bien
      - Les messages sur les topics "provisioning/a/{MyApp.locationPath}/{MyApp.id}/{tcl}"
        - Semble être conçu pour supporter plusieurs devices
        - En pratique, y en a qu'un seul pour le moment
          - Et donc qu'un seul suffixe pour le topic : "rpiBoard/TLC59108/leds"
        - De même, semble concerner la fréquence d'échantillonnage
    - [X] Découplage du self provisioning et de *MQTT/InfluxDBInterface*
      - Ajout d'un flag isProvisioningViaMQTT
      - Spécifie si le noeud doit se provisionner via MQTT
        - Dans ce cas envoie le 1er message
          - Sur le topic "provisioning/{self.currentSite}/node/{self.id}"
        - Et attend la réponse avant de passer à la suite
      - Sinon utilise d'emblée les valeurs dummy pour la génération de topics et le broker du secteur
      - Et intercepte les messages des modules *InaManager* et *TclManager* concernant le provisioning
        - Simule lui-même les réponses au lieu de déléguer au module *MQTT/InfluxDBInterface*
  - [X] Mettre en place un Enum pour gérer le niveau de QoS des messages
  - [X] Ne plus hériter de *mqtt.Client*
    - J'en ai profité pour refactorer le code et virer le code inutilisé
      - Par exemple /setCallbackOnMessage()/
    - Un cas particulier est celui du paramètre /protocol/ du constructeur
    - Dans la version d'origine de *MqttInterface*, s'agit à la fois
      - D'une propriété de la classe héritée de *mqtt.Client*, qui a pour valeurs possibles MQTTv31, MQTTv311 ou MQTTv5
        - Constantes fournies par *mqtt*
      - D'un flag pour déterminer la valeur d'une autre propriété, /transport/, qui a pour valeurs possibles "tcp" et "websocket"
    - Sauf que /protocol/ n'est jamais spécifié dans le moindre appel au constructeur de *MqttInterface*
      - Donc inutilisé et on repose donc sur la valeur par défaut de /transport/ : "tcp"
    - A donc supprimé le paramètre /protocol/
      - Voir si cela pose un problème
    - Rencontré quelques difficultés à faire fonctionner les tests
      - La connexion au broker se faisait correctement
      - L'envoi de messages aussi
      - Mais la réception, via *MqttInterface* ou *mqtt.Client* directement, ne se produisait pas
      - C'est tombé en marche, mais sans que je n'arrive à identifier le problème exactement
      - Trouvé, il s'agissait d'une erreur dans la callback /on_connect()/
      - Essayais d'accéder à /self.nodeId/, qui correspond à cet instant à /client.nodeId/ et non à /mqttInterface.nodeId/
        - Propriété non définie
      - Étrangement, fait planter silencieusement le client MQTT
      - De plus, utilisais dans tous mes tests une instance de *MqttInterface* en mode subscriber
        - Et donc exécutant les lignes de code problématiques
    - A pu en profiter pour rework le test "receive_msg_from_broker"
      - Surcharge callback /on_subscribe()/ pour déclencher l'envoi du message de test
      - Au lieu d'utiliser le workaround non-déterministe "j'attends un peu que l'instance *MqttInterface* se soit connectée au broker"
  - J'ai mes différentes modifs prêtes sous la forme de branches
  - Reste plus qu'à les faire valider
    - En profiter pour voir ce paramètre étrange /protocol/ et son impact sur /transport/
- SmartSense: Refactor *mainApp*
  - [X] Déplacer le code du scope global du module dans une fonction /main()/
  - [X] Utilisation de f-strings
  - [X] Rendre l'instanciation des *MqttInterface* dépendantes de la config
  - [X] Ajout d'une vérification de la config
    - Stop l'application si les sorties MQTT et InfluxDB sont toutes deux désactivées
    - Quel intérêt dans ce cas là ?
  - [X] Ajout d'un Enum pour représenter les différents noms de process
    - Permet l'utilisation de strings codées en dur
    - Et les erreurs que ça peut engendrait
  - [X] Utilisation de match/case au lieu de if/elif
- SmartSense: Réflexions
  - Est-ce vraiment InfluxDB qu'on veut utiliser dans le cas du mode autonome ?
    - À voir les traitements que l'on va effectuer
    - Et les données qu'ils requièrent
    - Mais plusieurs questions se posent
    - Est-ce que la dimension temporelle va être très importante ?
    - Est-ce qu'on ne peut pas reproduire cette dimension temporelle avec une fenêtre glissante des entrées ?
    - Grosso modo, va-t-on à un moment query la BDD locale ?
    - Pour quelles raisons le choix s'est porté sur InfluxDB pour le SGBD embarqué lors de la conception du mode autonome ?
      - Si c'est juste une question de compatibilité/d'export avec la BDD qui tourne sur le serveur, est-ce qu'un fichier ne suffit pas ?
** Semaine du <2024-01-22 Mon> au <2024-01-26 Fri>
*** Planned
**** DONE S'approprier SmartSense/smartsense-node-app
CLOSED: [2024-01-29 Mon 10:33]
- On est en train de revoir les priorités avec l'arrêt maladie de Guillaume
- Serait préférable que je passe sur SmartSense en attendant son retour
- Les différentes tâches identifiées concernent principalement le capteur SmartSense
  - Ajout d'une couche réseau WiFi
  - Ajout de traitements en local des données collectées
    - i.e. sur le capteur
  - Correction des bugs rencontrés lors de l'utilisation de la BDD en local
    - Dans la version du capteur pour un fonctionnement autonome
    - Le capteur fait tourner une instance de la BD, InfluxDB, en local
    - Mais lors de leurs expériences, ont constaté des crashs de la BDD
      - Parfois au bout de plusieurs heures
- Il serait donc intéressant que je rentre rapidement dans ce projet
**** DONE Mettre en place des tests pour MQTTInterface
CLOSED: [2024-01-29 Mon 10:33]
- Pistes de tests possibles
  - Se connecte bien au broker
    - Variante not subscriber
  - Si je fournis un message valide à l'instance MQTTInterface, il est bien publié sur le broker
    - Variante avec message invalide
  - Si je fournis un message valide au broker, il est bien récupéré par l'instance MQTTInterface
    - Variante avec message invalide
*** Done
- S'approprier SmartSense/smartsense-node-app
  - Pistes de travail
    - Identifier les dépendances
    - Mettre en place un gestionnaire de dépendances
      - Poetry est un bon candidat
    - Mettre en place un linter
      - pylint : https://www.pylint.org/
      - Ruff : https://astral.sh/ruff
    - Setup un task runner
      - poethepoet : https://github.com/nat-n/poethepoet
    - Automatiser le déclenchement des outils
      - Hook pré-commit
      - Config VSCode
    - Mettre en place un autoformatter
      - Ruff : https://astral.sh/ruff
      - Black : https://github.com/psf/black
      - isort: https://github.com/PyCQA/isort
    - Mettre en place un outil d'analyse des types
      - mypy : https://www.mypy-lang.org/
      - pyright : https://github.com/microsoft/pyright
  - Getting started
    - pipx install poetry
    - pipx install poethepoet
    - poetry add --group dev pylint
    - poetry add --group dev ruff
    - poetry add --group dev isort
  - Dépendances identifiées
    - influxdb
      - https://pypi.org/project/influxdb/
    - intelhex
      - https://pypi.org/project/intelhex/
    - ntplib
      - https://pypi.org/project/ntplib/
    - paho-mqqt
      - https://pypi.org/project/paho-mqtt/
    - pi-ina219
      - https://pypi.org/project/pi-ina219/
    - pyftdi
      - https://pypi.org/project/pyftdi/
    - pyserial
      - https://pyserial.readthedocs.io/en/latest/pyserial.html
    - RPi.GPIO
      - https://pypi.org/project/RPi.GPIO/
    - smbus2
      - https://pypi.org/project/smbus2/
    - spiflash
      - https://pypi.org/project/pyspiflash/
      - A pour dépendance pyftdi et pyserial
  - Rencontre une erreur lors de l'installation de
    - pi-ina219
    - RPi.GPIO
  - Une brève recherche m'informe que c'est parce qu'il me manquait une dépendance système
    - sudo dnf install python3-devel
  - J'ai pu installer les dépendances restantes
  - Obtient les erreurs suivantes
    - Module RPi.GPIO has no setmode/BCM/setup/.../OUT member
    - No name 'SMBusWrapper' in module 'smbus2'
  - SMBusWrapper
    - Le changelog de la librairie indique la suppression de la classe SMBusWrapper au profit de SMBus
    - Voir https://github.com/kplindegaard/smbus2/blob/master/CHANGELOG.md#040---2020-12-05
    - Remplacer toutes les occurrences de SMBusWrapper par SMBus corrige l'erreur relevée par pylint
  - RPi.GPIO
    - A tenté de lancer l'interpréteur python pour jouer avec ce module et voir ce qui pouvait poser problème
      - poetry shell
      - python3
      - import RPi.GPIO as GPIO
    - Mais rencontre l'erreur suivante
      - RuntimeError: This module can only be run on a Raspberry Pi!
    - Ce qui peut p-e expliquer l'erreur rencontrée
      - Le linter rencontre p-e des difficultés à gérer le module, si ne peut pas importer ce dernier
  - Comment résoudre ça ?
    - Dev sur raspberry/VM ?
  - J'ai tenté d'émuler la raspberry et de déployer l'environnement de dev dessus, histoire de tenter le coup
    - Setup la raspberry et les deps de dev
      - sudo apt update; sudo apt full-upgrade
      - sudo apt install python3-pip
      - sudo apt install python3-venv
      - python3 -m pip install --user pipx
      - python3 -m pipx ensurepath
      - source ~/.bashrc
      - pipx install poetry
      - pipx install poethepoet
      - poetry install --no-root --with dev
    - Transférer le projet sur la raspberry emulée
      - scp -P 2222 -r . pi@localhost:/home/pi/smartsense-node
  - Mais lors de l'install de poetry, j'ai rencontré un problème de stockage
    - La raspberry émulée ne dispose que de 1.7Go de stockage, et il ne m'en restait que 20Mo à ce stade
  - Comment allouer plus d'espace à la machine ?
    - Création d'une image à partir de l'image raspbian
      - qemu-img dd -f raw -O qcow2 if=2023-12-05-raspios-bullseye-arm64-lite.img of=rpi-bullseye.qcow2
      - qemu-img resize rpi-bullseye.qcow2 8G
    - Resize de la partition / une fois le système démarré
      - Suivi le tuto : https://raspberrypi.stackexchange.com/questions/499/how-can-i-resize-my-root-partition
      - Commandes
        - sudo fdisk /dev/mmcblk0
        - Suppression de la partition / (d)
        - Création d'une nouvelle partition / (n)
          - Bien préciser comme start number le start number de la partition existant précédemment
        - sudo reboot
        - sudo resize2fs /dev/mmcblk0p2 (p2 correspondant à la partition /)
        - sudo reboot
        - df -h (pour vérifier le résultat)
  - J'ai pu exécuter le linter sur la raspberry emulée
  - Malheureusement, rencontre la même erreur que sur ma machine
    - En démarrant l'interpréteur python et en essayant d'importer le module RPi.GPIO, retrouve le même message d'erreur
    - i.e. "Module can only be run on a Raspberry Pi!"
  - Pose la question de la librairie utilisée/à utiliser
    - Projet utilise : https://sourceforge.net/projects/raspberry-gpio-python/
      - Mais qui n'a pas l'air fonctionnel sur un autre système que RPi
        - Et encore, n'a pas l'air de fonctionner dans mon env virtuel
      - Dernier commit date de février 2022
      - Mais projet a l'air encore d'actualité
        - Des issues sont encore crées et attribuées au maintainer
        - Notamment pour dev en dehors de l'env RPi
    - Existe RPIO : https://github.com/metachris/RPIO
      - Une alternative à RPi.GPIO
        - Peut être utilisée à sa place sans modifications
      - Mais plus maintenue depuis fin 2022
      - 'Fin, repo archivé depuis fin 2022
      - Le dernier commit date de 2013
      - Est-ce compatible avec les modèles de RPi utilisés ?
    - Des posts abordent la question des différentes librairies
      - e.g. https://raspberrypi.stackexchange.com/questions/58820/compare-and-contrast-python-gpio-apis
    - Voir si on peut s'en sortir avec RPi.GPIO, i.e. dev facilement sur un autre env que RPi
    - Ou s'il est préférable de remplacer la librairie utilisée pour une plus adaptée à un process de CI
  - Suis tombé sur la librairie Mock.GPIO : https://github.com/codenio/mock.gpio
    - Via ce post https://stackoverflow.com/questions/51879185/how-to-mock-rpi-gpio-in-python
  - Permet de résoudre le problème posé par RPi.GPIO
  - J'ai expliqué à Mickaël ce que j'ai fait et que j'aimerais poursuivre par l'ajout de tests
    - Semble d'accord
    - Souhaite juste limiter le temps passé à faire des tests
  - Pour le moment, j'ai fait une branche regroupant mes changements et ouvert une MR
  - Ok maintenant, que faire ?
  - Serait intéressant de retravailler mainApp et MQTTInterface
  - MQTTInterface
    - A trop de responsabilité IMO
      - Permet au noeud d'interagir avec le broker MQTT
        - Établir la connexion, publier les données collectées sur le broker, gérer les messages reçus via le broker et gérer les déconnexions
      - Permet au noeud d'interagir avec la base de données InfluxDB
        - Instancie le composant se connectant avec la DB et permettant d'y enregistrer les données collectées
      - Détermine à quel endpoint transmettre les données en fonction de sa config
        - Si mqttEnabled, envoie les données au broker
        - Sinon si influxDbEmbedded et subscriber, i.e. si influxDbEmbedded and subscriber and not mqttEnabled, envoie les données à influxDB
      - Gère aussi la config des capteurs dans le cas où le noeud est en mode autonome, i.e. il n'y a pas de communication via le broker pour indiquer les settings du noeud
    - IMO, devrait juste permettre l'interaction avec le broker MQTT
    - L'interaction avec la BDD devrait se faire via un autre composant
    - Et la config du noeud en mode autonome devrait être gérée par mainApp
  - mainApp
    - Pourquoi instancie deux MQTTInterface ?
      - Une subscriber, l'autre non
      - L'instance subscriber permet d'envoyer les messages ayant trait au provisioning
        - Donc utile qu'en mode connecté
        - Ah non, permet dans le cas du mode autonome de simuler une réponse du serveur
      - Les autres messages sont diffusés via l'instance not subscriber
        - Concerne le mode connecté et le mode autonome
    - Y a-t-il un intérêt à conserver ces deux instances ?
      - Permet d'avoir des threads séparés pour les types de message
      - Est-ce nécessaire ?
    - Pourquoi ce n'est pas lui qui instancie le composant pour interagir avec la BDD ?
    - Et qui détermine où sont propagées les données ?
  - Comment procéder ?
    - Avant de modifier le code et sa logique, je voudrais bien mettre en place des tests
  - Qu'est-ce que je veux tester ?
    - MQTTInterface
      - Se connecte bien au broker
        - Variante not subscriber
      - Si je fournis un message valide à l'instance MQTTInterface, il est bien publié sur le broker
        - Variante avec message invalide
      - Si je fournis un message valide au broker, il est bien récupéré par l'instance MQTTInterface
        - Variante avec message invalide
- Mettre en place des tests pour MQTTInterface
  - Peut faire les questions réponses dans un même test
    - i.e. demander à MQTTInterface d'envoyer un message sur un topic donné
    - M'être abonné au préalable à ce topic
    - Et ainsi vérifier que j'ai bien reçu le message
    - Et inversement
  - J'ai réussi à mettre en place un test simple
    - Créé un client MQTT
    - L'abonne à un topic
    - Publie un message sur ce topic via le helper de la librairie
    - Assert si mon client a bien reçu le message
  - Par contre, un test n'échoue pas par défaut si aucun assert n'est exécuté
    - Voir si on peut modifier ce comportement dans la config
  - [X] Ajout test publier sur le broker
    - But est de tester que si je fournis un message dans la Queue de l'instance MqttInterface, celui-ci est bien envoyé au broker
  - [X] Ajout test lire depuis le broker
    - But est de tester que si je poste un message sur le broker, MqttInterface transmet bien ce message à l'application via la Queue
    - Juste un problème pour publier le message sur le broker à temps
    - Si l'envoie dès que possible, MqttInterface loupe le message car n'a pas encore subscribe
    - Peut pas déclencher l'envoi du message en utilisant on_subscribe() de MqttInterface
      - Rencontre une erreur lorsque j'essaie de set on_subscribe()
      - Semble lié au fait que MqttInterface n'appelle jamais le constructeur de Client
      - Et ne dispose pas de tous les attributs qu'il est censé posséder
    - Workaround pour le moment en ajoutant une attente avant la publication du message
    - À corriger dans MqttInterface
  - [ ] Ajout test publier dans InfluxDB
    - But est de tester que si je fournis un message dans la Queue de l'instance MqqtInterface, celui-ci est bien envoyé à la BDD
    - Plusieurs détails à gérer
    - Tous les messages ne sont pas envoyés à la BDD
      - Un filtre est effectué pour ne stocker que les données intéressantes/pertinentes
      - Reste à comprendre les données attendues et les topics associés
    - L'écriture en BDD ne se fait pas à chaque donnée, mais une fois que le buffer dépasse un certain seuil
      - Doit donc procéder à l'envoi de suffisamment de données pour déclencher une écriture
      - Le seuil a l'air d'être 100 messages
      - P-e rendre paramètrisable cette donnée de façon à simplifier la validation des tests
    - Pour procéder par étapes, je vais déjà ajouter des tests sur DataToLocalInfluxDB
      - Composant chargé d'interagir avec InfluxDB, de formater la trame MQTT en une trame InfluxDB et de gérer l'écriture en BD des données
  - [X] Ajout test création de DB
    - But est de tester que DataToLocalInfluxDB créé bien une base de données à son instanciation
  - [X] Ajout test parseMqtt
    - But est de tester que DataToLocalInfluxDB transforme bien une trame MQTT en une trame InfluxDB
  - [X] Ajout test sendPointsToInfluxDB
    - But est de tester que DataToLocalInfluxDB écrit bien les données en BDD
      - Doit par contre atteindre la taille minimum pour déclencher une écriture
      - En attendant, données bufferisées
** Semaine du <2024-01-15 Mon> au <2024-01-19 Fri>
*** Planned
**** DONE Prendre en main le script de connexion aux brokers MQTT
CLOSED: [2024-01-16 Tue 13:38]
- Mickaël m'a partagé son script de test
- Voir pour l'essayer et le comprendre
**** DONE Résoudre problème d'accès aux fichiers sur WordPress
CLOSED: [2024-01-18 Thu 10:25]
- Lorsque je réplique mon serveur d'application WordPress, je constate que WordPress rencontre régulièrement un problème pour afficher les images
  - Les fichiers que j'ai uploadé moi-même pour créer la page
- Probablement dû au fait que le volume qui stocke ces images n'est pas partagé par l'ensemble des instances
- Étudier comment faire évoluer l'application pour corriger cela
**** DONE Résoudre problème d'authentification sur WordPress
CLOSED: [2024-01-18 Thu 10:25]
- Lorsque je réplique mon serveur d'application WordPress, je me retrouve à devoir me reconnecter à chaque changement de page
- Probablement dû au fait que mon cookie d'auth est valide pour une instance donnée, et est invalidé par les autres
- Étudier comment faire évoluer l'application pour corriger cela
**** DONE Se familiariser avec le concept d'Infrastructure as Code (IaC)
CLOSED: [2024-01-22 Mon 09:11]
- Plutôt que de setup manuellement Kubernetes sur ses machines
- Semblerait que la pratique soit d'automatiser son setup
- Process connu comme l'Infrastructure as Code
- Se renseigner et documenter à ce sujet
- Ressources rapides
  - https://learn.microsoft.com/en-us/devops/deliver/what-is-infrastructure-as-code
  - https://aws.amazon.com/what-is/iac/
- Guillaume mentionnait notamment l'outil Vagrant
  - https://www.vagrantup.com
**** IN-PROGRESS Automatiser la création d'un cluster de Raspberry virtualisé
- Plutôt que de travailler avec le cluster physique
- Serait intéressant d'apprendre à provisionner un cluster virtuel de Raspberry
- Permettrait de pouvoir créer un environnement pour dev et effectuer des tests
  - Notamment comment setup k8s ou équivalent sur un tel système
**** IN-PROGRESS S'approprier SmartSense/smartsense-node-app
- On est en train de revoir les priorités avec l'arrêt maladie de Guillaume
- Serait préférable que je passe sur SmartSense en attendant son retour
- Les différentes tâches identifiées concernent principalement le capteur SmartSense
  - Ajout d'une couche réseau WiFi
  - Ajout de traitements en local des données collectées
    - i.e. sur le capteur
  - Correction des bugs rencontrés lors de l'utilisation de la BDD en local
    - Dans la version du capteur pour un fonctionnement autonome
    - Le capteur fait tourner une instance de la BD, InfluxDB, en local
    - Mais lors de leurs expériences, ont constaté des crashs de la BDD
      - Parfois au bout de plusieurs heures
- Il serait donc intéressant que je rentre rapidement dans ce projet
*** Done
- Résoudre problème d'accès aux fichiers sur WordPress
  - Plusieurs pistes possibles
  - Modification de la config du volume dans la description de l'application
    - La configuration que j'ai utilisé précise que le volume persistant est en ReadWriteOnce
    - i.e. qu'un seul noeud peut l'utiliser en mode RW
    - Cohérent avec le problème constaté
    - L'option ReadWriteMany permet de spécifier qu'il sera utilisé par plusieurs noeuds
      - Permettrait ainsi de rendre accessible les fichiers à tous les noeuds en ayant besoin
      - Solution limitée cependant
        - En interne, fonctionne avec un NFS d'après ce que m'explique Guillaume
        - Les performances sont donc pas adaptées à une charge conséquente
    - A testé avec cette configuration, sans succès
      - L'image, de nouveau, n'est pas chargée régulièrement lors de l'affichage de la page
    - La raison m'échappe
    - Je commence à avoir un doute sur le fait que le volume soit correctement partagé par les noeuds
    - En explorant les containers, j'ai observé des différences
      - kubetcl exec -it <container> -- /bin/bash
      - Seul un container possède le fichier uploadé
      - Un fichier créé manuellement dans un container n'apparait pas dans l'arborescence des autres
    - En creusant, j'ai remarqué que la configuration pour la gestion des volumes pour un cluster avait été perdue
    - En réactivant cette config, cela fonctionne
      - Fichier uploadé bien disponible sur l'ensemble des instances
      - Modifications manuelles sont bien observables sur l'ensemble des instances
    - Effet de bord intéressant mais intriguant : corrige aussi le problème d'authentification
      - Comprend pas la logique derrière
  - Ajout d'un service de sync des volumes
    - Je suis surpris de ne pas trouver d'articles qui présentent cette solution et détaillent comment la mettre en place
    - Ni de trouver un outil/composant qui assumerait ce rôle
    - Les gens reposent sur des solutions customs à base de rsync ?
    - Tombé sur *VolSync*, un système de réplication async entre volumes dans ou entre clusters
      - Disponible ici : https://github.com/backube/volsync
      - Projet Red Hat
      - Pas sûr que l'outil soit adapté pour répliquer des fichiers en temps réel vu les cas d'usages présentés
        - Plutôt  l'impression que c'est pour propager des données à terme, pour résilience ou traitements à posteriori sur les données
        - cf. https://next.redhat.com/2021/08/23/introducing-volsync-your-data-anywhere/
    - Demander des précisions sur le service mentionné par Guillaume
  - Est-il sinon possible d'override le fonctionnement de WordPress pour héberger les fichiers ?
    - Plutôt que d'essayer de retomber sur nos pattes en ajoutant des rustines
      - Partager un même volume entre pods
      - Sync les volumes de nos pods
    - Serait-il pas mieux et possible de faire déléguer à WordPress la gestion des fichiers à un service tiers, dédié à cela ?
      - Je suppose que WordPress est assez flexible pour cela
    - Genre mettre en place son propre CDN
- Résoudre problème d'authentification sur WordPress
  - Comme évoqué précédemment, partager un même volume entre les instances WordPress a pour effet de bord de résoudre ce problème
    - D'une manière que je ne comprends pas
    - Quoique
    - Cela s'explique si le serveur ne conserve aucune donnée en mémoire entre 2 requêtes
    - Et recréé l'état, e.g. de la session, à partir des infos fournies par la requête, e.g. cookies, et de fichiers
    - Une rapide recherche confirme ce mode de fonctionnement
  - Donc c'est une approche pour résoudre le problème des sessions, mais p-e pas la plus adaptée/conseillée
  - Guillaume m'a conseillé de regarder du côté des sessions PHP partagées
- Formation SED - Bonnes Pratiques du Dev Logiciel
  - Dans toute équipe de recherche, y a correspondant SED pour entrer en contact
  - Existe un GitLab pour projets avec données confidentielles
    - https://gitlab-int.inria.fr
  - Tests et intégration continue
    - Tests métiers
      - Vérifier que son logiciel est compris et utilisé par les personnes devant l'utiliser
    - Analyse statique
      - Mettre en place linter/convention de codage
      - Utiliser des outils de vérification de la qualité du code
        - e.g. sonarqube
        - Voir si facilement pluggable/hookable au gitlab
    - Documentation
      - Existe gitlab-pages
      - Recommandations sur comment écrire la doc : https://smartbear.com/blog/13-things-people-hate-about-your-open-source-docs/
  - Licences conseillées
    - MIT
    - GPL/LGPL
    - BSD
- Prendre en main le script de connexion aux brokers MQTT
  - Dépendance
    - Ce script nécessite pour fonctionner l'installation d'une librairie pour instancier un client MQTT
      - pip install paho-mqtt
  - Ce script se connecte à ou plusieurs brokers MQTT de la plateforme SmartSense
    - Choix codé en dur
  - Il récupère les messages postés sur le topic, les décode et les affiche
  - S'interrompt au bout d'une minute
  - Fonctionne
  - Particulier au niveau de la connexion aux topics
    - Tous les topics n'ont pas l'air d'utiliser la même configuration
    - Notamment au niveau du port utilisé
      - Pourquoi ?
  - Semble associer un port différent à chaque zone
    - Plusieurs zones à Lannion, plusieurs à Rennes
  - Semble retrouver cette information directement dans l'URL d'un topic
    - e.g. "event/p/production/Rennes/0/E/..." indique un topic d'un capteur situé à Rennes, dans le secteur 0
    - Si j'ai bien pigé
    - Un peu confus sur la signification du champ suivant
      - Vaut 3 ou E en fonction des topics présentés
    - D'après la doc, indique le sous-secteur
      - Doc dispo ici, page 8 : https://gitlab.inria.fr/smartsense/3douest/documents/conception/-/blob/master/CDC-20190806-Design%20MQTT%20et%20InfluxDB-V1.4.pdf
- Se familiariser avec le concept d'Infrastructure as Code (IaC)
  - Un peu de mal à piger si Vagrant est adapté à notre use-case
    - Outil permettant de déployer des environnements
      - Instancie des VMs selon la configuration donnée
      - Exécute le/les scripts fournis
      - Copie une partie du FS dans la VM
    - On a probablement pas envie de déployer notre application à même les raspberry du serveur
    - Mais voulons-nous utiliser des VMs pour autant ?
    - Préférions-nous pas utiliser simplement des conteneurs ?
      - Installer k8s (ou plutôt k3s probablement) sur les raspberry
      - Configurer le cluster pour définir les noeuds et leurs rôles
      - Déployer l'application
  - A un intérêt donc, mais plutôt pour la partie dev/testing IMO
    - Pour provisionner/recréer le cluster de raspberry localement
  - Est-ce que son rôle s'arrête là ?
    - i.e. utiliser un autre outil pour setup le cluster k8s ?
      - Ansible ?
  - Serait intéressant de voir avec Khaled ce qu'il fait dans le cadre de ses expériences
    - Est-ce qu'il simule des raspberry ?
    - Quels outils il utilise ?
      - Pour provisionner les machines virtuelles
    - Est-ce qu'il a mis ses ressources, configurations et scripts à disposition ?
      - Genre dans son article
  - Il m'a donné accès à son repo avec tout les scripts pour son setup experimental
    - Dispo ici : https://gitlab.inria.fr/stream-processing-autoscaling/scalehub
  - De ce que j'ai compris
    - Réserve des noeuds sur g5k
    - En utilisant les IPs des machines attribuées, lance un script Ansible qui les préparent à la configuration
      - Setup SSH
    - Puis exécute un script Ansible qui installe k3s sur les noeuds
      - A rencontré des problèmes pour setup k3s, mais ne se souvient plus quoi
      - A commencé à creuser l'alternative mini-k0s
      - Mais a résolu son problème sur k3s
    - À partir de là, fait tout par le biais de k3s
      - Déploie des services supplémentaires en fonction de ses besoins
      - Prometheus, Grafana, Kafka, Flink
  - Par contre, ne fait aucune virtualisation des machines
    - Va falloir que je me débrouille pour cette partie là
  - Parcours le livre *Infrastructure as Code* de Kief Morris
- Automatiser la création d'un cluster de Raspberry virtualisé
  - Ressources :
    - Tuto suivant a l'air plutôt complet sur comment virtualiser une Raspberry : https://linuxconfig.org/how-to-run-the-raspberry-pi-os-in-a-virtual-machine-with-qemu-and-kvm
    - Celui-ci a l'air plus d'actualité : https://interrupt.memfault.com/blog/emulating-raspberry-pi-in-qemu
    - Ou celui-ci : https://brettops.io/blog/custom-raspberry-pi-image-no-hardware/
  - Pose la question de l'outil VM à utiliser
    - Souhaite émuler du ARM
    - Est-ce une bonne idée ?
  - Essayons, on jugera à l'essai
  - Pars donc sur l'émulation d'une Raspberry à l'aide de QEMU
  - Suivi du tuto https://interrupt.memfault.com/blog/emulating-raspberry-pi-in-qemu
  - Erreur rencontrée avec la dernière image de Raspberry OS
    - OS : https://downloads.raspberrypi.com/raspios_lite_arm64/images/raspios_lite_arm64-2023-12-11/2023-12-11-raspios-bookworm-arm64-lite.img.xz
    - usbnet: failed control transaction: request 0x8006 value 0x600 index 0x0 length 0xa
    - Aucun autre message ne s'affiche dans le terminal, qui ne répond plus
  - J'ai re-essayé en utilisant cette fois-ci la version précédente de l'OS
    - OS : https://downloads.raspberrypi.com/raspios_oldstable_lite_arm64/images/raspios_oldstable_lite_arm64-2023-12-06/2023-12-05-raspios-bullseye-arm64-lite.img.xz
    - Cette fois-ci, la Raspberry a l'air de se lancer
      - Retrouve l'erreur parmi les logs, mais n'a pas l'air bloquante
    - Pu in fine me logger au système
  - Ok, comment on instrumentalise ça avec Vagrant maintenant ?
  - Plutôt voir déjà comment on automatise le lancement de VMs avec Vagrant
  - Suivi le tuto : https://developer.hashicorp.com/vagrant/tutorials/getting-started
    - Étrangement, n'utilise pas virtualbox en provider par défaut
      - Je ne l'avais pas installé au moment où j'ai installé vagrant, probablement pour cela
      - Utilise donc libvirt à la place
      - Sauf que libvirt n'a pas l'air compatible avec toutes les boxes
        - Genre, celle du tuto
      - J'ai set la variable d'env
        - VAGRANT_DEFAULT_PROVIDER="virtualbox"
      - Mais n'a aucun impact à la création d'un env
    - Doit donc spécifier le provider au démarrage
      - vagrant up --provider="virtualbox"
    - A pu démarrer la VM, une Ubuntu 18.04.3, et s'y connecter en SSH
      - vagrant ssh
  - Maintenant, comment on lance plusieurs VMs avec Vagrant ?
  - Ça se fait bien, suffit d'en définir plusieurs dans le fichier de config
    - https://gitlab.inria.fr/mnicolas/vagrant-getting-started/-/blob/382bfe988d13dfbe450cb0b5e3ee459bfd70cdbd/Vagrantfile
  - Temps de s'intéresser à la partie réseau maintenant
  - Notamment comment SSH une VM depuis l'autre, et inversement
- S'approprier SmartSense/smartsense-node-app
  - Point d'entrée est mainApp
    - Instancie les différents composants logiciels du capteur
  - Pour le moment, identifie les composants suivants
    - ClientMQQT & PublisherMQQT
    - INAManager
    - USBManager
    - NTPManager
    - RpiGpioManager
    - FirmwareUpdateManager
    - TLCManager
    - Syncer
  - MQQT
    - Système de message brokers
    - Permet au noeud de communiquer avec le serveur
      - Remonter les données collectées
      - Mais aussi de recevoir des instructions
        - e.g. changement de configuration
  - NTPManager
    - Système de synchronisation d'horloges du noeud avec le serveur central
    - Du sens si on utilise une timeseries database
  - RpiGpioManager
    - Gère l'alimentation des ports GPIO de la Raspberry
    - À quoi correspondent ces ports ?
    - Dans le code, on retrouve la mention de
      - Sensor Board
      - Ext1 et Ext2
    - Peut supposer que la sensor board est la carte sur laquelle sont branchés les différents capteurs
    - Tandis que Ext1 et Ext2 correspondent aux ports disponibles pour brancher des extensions supplémentaires
      - cf. https://gitlab.inria.fr/smartsense/3douest/documents/conception/-/blob/master/SMARTSENSE-Module%20d'extension%20pour%20noeud.pdf
  - mainApp
    - Instancie l'ensemble des composants
    - Met en place les processus de contrôle périodique du bon fonctionnement du capteur
    - Sync son horloge
    - Alimente les différents capteurs du noeud
    - Démarre le process mqttManagerCmdProvisioning
      - Lit en boucle la Queue donnée en entrée en quête de messages
      - Dès qu'un message est détecté, le publie au MQTT Broker
    - Pas sûr de comprendre les lignes suivantes
      - https://gitlab.inria.fr/smartsense/3douest/node-app/smartsense-node-app/-/blob/main/mainApp.py?ref_type=heads#L378-381
      - Envoie par le biais du message broker un message au serveur
      - Mais dans quel but ? Que signifie provisioning dans ce contexte ?
  - Plusieurs réflexions sur le code et projet
    - Absence d'un gestionnaire de dépendances
    - Absence de tests
      - Pose la question de comment développer et tester
      - Est-ce que développe à même le capteur ?
      - Ou fait tourner le programme dans une VM pour tester ?
      - Est-ce que l'architecture différente ARM implique des étapes supplémentaires ?
      - Globalement, qu'est-ce qu'il faudrait faire pour mettre en place un process de CI ?
    - Absence de linter
    - Quelle méthodologie de travail ?
      - Si je veux faire des modifs
    - Absence de doc
  - Réflexions sur mainApp
    - startProcess()
      - Pourrait utiliser un enum plutôt que des chaines de caractères pour spécifier le process à démarrer
        - cf. https://docs.python.org/3/library/enum.html
      - À l'exception de usbBoardManager, la logique est la même pour chaque process
        - Seul l'instanciation du process change
        - Pourrait factoriser le code
  - Réflexions sur MqttInterface
    - run()
      - Pourquoi while True and self.stopLoop is False ?
        - Et non pas while not self.stopLoop ?
      - Pourquoi une attente active sur la Queue ?
        - Pas possible d'utiliser de l'event-based ?
        - Semblerait que non, d'après l'API
          - cf. https://docs.python.org/3/library/multiprocessing.html
      - Pourquoi le type Queue ?
        - Permet de passer des messages
        - Canal de diffusion avec plusieurs producers et subscribers possibles
          - cf. https://docs.python.org/3/library/multiprocessing.html#pipes-and-queues
        - Mais à messages à usage unique
          - i.e. lire un message le consomme
        - Ne supporte pas le pattern fan out du coup
        - Pas le plus pratique si on veut déclencher plusieurs traitements pour un même message
          - e.g. stocker en local et diffuser sur le réseau
        - L'utilisation que j'en vois pour le moment est d'un composant à un autre
          - De mainApp à mqqtInterface par exemple
        - Voir si c'est la structure de données la plus adaptée à notre use case finalement
      - Pourquoi prend en paramètre queueDataIn et queueDataOut ?
        - Puisque n'utilise pas queueDataOut de toute la méthode
        - Ne pourrait-on pas passer ces attributs au constructeur plutôt ?
** Semaine du <2024-01-08 Mon> au <2024-01-12 Fri>
*** Planned
**** DONE Régulariser situation du 02/01
CLOSED: [2024-01-09 Tue 14:36]
**** DONE Suivre cours de Guillaume sur les technologies cloud
CLOSED: [2024-01-10 Wed 11:47]
- Disponible ici : https://gitlab.inria.fr/pierre/sct-m1info
**** DONE Trouver des ressources sur Docker & Kubernetes
CLOSED: [2024-01-10 Wed 13:49]
- Au-delà du cours de Guillaume, existe des ressources pour rentrer plus en détails sur ces outils (talks, livres)
- Voir pour en trouver et les consulter
**** DONE Regarder *Kubernetes Design Principles: Understand the Why*
CLOSED: [2024-01-11 Thu 15:57]
- Talk en 2018 de Saad Ali, ingé Google de l'équipe sur k8s
  - Dispo ici : https://www.youtube.com/watch?v=ZuIQurh_kDk
**** DONE Adapter la configuration réseau pour clusters multi-nodes
CLOSED: [2024-01-11 Thu 16:42]
- Lors de l'ajout du 2nd Node à mon cluster minikube, j'ai eu le warning suivant
  - Cluster was created without any CNI, adding a node to it might cause broken networking.
- Voir ce que cela signifie et ce que je dois modifier
**** DONE Utiliser un driver pour Volume adapté aux clusters multi-nodes
CLOSED: [2024-01-11 Thu 17:01]
- La page tuto de k8s indiquant comment lancer un cluster multi-nodes mentionne un problème avec le driver pour Volume par défaut
  - https://minikube.sigs.k8s.io/docs/tutorials/multi_node/
- Renvoie à la page suivante :
  - https://minikube.sigs.k8s.io/docs/tutorials/volume_snapshots_and_csi/
- Voir si le problème est toujours d'actualité et si c'est bien la solution conseillée
**** DONE Prendre en main Kubernetes
CLOSED: [2024-01-12 Fri 13:15]
- J'ai atteint la partie du cours de Guillaume présentant Kubernetes
- Voir maintenant pour expérimenter avec histoire de creuser l'outil
- Ressources disponibles :
  - Le TP du cours de Guillaume : [[file:~/Documents/sct-m1info/support/pdf/tp08.orchestration.pdf]]
  - Le tuto de Digital Ocean sur faire fonctionner Kubernetes en local : https://www.digitalocean.com/community/tutorials/how-to-use-minikube-for-local-kubernetes-development-and-testing
**** DONE Déployer une application complexe avec k8s
CLOSED: [2024-01-15 Mon 08:45]
- Les tutos que je suis pour le moment se contentent de déployer des applications simples
  - I.e. Un pod faisant tourner un nginx
- Pour apprendre correctement k8s, serait intéressant de déployer une application composée de
  - Serveurs d'applications, répliqués
    - Avec un load balancer pour répartir la charge
  - Interagissant avec une BDD
    - Elle aussi répliquée ?
- Cela permettrait de creuser
  - La configuration et le déploiement de pods différents
  - Les interactions entre ces pods, potentiellement sur des noeuds différents
  - L'utilisation de volumes
  - L'utilisation de fichiers de description
- Exemple
  - *Deployment of multiple apps on Kubernetes cluster — Walkthrough* : https://wkrzywiec.medium.com/deployment-of-multiple-apps-on-kubernetes-cluster-walkthrough-e05d37ed63d1
*** Done
- Suivre cours de Guillaume sur les technologies cloud
  - CM5 - Services cloud réseau
    - S'intéresse aux différents services réseau mis à disposition par les cloud providers
    - Bien beau d'instancier des VMs/conteneurs
    - Mais doit leur attribuer une adresse IP privée
      - Et une adresse IP publique pour ceux qui doivent pouvoir être contactés de l'extérieur
      - Possède un pool d'adresses IPs qui vont être attribuées dynamiquement aux instances
    - Doit créer les routes de communication entre ces instances, et entre ces instances et le monde extérieur
      - Utilise des VLANs et probablement des techniques de SDNs
    - Doit aussi considérer l'aspect sécurité
      - Mettre en place des pare-feux, VPNs
      - Provider clouds proposent des services de pare-feux
        - FWaaS : FireWall as a Service
    - Finalement, pour la scalabilité, doit généralement mettre en place du load balancing
      - LBaaS : Load-Balancing as a Service
    - Questions
      - C'est quoi exactement la différence entre VLANs et SDNs ?
        - P-e lire un peu à ce sujet
        - *Cloud Network Virtualization: Benefits of SDN over VLAN*
          - Blogpost disponible ici : https://cloudsecurityalliance.org/blog/2021/06/25/cloud-network-virtualization-benefits-of-sdn-over-vlan/
          - De ce que je comprends, les VLANs ont initialement été conçus pour créer plusieurs réseaux virtuels au sein d'un même réseau local
            - Limité au sein du LAN
          - Pas les mêmes conditions que le cloud
            - Un single-tenant vs. multi-tenant
            - Pas la même échelle
          - Ne sont donc pas adaptés à ce nouveau cas d'usage
            - Particulièrement d'un point de vue sécu/isolation
          - L'approche SDN répond à ce nouveau besoin
            - Découple le /control plane/ du /data plane/, i.e. découple le routing de l'envoi effectif des messages
              - Un peu de mal à piger les implications de cela
              - Cela me paraît évident que ça doit être découplé
              - Ne dois pas comprendre les contraintes matérielles
            - Permet de configurer plus finement et simplement les firewalls
              - Adopte la politique du /default deny/, contrairement à l'existant
            - Protège d'attaques nativement
            - Conçu pour l'élasticité
        - *Network Virtualisation and the difference with VLANs, SDNs*
          - Blogpost disponible ici : https://craigread.cloud/network-virtualisation-and-the-difference-with-vlans-sdns/
          - Re-explique qu'un VLAN permet de diviser un LAN en de multiples réseaux
          - Explique que le VLAN n'est pas de la virtualisation de réseau
            - Pas moyen de prendre une snapshot du réseau, de le cloner ou déplacer
            - Pas sûr de comprendre de ce qu'on entend par cloner un réseau concrètement
              - Et de l'usage qu'est fait de cette fonctionnalité
          - Précise aussi que SDN n'est pas de la virtualisation non plus
            - Ne virtualise pas les composants, e.g. switchs et routeurs
            - Mais permet de les contrôler logiciellement
          - Mais que la virtualisation de réseau existe belle & bien
            - Permet de virtualiser le réseau complet, hardware compris
          - Quand utiliser SDN vs. Network Virtualisation ?
  - CM6 - Microservices
    - Porte sur l'évolution de l'architecture système des applications
    - Anciennement, architecture monolithique
      - Simple
      - Mais des limites
        - Pas de contrôle de droits d'accès sur les données par domaine/métier
        - Un bug d'un domaine/métier de l'application peut la faire crasher dans son entièreté
          - i.e. pas d'isolation
        - Difficile à scale
          - La base de données est un bottleneck
          - De part le fonctionnement des writes et des transactions
    - Architecture orientée micro-services
      - Décompose l'application en multitude de services
      - Chaque service doit avoir une fonctionnalité précise
        - Separation of Concern
      - Les services peuvent communiquer entre eux, si nécessaire, par le biais de leur API
      - Chaque service est responsable de ses données
        - Chaque service peut ainsi choisir ses outils, i.e. son SGBD, en fonction de ses use cases
      - Principes d'une architecture orientée micro-services
        - Se base sur : https://nirmata.com/2015/02/02/microservices-five-architectural-constraints/
        - Elastic : chaque service doit pouvoir scale up/down de manière indépendante des autres services
        - Resilient : un service doit crasher sans impacter les autres services
        - Composable : les services doivent proposer des APIs uniformes et conçues pour la composition
        - Minimal : un microservice doit être composé uniquement des entités fortement liées
        - Complete : un microservice doit être fonctionnellement complet
      - Pour la communication entre services, une approche éprouvée est d'utiliser un message broker
        - Permet de découpler les composants
        - Pas de blocage pour l'initiateur d'une requête pendant le calcul de la réponse
        - Permet de scale le service produisant la réponse en fonction de la workload de manière transparente
    - Aborde ensuite l'approche DevOps
      - Là aussi, devrais lire plus à ce sujet
      - *What is DevOps*
        - Disponible ici : https://about.gitlab.com/topics/devops/
        - Méthodologie consistant à coupler les tâches des équipes de développement et d'opérations (déploiement)
        - A pour but de
          - Mettre en place un cycle de développement incrémental
          - Livrer rapidement les nouvelles versions du logiciel
          - Améliorer la qualité du logiciel
        - Cela passe par
          - Collaboration approfondie entre les équipes dev et ops
            - Des équipes à objectifs intrinséquemment différents et parfois contraire
              - Dev : Faire évoluer rapidement l'application pour répondre aux retours
              - Ops : Garantir le bon fonctionnement de l'application
            - L'idée est ici de les faire faire cause commune
          - Incorporation et automatisation de bonnes pratiques
            - Tests, Livraison, Déploiement
        - Se base sur les 4 principes suivant
          - Automatisation des phases du cycle de vie du logiciel
            - Test, build, release
          - Collaboration et communication
            - Entre les anciennes différentes équipes
          - Amélioration continue et minimisation des pertes de temps
            - Automatisation des tâches répétitives
            - Identification perpétuelle de pistes d'amélioration
          - Focalisation sur les besoins des utilisateur-rices
            - L'automatisation des tâches permet de se focaliser sur les retours des utilisateur-rices
            - Et livrer rapidement une nouvelle version y répondant grâce à l'accélération du cycle de vie de l'application
  - CM7 - Conteneurs et Docker
    - Présente Docker
    - Rappelle qu'on a un intérêt à virtualiser
      - Permet d'isoler les différents composants d'une application
      - D'embarquer l'ensemble des dépendances
      - Et d'éviter les potentiels conflits, e.g. dépendances incompatibles
    - Mais que les VMs sont volumineuses, lentes à instancier et ajoutent un surcoût computationnel
    - Les conteneurs répondent aux mêmes problématiques
    - Mais de manière plus efficace
      - Reposent sur l'OS de la machine
        - Permet d'éviter l'utilisation coûteuse d'un hyperviseur
      - Reposent sur le système de layers
        - Permet de partager/factoriser des mêmes layers entre conteneurs
    - Précise cependant que Docker n'est un outil nativement conçu pour un usage dans le cloud
      - Conçu plutôt pour tourner sur une machine donné
    - Un orchestrateur est nécessaire pour cela
  - CM8 - Kubernetes et Orchestration de conteneurs
    - Les conteneurs, c'est bien
    - Mais dans un environnement cloud, ils ne sont pas suffisants par eux-mêmes
    - Entre autres, des besoins de
      - Scaling automatique
      - Détecter et redémarrer les conteneurs ayant une panne
      - Mettre en place des configurations réseaux avancées
    - Kubernetes permet de répondre à ces besoins
    - Notion de pod
      - Kubernetes permet de créer des pods
      - Un pod contient un ou plusieurs conteneurs et volumes
      - Et possède une adresse IP pour le tout
      - *NOTE* Si un élément du pod rencontre une panne, Kubernetes tue le pod entier
      - Pour créer pods, se basent sur des fichiers de description
        - À la *docker-compose*
    - Insiste sur le fait qu'il *ne faut pas utiliser un unique pod*
      - Pod peu gourmand, n'utilise qu'une fraction des ressources du noeud
      - Pod éphémère, peut être tué par Kubernetes de manière inopinée, sans sommation
    - À la place, *utiliser un groupe de pods identiques*
    - Notion de Controller
      - Kubernetes est un outil déclaratif
        - Users n'indiquent pas quelles commandes effectuer
        - Mais quel est l'état désiré
        - Kubernetes se charge de transitionner de l'état courant à cet état cible
          - [[file:img/kubernetes-reconciliation-loop.png]]
      - Propose plusieurs types de controllers
        - /Deployment/ a l'air d'être le controller "par défaut"
        - /StatefulSet/ pour les applications stateful
          - À la mort d'un pod, le recréé en réutilisant le même volume
        - /Job/ pour les tâches courtes
        - /DeamonSet/ pour que tous les noeuds matchant un critère démarre une instance d'un pod
          - Prend en compte les noeuds qui apparaissent au cours de la vie de l'application
        - Possibilité de créer de nouveaux controllers si besoin
      - Commandes existent pour manipuler directement les controllers
        - E.g. pour déployer une application
        - Étrangement, le niveau de granularité a l'air d'être sur l'image Docker et non pas le pod
      - Mais fonctionne aussi via des fichiers de description
      - Comment ça marche si application nécessitent de combiner plusieurs controllers ?
        - Un fichier unique ?
        - Ou un ensemble de fichiers de descriptions ?
    - Controllers incorporent des mécanismes supplémentaires
      - E.g. *Rolling Updates* : déploie progressivement de nouveaux pods se basant sur une nouvelle image puis interrompt les anciens pods
    - Kubernetes déploient aussi des Services
      - Sert de front-end pour les pods
      - Observe les pods pour déterminer à quel pod transmettre une requête
      - Se base pour cela sur un (des?) Selector(s)
        - Comment fonctionnent-ils ?
        - Possibilité/Besoin d'en faire des customs ?
    - D'un point de vue réseau
      - Communications entre containers se font via localhost
      - Communications entre pods (d'un même noeud) se font via les adresses IPs uniques des pods
      - Communications entre pod et service se font via l'adresse IP unique du service
      - Comment un container découvre l'adresse IP d'un pod/du service ?
    - Précise que Kubernetes ne repose sur le runtime Docker depuis sa v1.20
      - Utilise toujours les images Docker
      - Mais utilise un (des?) runtime(s) plus efficaces et standardisés
      - Quid des volumes et networks ?
        - Ne reposent pas du tout sur les solutions proposées par Docker ?
      - Est-ce que ça a un impact sur la façon de créer ses images Docker ?
- Réunion avec Guillaume le <2024-01-10 Wed>
  - Préparation
    - HS RH
      - A fait une demande de régularisation de congé pour le 02/01
      - A permis de détecter quelques problèmes
        - Personne qui valide mes demandes de congés
        - Jours reportés de l'an dernier
    - Technologies Cloud
      - Suivi le cours jusqu'au CM sur l'orchestration
        - M'a permis de revoir les bases
          - I/P/SaaS
            - Un peu de mal à délimiter PaaS
          - Infrastructures et Services
            - Ne connaissais pas OpenStack
            - Et que certaines organisations mettaient en place leur cloud privé
            - Par contre, est-ce qu'on retrouve les mêmes outils dans le fog ?
              - Ou est-ce trop gourmand ?
          - Services de stockage
            - Les SGBDs relationnels sont si peu adaptés au cloud ?
            - Pas trop creusé le sujet, mais j'entendais parler de NewSQL
      - Commence à expérimenter avec k8s
        - Installé minikube sur ma machine
        - En train de parcourir les tutos sur créer cluster, déployer simple application web
        - Et d'apprendre les concepts (Pods, Nodes, Services, Deployment...)
        - Curieux du fonctionnement du Control Plane pour qu'il ne soit pas un SPOF
        - Surpris que k8s soit pas un environnement unique, mais une multitude de distribution
          - Ai vu qu'il y a des distribs faites pour l'IoT : k3s, k0s
      - As-tu des ressources que tu conseilles, notamment sur Docker & Kubernetes ?
        - Understanding Docker/Kubernetes in a visual way par Aurélie Vache
    - Observatoires
      - Consulté le site d'Ammar sur les résultats de son questionnaire
        - Et débriefé avec lui
      - M'a permis de constater la grande hétérogénéité des observatoires
        - Source d'énergie, réseau disponible, etc.
      - Quels sont nos objectifs ?
        - À qui on s'adresse ?
        - Quelles sont nos contraintes ?
      - Ammar m'a parlé d'OZCAR et m'a linké un article
        - Prévois de le lire pour mieux comprendre les enjeux des observatoires
  - Notes
    - Deployment
      - Outil de base de k8s
    - Peut associer un Service LoadBalancer à un Deployment
    - k8s se focalise sur l'état desiré et l'état observé
      - Enregistre dans BDD l'état désiré
      - Puis observe son état
        - Outil de monitoring souvent ajouté : Prometheus
    - Voir du côté de Vagrant
      - Infrastructure as Code
        - Décrit l'infrastructure que l'on souhaite déployer via des services Cloud
      - Vagrant est l'équivalent local
        - Utilisé dans LivingFog
      - Permet de déployer Kubernetes et consorts
    - Observatoires
      - Nous nous intéressons aux observatoires
        - Isolés
        - Variétés de capteurs
        - Variétés d'utilisateurs
        - Contraintes sur énergie et bande-passante
      - Mais aurons quand même grande hétérogénéité
        - Type de tâches
        - Volume de données
      - Sujet à considérer est la problématique du changement
        - Comment accompagner les scientifiques dans l'adoption de la solution que l'on va proposer ?
        - P-e voir avec les ingés du service d'hydrologie pour déployer nos essais
          - Ont mis en place un petit observatoire au niveau du ruisseau
            - Avec capteurs
            - Et autres ?
- Régulariser situation du 02/01
  - A envoyé une demande de régularisation
  - Sur les conseils de Myriam, en a profité pour notifier des problèmes de
    - Personne qui valide mes demandes de congés
    - Jours reportés de l'an dernier
  - Demande a été traitée
- Prendre en main Kubernetes
  - Plutôt que de faire tourner l'environnement kubernetes en complet sur sa machine
  - Semble plus commun d'utiliser un outil pour virtualiser le cluster et les différents composants de k8s
  - Plusieurs outils existent
    - minikube : https://github.com/kubernetes/minikube
      - Outil dev par l'équipe de k8s
    - kind : https://github.com/kubernetes-sigs/kind
      - Outil dev par l'équipe de k8s
      - Conçu initialement pour tester k8s
      - Indiqué comme pouvant être aussi utilisé pour le dev d'applis locales
  - Plusieurs blogposts font des comparaisons entre ces outils
    - https://www.blueshoe.io/blog/minikube-vs-k3d-vs-kind-vs-getdeck-beiboot/
    - https://shipit.dev/posts/minikube-vs-kind-vs-k3s.html
    - https://alperenbayramoglu2.medium.com/simple-comparison-of-lightweight-k8s-implementations-7c07c4e6e95f
    - Pour prendre en main k8s, les différentes options semblent se valoir
      - [[file:img/kubernetes-distrib-comparaison.png]]
  - Je croyais que k8s était un logiciel/environnement unique
  - Mais il semble y avoir une multitude de distributions différentes
    - Notamment des distribs conçues pour/orientées IoT & Edge
    - K3s : https://github.com/k3s-io/k3s
    - MicroK8s : https://github.com/canonical/microk8s
  - Pour démarrer, suis le tuto : https://www.digitalocean.com/community/tutorials/how-to-use-minikube-for-local-kubernetes-development-and-testing
    - Quelques difficultés à la première étape
      - minikube plantait silencieusement
      - Ajouter l'option /--driver=docker/ a permis de dépasser l'erreur rencontrée
        - Ai ajouté l'option à ma config par défaut
          - minikube config set driver docker
    - Ai pu suivre le reste du tuto sans erreurs
    - Pas trop compris les points suivants
      - kubectl create deployment web --image=gcr.io/google-samples/hello-app:1.0
        - Permet de créer un deployment nommé web en utilisant l'image passée en option
        - Mais c'est quoi un deployment ?
        - Options notables de la commande create deployment
          - --replicas=X : permet d'indiquer un nombre de replicas initial
          - --port=Y : permet d'exposer le port donné
        - C'est créé sur un ou plusieurs noeuds ?
      - kubetcl expose deployment web --type=NodePort --port=8080
        - Permet de créer un service qui expose la ressource demandée
        - À quoi correspondent les options --type et --port ?
        - --port
          - Le port sur lequel écoute l'application du ou des pods
          - Des pods ou des noeuds ?
        - --type
          - Le type de service qui va être créé
          - Ici, je suppose que c'est un service simple qui se contente de faire du port forwarding
          - Plus d'infos ici : https://kubernetes.io/docs/concepts/services-networking/service/#type-nodeport
          - Cette page précise qu'on peut aussi passer comme valeur LoadBalancer
          - Permet de provisionner un load balancer fournit par le cloud provider
          - Quid dans minikube ?
            - Le tuto de k8s le fait faire
            - Pas d'erreur reportée, service fonctionnel
  - Passe maintenant à : https://kubernetes.io/docs/tutorials/kubernetes-basics/
    - Cluster
      - Ensemble composé de Nodes et du Control Plane
      - Node
        - Machine, potentiellement virtuelle, qui sert de worker pour l'application
        - Va faire tourner des Pods
        - Chaque noeud possède un Kubelet
          - Agent qui gère le noeud et sa communication avec le Control Plane
      - Control Plane
        - Orchestrateur qui gère la maintenance de l'état de l'application, son passage à l'échelle et ses rolling updates
        - Quelles garanties sont offertes par le Control Plane ?
          - Est-il distribué ? Comment fonctionne-t-il ? Quel impact sur son comportement en cas de panne d'une des répliques ?
    - Deployment
      - Permet de décrire l'état souhaité
      - Un Deployment Controller, géré par le (faisant partie du ?) Control Plane, va ensuite monitorer l'état de l'application et instancier/retirer des Pods au besoin pour obtenir l'état souhaité
    - Le tuto fait remarquer que, au moment de lancer une application, on a un seul Node de disponible
      - Le Node qui fait aussi tourner le Control Plane
    - On peut cependant lancer une application
      - Celle-ci tournera alors sur le même Node que le Control Plane
    - Me paraît mieux de modifier le setup de base pour avoir plusieurs noeuds
      - Au moins 2, le Control Plane et un Worker
      - Pour cela, suis tutos :
        - https://minikube.sigs.k8s.io/docs/tutorials/multi_node/
        - https://medium.com/cloudnloud/how-to-minikube-with-multi-node-setup-1159006fc80e
      - Commandes
        - Créer cluster : minikube start --nodes=2
        - Ajouter Node à cluster existant : minikube node add
          - À voir comment on précise à quel cluster on ajoute ce Node
      - Semble y avoir un problème avec le driver par défaut pour les Volumes dans un cluster multi-nodes
        - https://minikube.sigs.k8s.io/docs/tutorials/volume_snapshots_and_csi/
        - Voir ce que cela implique et corriger si besoin
      - Minikube m'a aussi affiché un warning lors de l'ajout du Node
        - Cluster was created without any CNI, adding a node to it might cause broken networking.
        - Voir ce que cela implique et corriger si besoin
- Trouver des ressources Docker & Kubernetes
  - Guillaume m'a passé le livre *Understanding Kubernetes in a visual way* par Aurélie Vache
  - Elle a aussi fait une série de vidéos sur le sujet :
    - https://www.youtube.com/watch?v=a1Uwoq1Yv6U&list=PLmw3X80dPdlzksg6X9s23LEkLMWFGGUn5
  - Aussi trouvé les vidéos suivantes qui ont l'air pertinentes
    - *Kubernetes Design Principles: Understand the Why* : https://www.youtube.com/watch?v=ZuIQurh_kDk
    - *Kubernetes Explained in 6 Minutes | k8s Architecture* : https://www.youtube.com/watch?v=TlHvYWVUZyc
  - Me parait un bon début
- Discussion avec Éric Poiseau et Olivier Sentieys
  - En réponse au mail de Guillaume informant les autres membres du projet SmartOps, Éric m'a proposé de passer le voir
  - Il m'a présenté le SED et s'est occupé de quelques démarches
    - Ajout à la mailing list ingedev
    - Ajout au mattermost devel
    - Ajout au groupe gitlab SmartSense
    - Présentation rapide de l'AGOS
  - A insisté sur le fait que je passe le voir si je rencontre des difficultés ou ai besoin d'un avis extérieur
  - M'a ensuite présenté à Olivier Sentieys
    - Pensais qu'il était basé à Lannion
    - Mais semble être revenu à Rennes
    - Seul Mickael Le Gentil est basé à Lannion donc
  - M'ont parlé du projet SmartSense
  - Présenté les capteurs SmartSense
    - Carte sur laquelle les capteurs sont branchés
    - Interfacée avec une Raspberry Pi (version 3 si j'ai bien suivi)
      - Permet d'avoir un peu de moyen de calculs localement
      - Et appliquer des traitements sur les données avant de les remonter
      - Notamment, plutôt que de transmettre le flux vidéo/audio
      - Peut traiter ces flux pour remonter des métriques telles que présence de personnes dans la salle, nombre de personnes, type de sons identifiés
      - Permet ainsi de préserver l'anonymat et de limiter l'usage de bande-passante
    - Branché sur secteur pour l'alim électrique
    - Connecté en ethernet pour remonter les données
    - Existe une version adaptée pour l'extérieur
      - Fonctionne sur batterie
      - Et stocke les données collectées sur carte SD, à récupérer manuellement
  - Montré https://co2.irisa.fr/
    - Permet de suivre l'évolution de métriques remontées par les capteurs SmartSense d'une salle donnée
      - e.g. taux de CO2, la température
    - Surprenamment, n'interroge pas la BDD
    - Mais récupère et présente les métriques seulement à partir de l'instant T
    - N'a plus trop l'air de fonctionner
      - Affiche les données à un instant donné au moment où j'accède à l'application
      - Mais n'a pas l'air de récupérer/d'afficher de nouvelles données si je reste sur la page
        - Temps réel ? Fréquence d'échantillonnage ?
      - Plus troublant, la date donnée par le capteur SmartSense est incorrecte
        - [[file:img/2024-01-11-screen-co2-irisa.png]]
      - Et n'a pas l'air de s'actualiser régulièrement
      - Une slide de l'ADT mentionne que les données collectées sont transmises à une time series DB, InfluxDB
    - Est-ce que ça ne pose pas de problème d'avoir des données estampillées incorrectement ?
    - J'ai rien dit
    - En me reconnectant sur le site, je suis tombé sur une salle dans laquelle il y avait une réunion au même moment
    - J'ai ainsi pu faire les capteurs en cours de fonctionnement
      - [[file:img/2024-01-11-screen-2-co2-irisa.png]]
    - L'interface affiche ainsi les nouvelles entrées
      - Une mesure toutes les 20s semblerait
    - Les capteurs sont donc inactifs entre les réunions ?
      - Comment cela fonctionne ?
  - Montré https://smartsense-gest.inria.fr/
    - A l'air d'être une interface de gestion des capteurs
    - M'ont créé un compte, mais ne dispose d'aucun droit
  - Premières pistes de travail concernant SmartSense
    - Rencontrer Guillermo Andrade-Barroso
      - Ingénieur du SED qui a été impliqué de manière plus importante dans le projet SmartSense
      - Aura probablement une meilleure compréhension des différents repos qui composent le projet
        - De leur fonction, état et pistes de travail
    - Une piste déjà identifiée consiste en l'ajout du support du WiFi aux capteurs SmartSense
      - Permettrait dans un contexte en extérieur de transmettre les données
      - Et de me faire découvrir le système
    - Puis voir pour faire interagir les capteurs SmartSense avec la plateforme LivingFog
- Regarder *Kubernetes Design Principles: Understand the Why*
  - Pourquoi k8s ?
    - Souhaite déployer des conteneurs sur noeuds
    - Méthode traditionnelle consiste à se log en SSH sur la machine et exécuter la commande
    - Mais doit ensuite vérifier que tout se déroule correctement
      - Conteneur n'a pas crash
      - Noeud n'a pas crash
      - Connexion SSH a bien fonctionné
    - Besoin d'un outil de monitoring pour cela
    - Et de mécanismes de catch up pour gérer tous ces edge cases
    - Rejoint ce que m'expliquait Guillaume
      - Se retrouve avec une base de code complexe & lourde pour gérer tous les scénarios étranges
  - Approche déclarative
    - Permet en tant qu'user de ne plus se complexifier la tâche avec le "comment"
    - Se concentre juste sur le "quoi", l'état désiré
    - Et l'outil est en charge de réaliser ce "quoi", de mettre en place cet état
  - Pourquoi approche déclarative ?
    - Auto-recovery
      - Si une panne survient, c'est k8s qui est en charge de détecter la panne et de re-converger vers l'état désiré
      - Sans que l'user soit concerné/impliqué dans le "comment"
  - Comment déployer les containers ?
    - Approche naïve est que le Control Plane, à partir de la description de l'état désiré
      - Choisisse un noeud adapté
      - Commande à ce noeud de démarrer le container
    - Reproduirait le pattern qu'on aurait avec l'approche impérative
      - Control Plane devrait alors monitorer et incorporer des mécanismes de catch up en cas de défaillance
  - Pour éviter cela, ré-utilise une approche déclarative en interne
    - Control plane définit l'état désiré de chaque noeud
    - Chaque composant (les noeuds, le scheduler...) va alors oeuvrer pour converger vers l'état indiqué
    - Approche nommée Level Triggered (vs. Event Triggered)
      - Event Triggered : approche event-based
        - Les composants réagissent aux events propagés pour déterminer leurs actions
        - Si un composant a eu une défaillance et a manqué un event, doit mettre en place un mécanisme pour lui re-propager cet event
      - Level Triggered : approche par niveaux
        - Events font progresser de niveau
        - Niveau mis à disposition des composants
        - C'est à partir de son niveau courant et du niveau désiré qu'un composant détermine ses actions
    - Permet de concevoir un système plus simple et robuste
    - Clame cependant qu'aucun composant n'est un SPOF dans ce système
      - Quid du Control Plane ?
      - C'est lui qui conserve l'état désiré du système
      - Et qui reçoit/gère les demandes de MàJ de l'état
        - e.g. scheduler a décidé du noeud qui allait être responsable d'un pod donné
      - Comment il ne peut pas être un SPOF ?
    - Justifie cela de la manière suivante
      - Si le Control Plane rencontre une panne
      - Les différents composants du système continueront à tourner à partir des dernières informations obtenues sur l'état désiré
      - Si un autre composant a une panne
      - Le reste du système continue de fonctionner de manière indépendante
    - Curieux de la charge de travail du Control Plane et du Scheduler
      - Et de l'impact d'une panne du Scheduler
    - Cette approche permet aussi de faciliter l'ajout d'add-ons/l'implémentation de composants customs
      - Doit juste interagir avec le Control Plane pour mettre à jour le niveau comme souhaité
  - Comment fournir les secrets et autres données de config à l'application ?
    - L'API k8s fournit plusieurs objets pour représenter ces données
    - L'API étant transparente, peut modifier son application pour fetch ces données
    - Mais quid des applications legacy qui récupèrent ces données via un fichier ou des variables d'env depuis des temps immémoriaux ?
    - k8s permet de fournir ces données aux pods sous la forme de fichiers ou de variable d'env
  - Comment sont gérés les volumes distants ?
    - i.e. volumes fournis par des services cloud
    - Renseigné directement dans la définition du pod
    - Une fois que le pod schedulé pour un node, le storage controller vérifie si le volume indiqué est attaché au node
      - Effectue les démarches nécessaires si besoin
    - Et MàJ l'état du node
    - Mais c'est une erreur de référencer le type de stockage directement dans la config du pod
      - Pod plus portable, vendor-locked
    - Ont mis en place des abstractions pour répondre à ce problème
      - PersistentVolume et PersistentVolumeClaim
      - Référence une claim dans la config d'un pod
      - Une Claim est un objet k8s aussi
        - Décrit les caractéristiques du volume demandé
        - e.g. accès read-only/rw, type de stockage
      - Et un Controller, le Persistent Volume Controller se charge d'allouer un volume correspond aux besoins par rapport aux services disponibles
  - Pourquoi rendre l'application portable ?
    - Permet de découpler le dev de l'application du cluster/service cloud sur lequel elle va tourner
    - Fait la comparaison suivante : k8s, c'est comme un OS pour les applications distribués
      - Permet de ne plus se soucier lors du dev d'une appli distribué de l'environnement dans lequel cette dernière va tourner
- Adapter la configuration réseau pour clusters multi-nodes
  - Pas particulièrement réussi à trouver des ressources sur le sujet
  - J'ai redémarré minikube cette fois-ci avec 2 nodes d'entrée de jeu
    - minikube start --nodes 2
  - Le log au démarrage ne m'a pas indiqué le moindre warning
  - On va considérer que c'est bon du coup
    - Jusqu'à preuve du contraire
- Utiliser un driver pour Volume adapté aux clusters multi-nodes
  - L'issue indiquée ne propose pas d'autres solutions/d'alternatives à celle présentée
    - Issue : https://github.com/kubernetes/minikube/issues/12360
  - Et semble assez récente
    - Correctif courant février 2023
    - Des users qui confirment la correction du problème courant août 2023
  - J'ai donc suivi les étapes indiquées
  - Le setup de la classe de storage semble s'être effectué correctement
- Réunion SmartSense
  - Réunion ayant pour objectifs principaux de
    - Me présenter la plateforme SmartSense
    - Me présenter les problématiques/pistes de travail que Mickaël & Olivier souhaiteraient qu'on explore au cours de l'ADT
  - Préparation
    - Olivier & Eric m'ont déjà présenté les capteurs SmartSense
      - Le fait qu'ils sont équipés d'une Raspberry Pi 3 pour avoir un peu d'intelligence/puissance de calcul en local
    - M'ont aussi parlé de Guillermo Andrade-Barroso
      - Attendais un peu explorer les repos de mon côté pour le contacter
      - Et d'avoir eu cette réunion
    - M'ont parlé de 2 applications principalement
      - https://co2.irisa.fr/
      - https://smartsense-gest.inria.fr/dashboard
    - CO2
      - Permet de suivre les relevés de données par les capteurs dans une salle à partir d'un instant T
      - Ne voyant pas d'évolution, et les données datant de l'an dernier, pensais qu'il était planté
      - Mais j'ai eu la chance de tomber sur une réunion lors d'un test
      - Et pu voir son fonctionnement
    - Gest
      - Dashboard du système
      - Avait l'air de rencontrer des problèmes de certificats quand Eric a souhaité me le présenter
      - J'ai un compte, mais sans droits d'accès
    - Curieux de mieux comprendre la galaxie de repos du groupe GitLab
      - Quels sont les principaux projets ?
      - Quel est leur rôle respectif ?
      - Est-ce que certains ne sont plus d'actualité ?
      - Y a-t-il un document récapitulant l'architecture globale du système ?
  - Notes
    - Actuellement, raspberry peu utilisée
      - Sert juste à passer les données au réseau
    - Idée serait d'utiliser cette carte pour ajouter des traitements
      - E.g. préparer les données pour permettre la désaggrégation des données
        - Histoire de suivre la consommation énergétique de chaque équipement
      - Détecter la présence de personne
        - Peut utiliser la vidéo
        - Mais aussi le CO2
          - Semblerait qu'il est possible d'estimer le nombre de personnes présentes dans une pièce en fonction de la croissance du taux de CO2
    - Objectif
      - Mettre des traitements à chaque tier de l'architecture
      - Tout au long de la vie de la donnée
        - De la collecte au cloud
    - Axe de travail SmartOps
      - Mettre en place la communication sans-fil
      - Pour permettre interaction avec Living Fog
    - Bug d'InfluxDB sur version en extérieur de SmartSense
      - Fait tourner sur la raspberry une instance InfluxDB
        - Puisque pas de connexion pour remonter les données
      - Mais rencontraient des problèmes de stabilité de l'instance
        - Tâches trop couteuses ?
        - Serait intéressant de creuser et d'identifier l'origine du problème
    - Dernière étape
      - Utiliser du hardware spécialisé, un Digital Software Processor, pour faire un pré-traitement sur les flux (audio/vidéo)
        - Flux trop important/trop coûteux à traiter par les microprocesseurs équipés
      - Actuellement, déjà un DSP d'équipé sur les capteurs SmartSense
      - Un étudiant travaille actuellement sur un projet de cette nature
        - But est de router les micros sur le DSP pour traiter leurs entrées
        - Dans le but de faire par ex de la spatialisation de sources sonores
    - CO2
      - Application réalisée dans le cadre d'un stage
        - Pas vraiment testée/validée
      - Mais bon point d'entrée pour comprendre comment on interagit avec le système pour récupérer les données et effectuer des traitements
    - Gest
      - Possibilité de récupérer les données via un export de la BDD
      - Sinon possibilité de se connecter directement au broker pour récupérer les données en temps réel
        - Mickaël a un script python qui fait ça
  - Prochaines étapes
    - Continuer à me former sur la partie Fog
    - Et découvrir SmartSense
      - Consulter les documents d'architectures dans 3Douest/Documents
      - Consulter le script python permettant en local de consulter les données remontées par SmartSense
      - Consulter le projet CO2 pour creuser plus loin
    - En parallèle, Mickaël voit comment setup l'environnement de dev pour SmartSense
      - Et m'apportera le matériel nécessaire
    - Une fois l'environnement mis en place, première étape sera probablement de mettre en place une communication WiFi
      - Puis d'ajouter des traitements en local sur le capteur
- Déployer une application complexe avec k8s
  - Plusieurs points à creuser au préalable
    - Gestion des volumes
      - On ne créé pas directement les volumes
      - Les abstractions Persistent Volume et Persistent Volume Claim sont là pour permettre de découpler les volumes des cloud providers
      - Indique via une Claim les caractéristiques du volume que l'on souhaite obtenir/mis à disposition de notre application
      - k8s se charge d'allouer un volume fittant ces critères
      - Et on indique dans la specification d'un pod le ou les volumes qui doivent être montés
      - Comment ça se passe si on re-déploie l'application ?
        - Comment garantir que le même volume soit alloué à la même claim ?
          - Est-ce que k8s gère ça de son côté ?
        - On associe un nom de volume à une claim dans la specification du deployment
    - Gestion des services
      - Lors de la création d'un service, plusieurs données sont récupérées
        - L'adresse IP du service
        - Le port sur lequel il accepte les connexions
      - Ses données sont accessibles aux pods par le biais de variables d'env
        - <NAME>_SERVICE_HOST/PORT
      - Mais ces variables d'env ne seront set que pour les pods créés après le service
      - Recommandé donc de créer les services avant les deployments correspondants
      - Comment on fait ce mapping service/deployment dans le fichier de config ?
        - Via les labels ?
        - Le nom plus probablement
      - Utiliser le nom du service ou les variables d'env définies du coup ?
    - Configuration et secrets
      - Possible de définir un fichier de configuration où centraliser les informations
      - e.g. mot de passe de la BDD
      - Quelle est la bonne pratique vis-à-vis de ces fichiers ?
        - Si une donnée est utilisée plusieurs fois dans le/les fichiers de description, la déplacer dans le fichier de configuration ?
    - Gestion des labels et selectors
      - Les fichiers de config que je rencontre renseignent régulièrement des métadonnées pour chaque objet k8s
        - labels
        - selectors
          - matchLabels
          - app
          - tier
      - Quelle est la liste de ces métadonnées ?
      - Quelle est leur rôle respectif ?
    - Organisation du fichier de description
      - Possible de faire un fichier de description par entité k8s
        - service, deployment, etc
      - Un peu lourd et peu pratique
      - Possible de regrouper plusieurs descriptions dans un même fichier
        - En séparant les descriptions respectives par des ---
    - Possible de lier les fichiers entre eux ?
      - Avoir un fichier index en quelque sorte
      - Ou ce n'est pas la bonne pratique ?
  - J'ai suivi le tuto suivant *Example: Deploying WordPress and MySQL with Persistent Volumes*
    - Dispo ici : https://kubernetes.io/docs/tutorials/stateful-application/mysql-wordpress-persistent-volume/
    - Définit un Secret pour indiquer le mot de passe de la BDD
    - Définit des fichiers de description distincts pour
      - L'instance de MySQL et les composants associés
        - PVC, Service, Deployment
      - L'instance de WordPress et les composants associés
        - PVC, Service, Deployment
    - Regroupe la description du système par le biais du fichier Kustomization
    - Déploie le système via la commande suivante
      - kubetcl apply -k ./
      - Warning : option -k et non pas -f vu qu'on passe par un objet Kustomization
    - Fonctionne nickel
      - A pu administrer et modifier le site par défaut proposé par WordPress
        - Création d'une page
        - Ajout d'une image uploadée
  - J'ai voulu répliquer ensuite l'instance de WordPress
    - kubetcl scale replicas=3 deployment wordpress
  - Obtient un pod correspondant sur chaque noeud
  - J'ai alors rencontré les problèmes évoqués par Guillaume
    - Déconnexions intempestives
      - Si on est dirigé vers une instance autre que celle qui a issue notre cookie d'authentification
      - Ce dernier est invalidé
      - Besoin de se reconnecter
      - Mais ne dure que si on continue d'interagir par chance avec la même instance
    - Contenu indisponible
      - Les pages sont bien partagées entre instances
      - Puisqu'elles doivent être décrites en BDD
        - Qui elle est commune à l'ensemble des instances
      - Par contre, l'image uploadée est elle indisponible régulièrement
      - Doit être conservée que par une des instances de WordPress
      - Le volume n'est donc pas partagé par l'ensemble des noeuds
  - Comment les corriger ?
** Semaine du <2024-01-03 Wed> au <2024-01-05 Fri>
*** Planned
**** DONE Installer logiciels
CLOSED: [2024-01-03 Wed 14:39]
- Emacs, VSCode, Git, Docker
**** DONE Configurer Org-mode
CLOSED: [2024-01-03 Wed 14:39]
**** DONE Résoudre problème ethernet
CLOSED: [2024-01-04 Thu 14:09]
- Semblerait que la connexion ethernet échoue à mon bureau
- Trouver et corriger du problème
**** DONE Consulter résultats questionnaire de Ammar
CLOSED: [2024-01-04 Thu 16:37]
- Ammar a produit et envoyé un questionnaire aux gestionnaires d'observatoires d'environnements naturels
  - Afin de comprendre l'existant, leurs usages et besoins
- Disponible ici : https://survey-results.kazem.fr/protected-routes/survey_stats
- Consulter cette ressource pour en apprendre plus sur l'existant et les problèmes rencontrés par les gestionnaires d'observatoires
  - Permettrait ensuite d'en discuter avec Ammar
**** DONE Apprendre raccourcis clavier de Fedora
CLOSED: [2024-01-05 Fri 07:57]
- Ouvrir terminal
- Gérer bureaux virtuels
  - Se déplacer entre bureaux
  - Déplacer applications entre bureaux
- Augmenter/Diminuer volume
- Mettre en veille
- Prendre en screenshot une zone de l'écran
**** IN-PROGRESS Suivre cours de Guillaume sur les technologies cloud
- Disponible ici : https://gitlab.inria.fr/pierre/sct-m1info
**** IN-PROGRESS Régulariser situation du 02/01
**** TODO Trouver des ressources sur Docker & Kubernetes
- Au-delà du cours de Guillaume, existe des ressources pour rentrer plus en détails sur ces outils (talks, livres)
- Voir pour en trouver et les consulter
*** Done
- Installer logiciels
  - Emacs & Git étaient déjà installé
  - A ajouté le repo officiel pour Docker
  - VSCode, c'était un fichier à installer
- Configurer Org-mode
  - Pour org-mode, je suis retourner lire la page de Martin sur la méthodo :
    - https://people.irisa.fr/Martin.Quinson/Research/Students/Methodo/
  - Il y parle de spacemacs, une configuration préfaite d'emacs
    - https://www.spacemacs.org/
  - Je l'ai installé et fait son tuto
  - Un peu pertubante initialement puisque cette config combine les commandes de vim & celles d'emacs
  - À voir ce que cela donne à l'usage
- Réunion avec Guillaume <2024-01-03 Wed> à 15h00
  - Questions
    - Par où commencer ?
      - Documents à lire ?
      - Code ?
    - Comment communiquer ?
      - Mattermost ?
  - Notes
    - Olivier s'intéresse aux capteurs Smartsense
    - Travaille avec Guillaume sur le projet Terra Forma
    - Projet coordonné par membre du département de géo-sciences de l'univ de Rennes
    - Majorité des membres du projet sont non-informaticiens, étudient les sciences de l'environnement
    - Intéressés par des observatoires de l'environnement naturels
    - Délimitent des territoires intéressants et les équipent de capteurs intelligents
    - Solution de base nécessite de récupérer les données sur le terrain après temps de collecte
      - Mais sujets de recherche peuvent nécessiter de traiter les données régulièrement
      - Mais territoires pas forcément accessibles
      - Mettent donc des stations de calculs au sein des environnements
    - Mais stations de calculs existantes répondent pas au besoin
      - Généralement propriétaires
      - Ne permettent que l'archivage des données et la transmission à un cloud
      - Souhaiteraient mettre en place leurs propres applications
        - Déclencher des actions (mettre en route capteurs, changer fréquence d'échantillonnage...) suite à un évenement en temps réel
        - Faire tourner des modèles de l'environnement et les comparer aux données réelles pour les valider/invalider
          - Et potentiellement évaluer l'état de l'environnement si on joue sur un de ces paramètres
      - Mais les solutions ne le permettent pas
    - Utilisation de plateformes de calcul en milieu naturel isolés posent des questions
      - Où trouver l'énergie pour les alimenter ?
        - Solaire probablement, mais s'agit d'une ressource intermittente (jour/nuit, été/hiver)
      - L'énergie étant limitée, comment adapter les traitements en fonction de la quantité à disposition (allumer/éteindre capteurs) ?
      - Comment relancer la plateforme si à court de jus momentanément ?
    - Ammar travaille sur ces problématiques
      - A rencontré et fait un questionnaire à l'attention des gestionnaires d'observatoires
        - Sur l'existant, leurs besoins, leurs attentes
      - Aurait récupérer et mis en forme les résultats de ce questionnaire
      - Voir avec lui à ce sujet
    - En ce qui me concerne, but du projet est de prendre en main la plateforme LivingFog
      - Plateforme développée par plusieurs doctorant-es
        - Probablement pas parfait d'un point de vue technique
        - Mais de la doc existe (livrables pour projet européen, doc technique)
      - A été déployée à Valence dans le cadre d'un hackathon
        - Consistait à proposer des applis de smart city (application de suivi de l'ensablement du port, application de détection de la fréquentation des différentes activités proposées)
        - Résultats très satisfaisants semblerait
      - But est d'évaluer cette plateforme pour notre nouvel usage
        - De déterminer ce qui nous intéresse et non
        - De virer ce qui nous est inutile
        - De consolider ce qui existe et intéressant pour nous
        - Et de l'adapter à notre usage
    - LivingFog repose sur la techno LoRaWAN pour la communication
      - Pratique pour échanger à longue distance en utilisant peu d'énergie
      - Mais faible bande-passante
      - Et qui pose des contraintes supplémentaires
        - Capteurs envoient les données à des gateways qui relaient les messages
        - Mais pas d'association entre capteurs et gateways
          - Les messages sont donc dupliqués
        - La déduplication des messages est effectuée de manière centralisée
      - Des gens de Terra Forma se penchent dessus, nous, on ne va pas se concentrer dessus
    - On va plutôt se pencher sur la partie cluster
      - Utilise des clusters de raspberry
      - Fait tourner kubernetes dessus pour gérer un ensemble d'applications sur un cluster
        - Existe des versions allégées de kubernetes k3s pour cluster de raspberry
    - Première étape est donc de monter en compétence sur les technos correspondantes
    - Guillaume a un cours sur les technos Cloud
      - Va m'y donner accès pour que je le suive et que je monte en compétence là-dessus
    - Creuser plus particulièrement Docker & Kubernetes
- Résoudre problème ethernet
  - Guillaume m'a explique que les prises Ethernet ne sont pas toutes rattachées au même réseau
  - Peut être nécessaire de changer la prise sur laquelle je suis branché
  - Cela n'a rien changé
  - Après discussion avec les membres de la DSI, m'ont dit d'ouvrir un ticket pour qu'ils affectent en dur l'adresse mac du dock à ma machine
  - Ça a résolu mon problème de connexion
- Suivre cours de Guillaume sur les technologies cloud
  - CM1 - Introduction au Cloud
    - Pour offrir un service plutôt qu'un produit, nécessité d'une infrastructure
    - Cloud offre plusieurs bénéfices aux users
      - Comparé à un système traditionnel, permet de déléguer la gestion de l'environnement au provider
      - Permet d'utiliser uniquement les ressources dont l'on a besoin à un instant T
        - Et non pas perpetuellement les ressources dont l'on a besoin pour tenir la charge lors des pics d'activité
      - Permet donc de scale de manière flexible en fonction des besoin
    - Différences entre IaaS, PaaS et SaaS
      - [[file:img/iaas-paas-saas.png]]
      - IaaS
        - Provider ne fournit que les machines virtuelles
        - C'est aux users de setup leurs machines à partir de l'install de l'OS
      - PaaS
        - Ici la machine est déjà installée
        - Il ne reste plus qu'à installer son ou ses applications
      - SaaS
        - Ici, aucune installation nécessaire
        - On souscrit directement une instance de l'application désirée
    - Mentionne que certaines entreprises créent leur propre cloud privé
      - Détaillé par : https://www.datamation.com/cloud/private-vs-public-cloud/
    - Cloud public
      - Cloud tel que je l'imagine et connais
      - Géré par un provider
      - Les entreprises ont recours à ses services et se "contentent" de l'utiliser
    - Cloud privé sur site
      - L'entreprise recrée un cloud chez elle
        - Data-center, machines, gestion
      - Pour cela, peut reposer sur des outils mis à disposition par les cloud providers ou des projets OS (OpenStack)
      - Offre la confidentialité et souveraineté des données
      - Mais en échange, introduit
        - Une charge de travail (setup et manage le cloud)
        - Des coûts à priori (data center, machines)
        - Une limitation de la scalability (doit acheter des machines supplémentaires lorsque atteint la charge limite)
    - Cloud privé hébergé
      - Possible aussi de demander à un provider de s'occuper de notre cloud privé
      - Caractéristiques similaires à un cloud public
        - Même si nécessite plus de préparations et de coûts en amonts qu'une offre publique
      - Mais permet de reposer sur des machines dédiées à notre usage, offrant ainsi sécurité et confidentialité
    - Majorité des entreprises ont un usage hybride entre cloud public et privé
      - [[file:img/usage-cloud.png]]
    - Et rien n'empêche d'utiliser plusieurs clouds d'un même type
      - Pour silo-er les apps, avoir de la redondance en cas de panne d'un provider
    - Questions
      - Un peu de mal à formaliser le PaaS et ce qu'il comprend
        - Je vois ça comme une machine avec déjà son OS de setup
        - Il ne reste plus qu'à installer son application
        - Mais le cours mentionne la couche middleware
        - Qu'est-ce qu'elle couvre et peut offrir comme services ?
          - Mention de DBs et frameworks HPC
  - CM2 - Virtualisation
    - Définition
      - Un logiciel qui imite un appareil physique
      - Fournit au moins les mêmes fonctionnalités
      - Utilise une interface identique
    - Avantages
      - Peut être créé et supprimé à la volée
      - Peut être facilement modifié/configuré
      - Peut proposer des fonctionnalités supplémentaires à la version physique
    - Exemples
      - Clavier virtuel
      - Disque virtuel
      - Système de stockage virtuel (NAS, SAN)
        - Un peu de mal à piger la différence entre ces technos
      - Réseau virtuel (VLAN, SDN)
      - Machine virtuelle
      - Conteneur
    - Remarques
      - Slide 8, opération /take snapshot/ : c'est pas /head = new_snapshot/ plutôt ?
        - Ou /head = empty/ plutôt ?
  - CM3 - Infrastructures cloud VM-based
    - Porte principalement sur la description de l'architecture système d'un cloud
    - Prend pour cela comme exemple OpenStack
      - Se base sur la présentation qui en est faite lors de la *Cloud Architect Alliance #15*
      - Disponible ici : https://www.slideshare.net/alessandrovozza/cloud-architect-alliance-15-openstack
    - Globalement, une multitude de différents services
      - [[file:img/2015-open-stack-architecture.png]]
    - Chacun ayant son rôle et ses responsabilités
      - E.g. Keystone
        - Service d'authentification et d'autorisation
        - Fournit aussi la liste des autres services
    - Composants autonomes, pouvant être indépendamment répliqués pour répondre aux besoins (charge, disponibilité...)
  - CM4 - Services cloud de stockage
    - Présente les différents types de service de stockage offerts par les cloud providers
    - Object storage
      - Niveau de granularité est le fichier
      - Permet de créer,lire et supprimer des fichiers
      - Mais pas de les modifier
        - Les fichiers sont donc immuables
      - E.g. Amazon S3 (Simple Storage Service)
    - Block storage
      - Niveau de granularité est le volume, i.e. des partitions disques virtuelles
      - Permet de créer, modifier les caractéristiques (taille, type de stockage), et d'attacher des volumes aux VMs
      - E.g. Amazon EBS (Elastic Block Store)
      - Propose généralement services supplémentaires
        - Snapshotting, et sauvegarde/réplication des snapshots effectuées
    - Relationnal storage
      - Indique qu'on peut démarrer et gérer son propre SGBD relationnel sur une VM
      - Mais que les cloud providers proposent directement des services de BDDs relationnelles
      - Insiste cependant sur les limites de ce type de système
        - Ne tolèrent pas les partitions réseaux généralement
        - Deviennent soit indisponible, soit incohérente de manière non-maitrisée
    - NoSQL storage
      - Présente les bases de données NoSQL
      - Précise qu'elles ont été conçues pour les besoins des applications cloud, notamment
        - Scalable, i.e. supporter un dataset de très grande taille et une charge importante
        - Elastic, i.e. faciliter l'ajout & la suppression d'instances à la volée
        - Partition tolerant
      - Détaille ensuite différents SGBDs NoSQL
        - DynamoDB (KV-Store)
        - MongoDB (Document-based)
        - Apache Cassandra (Column-based)
    - Remarques
      - Je sais pas si Guillaume mentionne la vague NewSQL dans la partie sur les services de SGBDs relationnels
        - Mais le constat est peu élogieux sous la forme actuelle
        - Est-ce que les SGBDs relationnels sont si inadaptées aux applications distribuées ?
          - Notamment, les systèmes appartenant à la vague NewSQL ne sont pas partition tolerant ?
        - Peut aussi s'intéresser à ce qui se fait du côté de ElectricSQL
- Régulariser situation du 02/01
  - J'ai essayé de déposer mon jour de congé pour le 02/01 le <2024-01-04 Thu>
  - Mais Casa m'empêche de le faire car la date est antérieure à la date du jour
  - À voir au retour de l'assistante d'équipe
- Consulter résultats questionnaire de Ammar
  - Résultats obtenus via 59 réponses (17 complètes, 42 incomplètes), couvrant 25 observatoires
  - Systèmes existants
    - Sources d'énergie
      - Principalement du solaire et de la batterie
      - Comment les observatoires gèrent-ils les limites de ces sources (nuit, batterie vide...) ?
      - Mentionne une source "Autres", des exemples ?
    - Techniques de communication
      - Principalement via mémoire interne (?) et 4G
        - Qu'est-ce qu'on entend par mémoire interne ?
        - C'est pas trop coûteux la 4G ?
    - Équipement info sur site
      - 47% déclarent que les observatoires incluent de l'équipement info en plus des capteurs
      - On a une idée du type d'équipement ?
    - Nombre de capteurs par site
      - La majorité des sites est regroupée (gausienne) dans les tranches 10-20, 20-50 et 50-100
    - Utilisation des données par des /operational players/
      - C'est quoi ?
    - Détection d'évènements automatique
      - Peu de détection automatique (17,65%), et encore moins de réponse automatique (29,41%)
      - À quoi sert la détection sans réponse ?
      - Exemples de réponse ?
    - Pré-processing sur site
      - Peu répandu (29,41%)
      - À quoi sert ce pré-processing ?
      - Les données invalides détectées lors de ce pré-processing sont généralement supprimées (81,82%)
      - Choix ou contrainte ?
    - Détection automatique de capteurs défectueux
      - Seulement 5,88% de sites avec cette fonctionnalité
      - Que font-ils dans ce cas ? Coupure du capteur incriminé ?
  - Systèmes futurs
    - Ammar a abordé dans une seconde du questionnaire la question des systèmes futurs et des fonctionnalités désirées
    - Intégration de données extérieures
      - En plus des données collectées, une partie considérable des réponses montre un intérêt pour intégrer dans les données d'un site des données extérieures
      - Plusieurs provenances suggérées
        - Autres sites/observatoires (52,94%)
        - Services tiers, e.g. Météo France (76,74%)
        - Données manuelles (47,06%)
    - Monitoring de la santé du système
      - Les réponses au questionnaire montre un intérêt/besoin à ce sujet
      - Pistes indiquées
        - Intégrer des données d'autres sources pour aider la détection
          - Quelles données ?
          - Comment cela fonctionne ?
        - Notification
  - Questions globales
    - Les figures présentent-elles les résultats des observatoires des 17 réponses complètes uniquement ?
    - Y a-t-il différents niveaux de réponses aux questions "intéressé-e/non-intéressé-e" ?
      - Y a une différence entre vouloir "pourquoi pas" une fonctionnalité et avoir besoin d'une fonctionnalité
      - Cela peut avoir un impact sur les contraintes du système quand la fonctionnalité considérée est "intégrer des données de services tiers" ou "données en temps réel"
    - Quel est le but de ces observatoires ?
      - Est-ce qu'ils sont là pour simplement observer ?
      - Ou certains ont vocation à agir sur l'environnement observé en cas d'évènement (sécheresse, inondation...) ?
    - Y avait-il des questions libres à ce questionnaire ?
      - Ammar pose des questions sur des aspects précis des observatoires et définit des pistes d'améliorations de par son formulaire
      - Mais est-ce que d'autres aspects sont importants pour les gestionnaires d'observatoires et n'étaient pas abordés dans le questionnaire ?
  - Remarques
    - Choix des couleurs
      - L'utilisation du vert pour indiqué "non" me paraît non-intuitif
        - Une couleur proche du vert pour indiquer un résultat positif et une autre proche du rouge pour indiquer un résultat négatif me semble plus commun
    - Graphique sur l'âge des données collectées
      - L'unité me paraît pas super adaptée
  - Discussion
    - Source d'énergie
      - Batterie interne = piles
      - Mais nécessite tournée régulière pour la maintenance
        - Nécessaire de toute façon pour récupérer les données
    - Équipement IT
      - Ammar n'est pas convaincu qu'il y ait tant d'observatoires sans data logger
        - À creuser avec les gestionnaires d'observatoires
    - Operational Players
      - Organisations tierces, généralement services publiques, qui pourraient utiliser les données collectées par les observatoires pour leur tâche, e.g. alerter la population sur un risque d'inondation, de sécheresse
      - Mais l'utilisation des données des observatoires par les acteurs opérationnels n'est pas le but de tous les observatoires
    - Réponse automatique à un event
      - C'est 30% global, pas juste en réponse à la détection automatique
      - Y a des events manuels, e.g. le passage en hiver
    - Hétérogénéité des observatoires
      - L'ensemble des observatoires montre une grande hétérogénéité de leurs buts, contraintes et besoins
        - Certains ont accès à la 4G, au réseau électrique
      - Tous ne nous intéressent pas dans le cadre de ce projet
      - But du questionnaire est d'identifier les observatoires auxquels nous pouvons apporter quelque chose
* Autres
** Commandes utiles
- Mettre à jour paquets
  - dnf check-update
  - sudo dnf upgrade
- Kubernetes
  - Pod
    - Créer un pod : kubectl create -f pod.yaml
    - Lister les pods existants : kubectl get pods
    - Inspecter un pod : kubectl describe pod mysmallpod
** Raccourcis utiles
*** Fedora
- Se déplacer entre bureaux virtuels
  - Ctrl + Alt + Left/Right
- Déplacer application courante entre bureaux virtuels
  - Ctrl + Alt + Shift + Left/Right
- Redimensionner l'application courante
  - Super + Left/Right/Up/Down
- Déplacer application courante entre écrans
  - Super + Shift + Left/Right/Up/Down
- Prendre en screenshot une zone de l'écran
  - Print Screen
- Verrouiller l'écran
  - Super + l
*** Emacs
- Naviguer dans le fichier
  - Haut/bas : k/j
  - Gauche/droite : h/l
  - Début/fin : g-g/G
- Copier/Coller
  - Sélection : C-SPC
  - Copier (yank) : y (ou M-y pour capturer la ligne entière et le retour à la ligne précédent)
  - Coller : p (après) ou P (avant)
- Afficher image
  - Insérer lien vers image : C-c C-l
  - Toggle inline image : C-c C-x C-v
- Recherche
  - /mot RET pour déclencher la recherche
  - n pour avancer jusqu'à l'occurrence suivante
  - N pour l'occurrence précédente
  - ?mot RET pour déclencher la recherche en sens inverse
- Buffer
  - Revenir au buffer précédent : SPC TAB
*** Terminal
- Ouvrir un nouvel onglet
  - Ctrl + Shift + T
- Changer d'onglet
  - Alt + 1/2/3
- Fermer onglet
  - Ctrl + Shift + W
